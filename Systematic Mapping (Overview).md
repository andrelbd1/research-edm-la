## 2010

### Elsevier
#### **Evaluation of e-learning systems based on fuzzy clustering models and statistical tools** - [10.1016/j.eswa.2010.03.032](http://dx.doi.org/10.1016/j.eswa.2010.03.032)
- **Source:** Expert Systems with Applications.
- **Problem:** Due to the vast quantities of data these e-learning systems can generate daily, it is very difficult to manage manually, and authors demand tools which assist them in this task.
- **Objective:** A system for evaluating the e-learning systems.
- **Methodology:** 1-To use the statiscal tools for analyzing the log files; 2-Developing the application of different fuzzy clustering models (kernelized fuzzy C-means (KFCM)); 3-Combination between the results from the statistical tools and the clustering models to enhance the evaluation for the e-learning systems.
- **Dataset:** Dataset from computing science programme at Saint Mary’s University. Logs were obtained from web access of two courses: Data Structures (180 students); Introduction to Computing Science and Programing (25 students). Data were collected over 4 months. The log including access, http request method and number of bytes transmitted. The paper focuses on the learners profiles based on number of hits, downloaded documents, time of accessing the web course, and day of accessing the course materials.
- **Metrics:** Using two fuzzy clustering techniques, learners were categorized into 5 categories: 1-Regular students: These learners download the current set of notes. Since they download a limited/current set of notes, they probably study class-notes on a regular basis; 2-Bad students: These learners download a large set of notes. This indicates that they have stayed away from the class-notes for a long period of time. They are planning for pretest cramming; 3-Worker students: These visitors are mostly working on class or lab assignments or accessing the discussion board; 4-Casual students: Those students who did not interact with the course material and if they visit the web course they do not download any documents; 5-Absent students: Those students who are absent during the teaching course.
- **Case study:** two courses from Saint Mary’s University: Data Structures; Introduction to Computing Science and Programing.
- **Results:** a. The number of bad students was significantly less than the numbers of worker students and regular students visitors, and bad students class was identified by the high number of hits and document-downloads; b. The size of the worker students class was the biggest one, and identified by the lowest number of hits and document-downloads; c. The size of the regular students class was moderate smaller than worker students and larger than bad students, identified by the moderate number of hits and document-downloads, and regularity of downloading behavior. Both methods were able to cluster the data sets into the specified classes. The results obtained from KFCM were better than FCM when compared with the real world behavior of the students. The majority of the grades will be grad B+ that can fit with workers students, secondly: the next large grade will be A that match with the class regular, and finally the minority that has grade C and fall in the course (grade D) was matched with the Bad student class.
- **Notes:** Good survey on soft computing in e-learning, including data mining to identify learner behavior. Suggestions: Examining how students use the system is one way to evaluate the instructional design in a formative manner and it may help the educator to improve the instructional materials.

### IEEE
#### **Fuzzy predictive models to help teachers in e-learning courses** - [10.1109/IJCNN.2010.5596582](http://dx.doi.org/10.1109/IJCNN.2010.5596582)
- **Source:** The 2010 International Joint Conference on Neural Networks (IJCNN)
- **Problem:** One of the most difficult and time consuming activities for in distance education courses is the evaluation teachers process, due to the fact that, in this kind of courses, the review process should be done using collaborative resources such as e-mail, discussion forums, chats, etc.
- **Objective:** To allow teachers to dynamically know students’ learning performance while the curse goes through.
- **Methodology:** To use the Fuzzy Inductive Reasoning (FIR) methodology to identify a set of incremental models to forecast the final mark of the students and additionally to determine the most relevant features involved in this process. Data set contains only 146 students, from which 30 of them are separated and used latter to evaluate the dynamic assessment of students’ learning performance. From the remaining 116 students a 4-fold cross-validation is carried out to identify and validate the FIR models. Each test set is composed of 29 students whereas the training sets contain 87 students. Four FIR models are made: 1-Model 1 is identified after the evaluation of the first homework, therefore, only GROUP and H1 variables are used; 2-Model 2 is identified after the evaluation of the second homework, therefore, input variables are GROUP, H1 and H2; 3-Model 3 is identified after grading the co-evaluation task and after having available the mark of the forum didactic problem, therefore, input variables are GROUP, H1, H2, COEV and DPF; 4-Model 4 contains all the input variables, i.e. GROUP, H1, H2, COEV, DPF, H3, TF and FIDP.
- **Dataset:** The complete data set contains only 146 students.
- **Metrics:** Fail (0-6), Pass (>6-8), Excellent (>8-10).
- **Case study:** The CECTE (Centre of Studies in Communication and Educational Technologies) Introductory Course was selected for the experiments performed in this study. The introductory course is mandatory for all students that want to get enrolled to any of the masters offered by the CECTE. A set of 146 students enrolled in the introductory course is used in this study. The course has 9 stages:
1-GROUP: The group where the student was enrolled. This course has 7 groups; 2-H1: Mark obtained by the student in the homework #1; 3-H2: Mark obtained by the student in the homework #2; 4-COEV: Mark of the co-evaluation performed by the student of the work of other students; 5-DPF: Mark of the forum didactic problem (referred exclusively to the didactic problem); 6-H3: Mark obtained by the student in the homework #3; 7-TF: Mark of the student’s forum participation (referred to all the topics of the course); 8-FIDP Mark obtained by the student in his/her final report of the didactic problem; 9-MARK: Final mark obtained by the student in the course. All of them are grades except for the group variable that defines in which group the student is enrolled.
- **Results:** The prediction accuracy increases with the model, as expected. Model 1 predicts correctly 17 of the 30 students using only the information giving by the group of that student and its first homework grade. Models 2 and 3 perform much better predicting correctly 25 students out of 30. Finally, Model 4 has only 4 mistakes between Pass and Excellent classes. All the students that fail the course are predicted correctly already by Model 2 that means after the second homework.
- **Notes:** Results show that the second, third and fourth models are able to predict with maximum accuracy the failing students. Therefore, teachers students and this, can give more attention to this set of hopefully, will imply students’ better performance.


## 2011

### ACM
#### **The who, what, when, and why of lecture capture** - [10.1145/2090116.2090128](http://dx.doi.org/10.1145/2090116.2090128)
- **Source:** Proceedings of the 1st International Conference on Learning Analytics and Knowledge - LAK '11
- **Problem:** Lecture video are rapidly being adopted by traditional higher education institutions. However, very few studies have been done on the ways students use lecture capture technology to assist in their learning.
- **Objective:** This paper furthers this area by considering explicitly how students use the underlying technology, not necessarily its effects on student marks or enthusiasm. This study had two goals: to create a low-level semantic logging framework that collected student interactions within the learning environment, and to analyse student interaction and perception data to form groups based on learning preferences.
- **Methodology:** A total of 1125 students participated in using the lecture capture system (Recollect), out of roughly 2,000 eligible students1. Students were shown a brief five-minute introduction to the tool on the first day of class. At the end of the term, students were invited to fill out a 20 minute survey, on paper or online, describing their experiences with the lecture-capture system for a chance to win a gift certificate. It was hypothesised that Students could be categorized into different groups based on their access patterns. Sub-hypotheses: - H1: Group of minimal activity learners. Students may have preferred methods of achieving their learning goals (Not use the tool); - H2: Group of high activity learners. Students may not watch all of each lecture, but will watch some content each week; - H3: Group of disillusioned learners. Students will be keen enough to use the tool near the beginning of the course but will stop using it because they found it did not aid in their learning; - H4: Group of deferred learners. Students will not use the tool at the beginning of the course but began to use the tool closer to the end of the course; It was inspected heartbeat data for each student who used the tool for each week in the course. sing k-means clustering with the Weka toolkit, we aggregated data for a large class participating in our study. We changed student access data into nominal values of “y” indicating that the student watched at least 10 minutes of lecture video that week or “n” to represent that they didn’t watch 10 minutes or more video. We chose a number of clusters equal to our hypotheses plus one as an initial metric. After the number of clusters was increased to 6.
- **Dataset:** Over the four month period of the study 3.4 million events were logged with the Recollect system.
- **Metrics:** - H1: Group of minimal activity learners. Students may have preferred methods of achieving their learning goals (Not use the tool); - H2: Group of high activity learners. Students may not watch all of each lecture, but will watch some content each week; - H3: Group of disillusioned learners. Students will be keen enough to use the tool near the beginning of the course but will stop using it because they found it did not aid in their learning; - H4: Group of deferred learners. Students will not use the tool at the beginning of the course but began to use the tool closer to the end of the course; We changed student access data into nominal values of “y” indicating that the student watched at least 10 minutes of lecture video that week or “n” to represent that they didn’t watch 10 minutes or more video.
- **Case study:** Study investigating the use of Recollect was conducted over one 15 week academic term in 2010. During this term, students from professional colleges and a number of different disciplines, including the sciences, social sciences, and humanities, were invited to use the tool to augment their in-class learning.
- **Results:** The results of k-means clustering with five categories are shows that the data was supported for three of our four hypotheses. H1 (110 participants - 47%), H2 (9 participants - 4%) and H4 (11 participants - 5%). H3 was not presented. The formation of two clusters around watching the video only in week eight or the combination of week seven and week eight, provided data for a new hypothesis: - H5: group of just-in-time learners, students use the tool only for midterm exam review. The results running k-means with a cluster size of 6 shows students can be clustered well into all of the hypothesis: H1 (104 participants - 44%), H2 (9 participants - 4%), H3 (8 participants - 3%), H4 (11 participants - 5%). As a result of the survey about the use of Recollect, 81 selected a predefined answer. 52 answer "I thought the recoded lectures were not valuable", 19 answer that they were unaware that Recollect was available and 10 had limited computer and internet access. Explanations given by 176 students shows  that the primary reason for students abstaining from Recollect use was that they did not feel that they needed the provided support. Many students "… only used it when they missed class…" or "…as a back-up reference…". The second most common response involved their lack of use of Recollect because of scheduling or technical constraints, such as the inability to read what was written on the chalkboard or hear student questions. In regard the students that did use Recollect we analyzed their 207 responses. They show that appreciated being able to miss class when they needed to. Recollect enabled them to stay at home when they were sick without sacrificing their ability to receive a similar educational experience as they would have had in class. Students also felt that Recollect "…helped when they missed class or couldn't hear…" what their instructor was saying. One nursing student said that she "…was able to get a missed lecture and take better notes…" by using Recollect's pause and play functionality. The second most appreciated functionality provided by Recollect relates to this latter student comment since the system allows students to review materials. Students also liked having recorded lectures because it allowed them to listen to their instructor's explanation, which allowed them to focus on understanding so that they could ask questions in class and fill in the details in their notes later.
- **Notes:** This paper has made contributions to the understanding of what kinds of students use lecture capture systems, when those students engage in reviewing content online, and why they are motivated to use this technology. Analytics in learning systems can be used to provide both auditing and interventions in student learning.
learning.

#### **How active are students in online discussion forums?** - [978-1-920682-94-1](https://dl.acm.org/citation.cfm?id=2459936.2459952)
- **Source:** Proceedings of the Thirteenth Australasian Computing Education Conference (ACE 2011)
- **Problem:** How active are students in online discussion forums?; Does the online discussion forum activity have an effect on the marks obtained?
- **Objective:** To investigate the factors that lead to effective online interaction in fully online introductory Computer Science and Information Technology (IT) courses and propose a framework with design principles for online interaction.
- **Methodology:** To find out the distribution of activity of online students throughout a study period and the correlation between this activity and grades achieved. To determine the distribution of student activity we recorded the number of accesses and posts by the students throughout the study period. At the end of the study period, assignment and final examination results for each student were recorded. Using these assessment results we investigated if there is a correlation between the level of student activity in discussion forums and the grades they achieved.
- **Dataset:** We collected data throughout the study period which began in September 2009 and ended in November 2009. 2726 post from introductory IT course and 1144 from introductory programming course.
- **Metrics:** The grades of High Distinction (HD) refers to marks which are in between 80-100, whereas Distinction (DI) refers to 70-79, Credit refers to 60-69, Pass (PA) refers to 50-59 and Fail refers to the of 0-49.
- **Case study:** To conduct the research we have chosen two fully online introductory Computer Science/IT courses. One of them is an Introduction to Programming course and the other is an Introduction to IT course. Both the courses are conducted in a fully online environment and there are absolutely no face-to-face classes. The introductory IT course had 299 students enrolled whereas there were 346 enrolments in the introductory programming course. The students were located in many parts of Australia and also different parts of the world while studying the courses. The age of the students ranged in between 20 to 70 which represent diversity in maturity and motivations of the students.
- **Results:** *First question*: In introductory IT course there were a high number of posts by students during the first couple of teaching weeks where students may have tried to get accustomed of the course details. The number of posts gradually decreased after the initial teaching weeks and again rises during weeks when assignments and examinations are due. In introductory programming course there were soaring numbers of posts in the first couple of weeks. However the number of posts gradually declined and was quite low before the first assignment was due. There are a high number of students who have zero posts (174 in Introduction to IT and 218 in Introduction to Programming). Further, there is a similarity between the trends of student accesses in both subjects; there are a high number of students who did not access the discussion board. Almost 58 percent of students did not access and post in the Introduction to IT subject. This rate of “0” accessing and posting is around 63 percent in the Introduction to Programming subject. Consequently only around 40 percent of the total students in both the subjects accessed and posted in the online discussion forum.  Investigating the data closely demonstrated that students who accessed at least once also posted in the discussion forum. *Second question*: In general, most of the students with higher number of posts achieved Distinction or High Distinction in the assignments and final assessment. There is a definite correlation as most of the students who posted for a few number of times either failed or just passed the course. We found there is a trend between student activity in online discussion forums and the grades they achieved in each assessment.  The students, who posted more, got higher marks in each assignment and in the exam than others, with this trend being same for both the courses. However, looking at this trend, it can not be concluded that active participation is the only reason behind higher marks in assessments. We found there were no students who accessed but did not post over the study period. However there  were students who accessed more, but posted less, at different times.
- **Notes:** The way course content is managed has an impact on how students’ participate in the discussion board. As a result of our study and observation, we believe that it is better to release the assignments periodically. By releasing the assignments periodically, the focus of the students can be diverted towards learning the subject matter sequentially which can provide them with a strong background on the subject material. Managing the expectation is another lesson that we learned from this research. The expectations of the instructors on how the students participate online has to be dependent on the content of the subject. In a course like Introduction to Information technology, where there is a vast opportunity for discussion, the expectation can be around 5-6 posts per week by the students. This number can be a bit too high for courses like Introduction to Programming where there might be fewer prospects for the discussion to broaden. We found there were no students who accessed but did not post over the study period. However there were students who accessed more, but posted less, at different times.  In a subject like Introduction to Programming, there might be a lot of “lurkers”, because the solution of a problem might already be there and lot of students might just view it and not post. Students prefer to build a learning community early on in first few weeks. They introduce themselves and sometimes post their personal email addresses so that they can chat about the subject matter informally.


## 2012

### ACM
#### **Using agglomerative hierarchical clustering to model learner participation profiles in online discussion forums** - [10.1145/2330601.2330660](http://dx.doi.org/10.1145/2330601.2330660)
- **Source:** Proceedings of the 2nd International Conference on Learning Analytics and Knowledge - LAK '12
- **Problem:** Online learner participation has been defined as a complex and intrinsic part of online learning.
- **Objective:** The purpose of the present work is to present a two-stage analysis strategy in order to model and identify learners’ participation profiles in online discussion forums.
- **Methodology:** Based on identifying participation profiles from the different activity patterns conducted by learners in online discussion forums in order to group learners with similar activity patterns together. The analysis strategy conducted in the present work consists of two main stages. - In the first stage, learners’ activity in online discussion forums is characterized in two different domains (writing and reading) and learners with similar activity patterns are grouped together in each domain separately; - the second stage of the analysis strategy consists in grouping together those learners belonging to the same clusters in both writing and reading domains.
- **Dataset:** Thus, the whole dataset involves a total amount of 672 learners (NTOT) distributed in eighteen different virtual classrooms and a total amount of 3842 posts.
- **Metrics:** In writing domain, each learner is characterized according the following four parameters: - Depth: ratio of threads initiated by learner over total amount of threads; - REposts: ratio of reply posts written by learner over total amount of reply posts; - REcross: ratio of learners replied –at least, once– by learner over total amount of learners; - WRrhythm: ratio of days when learner writes at least one post over total amount of days. In reading domain: - RDposts: ratio of posts read by learner over total amount of posts (rdposts); - RDthreads: ratio of threads where learner reads at least one post over total amount of threads; - RDcross: ratio of learners read –at least, once– by learner over total amount of learners; - RDrhythm: ratio of days when learner read at least one post over total amount of days. Participation profiles can be easily identified by shirkers (inactive learners), lurkers (only readers) and workers (active learners).
- **Case study:** The experiments conducted in this paper analyze the activity carried out by learners within the online discussion forums of three different subjects in a virtual Telecommunications Degree (Electronic Circuits, Linear Systems Theory and Mathematics) and throughout three complete semesters (from February 2009 to July 2010).
- **Results:** The withdrawal (dropout) rates of shirkers and low-level lurkers are the top highest and the passing rates of high-level lurkers are comparable with the ones of high- and mid-level workers’ (which are logically the top highest).
- **Notes:**

#### **Modelling learning & performance: a social networks perspective** - [10.1145/2330601.2330617](http://dx.doi.org/10.1145/2330601.2330617)
- **Source:** Proceedings of the 2nd International Conference on Learning Analytics and Knowledge - LAK '12
- **Problem:** Despite the attractive advantages presented by scholars about the impact of technology in the learning process, there is still a lack of understanding of the dynamics of social interaction within learning communities.
- **Objective:** (i) Is there an interplay between social networks, learning and performance? (ii) If so, what is the role of social learning in the inherent relationship between properties of social networks and performance? (iii) how does one quantify and measure learning within a social context? (iv) how does one account for social network properties of structure, relations and position in modeling learning for the purpose of learning analytics?
- **Methodology:** To develop a theoretical model based on social learning and social network theories to understand how knowledge professionals engage in learning and performance, both as individuals and as groups. The study also focuses on how an individual’s levels of participation and depth of engagement in the learning process are impacted by social interactions.
- **Dataset:** In total there were 845 messages in the public forum, and a total of 722 across the private forum. Interactions to and from the lecturer were discarded.
- **Metrics:** 1- Density: Density D is described as a measure of network cohesiveness and is defined as the relation of the existing number ties to the maximum number of ties; 2- Efficiency: Effective size is a measure of the number of alters minus the average degree of the alters within the ego network; 3- Contribution Index: Contribution Index (CI) to measure the level of participation in social learning settings; 4- External-Internal Index: External-Internal (E-I) index is a measure that compares the number and average strength of external ties to internal ties within different sub-groups in a network; 5- Content Richness Score: the dataset was classified according to the level of richness of its content: - Empty Message: Inexistent content, file exchange without dialogue, greeting messages. Weight = 0. Example: "Thank you Peter". - Team Building Message: Team member personal introductions and very basic coordination. Final group closing activities, congratulations for group achievements and recognition for mutual cooperation. Weight = 1. Example: "Excellent work team". - Dissemination Message: Information about group submissions and notifications about new document versions. Weight = 2. Example: "I submitted the last version of our report". - Coordination Message: Team meeting coordination. Weight = 3. Example: "Lets meet tomorrow at 7pm Sydney time". - Collaboration Message: Messages that add value to the group work in terms of knowledge creation. Weight = 4. Example: "Dear Peter I think your answer to the question is correct. However, I found this article in which the authors analyze the issues from a different perspective. Please consider also ..."; 6- Average Tie Strength: we used frequency of contact to calculate average tie strength.
- **Case study:** The domain for this study is an eLearning environment where an online project management course was delivered during the last semester of 2009 for the Project Management Graduate Program in a leading ‘Group of Eight (Go8)’ university in Australia. The course was undertaken by 36 full-time working industry professionals ranging from healthcare to banking, information technology and engineering. The eLearning platform provides a channel for synchronous communication via chat, and asynchronous communication via group discussion boards. There were two kinds of forum. The public forums were focused upon solving general questions about the course, assignments and so on. The private forums, or group forums were created exclusively for internal group coordination and collaboration.
- **Results:** The results show that rather than performance, social learning is highly influenced by the learners’ network of contacts. - Indicative of an equally distributed rate of messages sent and received; - Indicates that there was more internal communication rather than dialogues external to the groups; - Indicative of meaningful content exchange within the groups; - Highly efficient learners are connected with contacts that are expected to provide good quality of content (high Content Richness), so they can fulfill the informational needs of the learner without having to look for other sources; - Learners with higher Content Richness score send more messages than they receive, and as a consequence, there is no reciprocity in terms of meaningful content exchange for social learning; - There is a significant positive relationship between Content Richness score and the individual assignment marks, as well as between Content Richness score and the quiz mark. However the exam result, does not seem to be significantly associated with Content Richness score and for none of the engagement measures proposed in this study; According to the results we can argue that rather than performance, social learning is influenced by social network properties such structure, relations and position. A better understanding of how individuals organize themselves and interact with others can prove useful to to improve learning programs that directly influence performance. Content Richness was shown to be a good predictor of social learning due to the interesting findings that connect the measure with most social network characteristics modeled here. The results are indicative of the power of social networks in influencing learning and indirectly, performance.
- **Notes:** There is no single theory that can explain by itself how people learn.

#### **Course correction: using analytics to predict course success** - [10.1145/2330601.2330664](http://dx.doi.org/10.1145/2330601.2330664)
- **Source:** Proceedings of the 2nd International Conference on Learning Analytics and Knowledge - LAK '12
- **Problem:** One predictor of students’ decision to drop out that is under the institution’s purview, however, is a lack of preparation or effort as reflected in their course grades.
- **Objective:** This paper discusses the rationale for the model; the process through which it was developed, revised and refined; and the validation of the model by the operational team.
- **Methodology:** Based on the literature, the variables were identified as potentially useful and worth examining. - Model 1: The data included a unique identifier; basic demographic information from a voluntary survey completed by the student at time of admission; and academic history within the University, including number of transfer credits, number of courses taken, and percentage of points earned in these courses.  These data were analyzed using logistic regression, with the outcome variable being an indicator of whether the student passed the course. - Model 2: The data included military status and financial status. Gender, age, military status, pell grant receipt and responsible party (whether the student was receiving financial aid, paying through their employer, or paying directly) were not significant or resulted in extremely low weights. Model 2 was built using a Naïve Bayes algorithm in RapidMiner and validated using 10x cross validation.
- **Dataset:**
- **Metrics:** - Model 1:less of 65% points in prior courses; more of 85% point in prior courses; Credits earned at Univ of Phx; Online Posts; Cumulative points Earned. These coefficients allow us to classify each student into one of three tiers in week zero: high risk, low risk, or neutral risk. The team felt that a better way to categorize the students in that zone was to assigned a “score” to each student, ranging from 1 (unlikely to pass) – 10 (nearly sure to pass). - Model 2: Cumulative points earned (%); Financial Status not current; Ratio credits earned/attempted; Points earned - prior courses (%); Online post count; Point delta - prior courses >10%; Sum of student loans taken; Number of concurrent courses; Attendance.
- **Case study:** Model that was created for the University of Phoenix. Within the model’s architecture, data from the learning management system (LMS), financial aid system, and student system are combined to calculate a likelihood of any given student failing the current course.
- **Results:** - Model 1: By week 3 all bachelor’s degree students were out of the neutral zone. Results for students not in the neutral zone were accurately predicted on average 94% of the time, with no week below 85%. In other words the prediction of pass (low risk) or fail (high risk) was accurate more than 90% of the time. - Model 2: Model 2 accurately predicted 85% of all students at week 0 rising to 95% by week 3. The ratio of credits earned to credits attempted was a substantial indicator of potential problems, as was a financial status other than current.
- **Notes:** There are known reasons students fail to graduate. Work schedules, health problems, child care challenges, transportation, and financial issues comprise some of the reasons students drop out that are largely outside of the institution’s control.


## 2013

### ACM
#### **Improving retention: predicting at-risk students by analysing clicking behaviour in a virtual learning environment** - [10.1145/2460296.2460324](http://dx.doi.org/10.1145/2460296.2460324)
- **Source:** Proceedings of the Third International Conference on Learning Analytics and Knowledge - LAK '13
- **Problem:** How learning analytics can be used to improve retention?
- **Objective:** To discuss work conducted by the Open University using data from the Virtual Learning Environment (VLE) combined with other data sources to predict students at risk of failing a module.
- **Methodology:** The aim was to develop models for predicting student failure using data from the Virtual Learning Environment in combination with the assessment data: each Open University module has a number of Tutor marked Assessments (TMA’s) as well as a final exam. The models were developed and tested on historic data sets from three modules. Student’s VLE activity was analysed on the three modules, firstly by looking at all clicks, and then by breaking it down by the VLE data categories. The next stage involved developing classifiers to predict risk, which is defined as either: - performance drop: predicting a previously well-performing student will fall below the pass threshold in the next activity period; - final outcome: predicting whether a student will pass or fail the course; Training and testing were performed with the historical data from three modules. For both types of models, decision trees were found to outperform SVM’s. The features of the performance drop model included the students’ assessment scores and the number of VLE clicks in a time window. The feature to predict was the nominal class label (“drop”/“no-drop”). The final outcome prediction model uses, as features, the scores for TMAs, the average TMA score, and the number of VLE clicks in periods between submission dates of each two subsequent TMAs. Demographic data was added to see if this would improve prediction of final outcome.
- **Dataset:** The modules were from the different subject areas of art, mathematics and business. Module A: 4.397 students and 1.570.402 clicks; Module B: 1.292 students and 2.750.432 clicks; Module C: 2.012 students and 1.218.327 clicks; The VLE clicks are recorded by category, such as whether they were clicks for a module page, or clicks on a forum or discussion topic.
- **Metrics:** The results are measured using the standard precision (p), recall (r), and F1 measures for the class “drop”1. With the window size 3, the measured using the standard precision (p), recall (r), and F1 measures for the class “drop”. precision = (true positives)/(true positives + false positives); recall = (true positives)/(true positives + false negatives); F1= 2*(precision)*(recall)/(precision)+(recall).
- **Case study:** Work conducted at the Open University (OU) into predicting students who are at risk of failing their module. The Open University is one of the worlds largest distance learning institutions.
- **Results:** The first general finding was that student activity, on average, increases in the week that an assessment is due. A second important finding was that there is no such thing as an average student. There were students who clicked a lot and still failed, or those who clicked hardly (if) at all and yet passed. The number of VLE clicks occurring just before the TMA being predicted was found to be the most informative attribute: a student who used to work with the VLE before but then stopped is likely to fail at the next TMA. In regard performance drop the number of VLE clicks occurring just before the TMA being predicted was found to be the most informative attribute: a student who used to work with the VLE before but then stopped is likely to fail at the next TMA. TMA data on its own was not found to be very good at predicting performance drop. VLE data on its own was marginally better. The best prediction occurred when these two data sources were combined. In regard final outcome VLE clicks were again found to be better for prediction than the assessment scores. Again, VLE and TMA data combined are generally better for prediction, especially when compared to using only assessment data. The importance of VLE, demographic or TMA data for prediction depends on the point in the module at which the prediction is being made. The data suggested that in the early stages the VLE data is best for prediction, then the demographic data and finally the assessment scores. However after the third assessment, the assessment scores become more informative. This seems reasonable, since the final result depends in some way on the assessment performance. Another possible explanation is that the students who drop out due to lack of motivation/difficulties, as evidenced by their VLE activity, tend to do so earlier in the module. Those who drop out later may do so for more unpredictable reasons, such as sudden personal problems. In a final test of using decision tree classifiers for prediction, the trained models were applied to different data sets. Unsurprisingly, the models generally proved best when applied to the same module. However, the quality on the ‘transferred’ models were still of reasonable quality and even in some cases better than on the original course it was trained on. They do not give much precision for later stages of student drop out, nor do they provide module specific information for informing managers of why students might be struggling. If a student has started out clicking and then stops, this is more of a warning than if their clicking behaviour has been low to start with. If a low-clicking student reduces clicking by only a small amount, then this may be significant in terms of the percentage drop compared to their previous clicking behaviour, rather than in terms of their overall level of activity.
- **Notes:** Investigations into the differences between ‘click’ and ‘brick’ establishments suggest several contributory factors, but with  conflicting results among different studies as to which have the biggest impact on student drop out.

#### **Assessing students' performance using the learning analytics enriched rubrics** - [10.1145/2460296.2460335](http://dx.doi.org/10.1145/2460296.2460335)
- **Source:** Proceedings of the Third International Conference on Learning Analytics and Knowledge - LAK '13
- **Problem:** One of the major challenges in these technologically and pedagogically enhanced learning environments is that teachers face serious difficulties to thoroughly capture, track, and analyze the massive amounts of  data gathered during an interactive  learning process  in order to assess students’ performance.
- **Objective:** To help teachers analyze data according to a set of indicators.
- **Methodology:** This paper proposes an innovative assessment tool, called Learning Analytics Enriched Rubric (LAe-R), which has been developed as a Moodle plug-in. The tool allows teachers to easily create an “enriched rubric” containing criteria and related grading levels that are associated with indicators about learners’ interaction and learning behavior in a Moodle course.
- **Dataset:**
- **Metrics:**
- **Case study:**
- **Results:** A usability test and acceptance of LAe-R were evaluated. Regarding LAe-R, 91,7% of the teachers stated that they felt comfortable using this tool witch was easy for them to work with and 75% were satisfied by its’ interface. According to LAe-R’s use from student evaluation, 91,7% declared that they found LAe-R very useful, 83,3% quick to process and 75% stated that performing student evaluation seemed effortless.
- **Notes:**

#### **Tap into visual analysis of the customization of grouping of activities in eLearning** - [10.1145/2536536.2536575](http://dx.doi.org/10.1145/2536536.2536575)
- **Source:** Proceedings of the First International Conference on Technological Ecosystem for Enhancing Multiculturality - TEEM '13
- **Problem:** Is it possible to find a relationship between student performance and participation using visual analysis tool and characterizations of activities?
- **Objective:** To analyse the following cases: - Temporal relationship between the categorizations of custom activities and performance of students. - Search for patterns in the frequencies of the categorizations of activities and / or student performance.
- **Methodology:** An interactive visualization multidimensional representation using the spiral technique used in Semantic Spiral Timeline (SST) was implemented in order to improve it and carry out this study. We expanded the spiral timeline, which permitted the users to create, export and import different personalized grouping of activities (Student- Teacher, Passive, Create-Interaction, etc.), and some of these groups were produced automatically (grade, role, year, day, etc.). The analysis consists of selecting for each course only those activities which where performed by students, which were then filtered by each grade of performance of students, and finally, each of the grouping activities was filtered.
- **Dataset:** Almost 55000 events were analysed.
- **Metrics:** 1. The grouping activities by agent: - Student-Teacher; - Student-Content; 2. The grouping activities by purpose: - Active; - Passive; 3. The grouping activities by interaction: - Transmit-Content; - Create-Interaction; - Evaluation Course and Teacher.
- **Case study:** We have used three data courses of a different nature (course 1, course 2, and course 3). The platform of choice for the delivery of the courses is Moodle. All students and teachers were given a user manual before starting the course, and they have established channels for the resolution of technical questions via email or forums in the platform and the statistics collection mechanisms of interaction provided by the VLE platform. We used our visual tool to analyse a total of 91 students. Data for this study were collected over a period: in the case of Course 1, May 2011 to August 2011, and for the case of Course 2 and Course 3 November 2011 to March 2012.
- **Results:** The days with greatest activity are Monday, Tuesday, Friday and Saturday. The most significant relationship is that each of the courses changes the greatest activity day at the same moment that changes the grade of performance of the student. A  group of students that have determinate grades on a course, do not share a day of greatest activity with the other group. And more specifically the group of students with the best grades did not share their day of greatest activity with the other groups. It is very interesting to note that independently of the time of the year in which the courses were held, the type of the courses and the students, all three courses seems to follow the same behaviour.
- **Notes:** There will be greater opportunities for teachers and administrators to monitor student activity and for interaction with the course content and their peers.

#### **Visualisation of learning management system usage for detecting student behaviour patterns** - [978-1-921770-21-0](http://dl.acm.org/citation.cfm?id=2667199.2667211)
- **Source:** Proceedings of the Fifteenth Australasian Computing Education Conference
- **Problem:** At-risk behaviours (those students who are likely to fail, or withdraw, from a course) are becoming increasingly difficult to detect within our overburdened higher education systems, with large classes, depersonalised administrative systems and separation from peer groups.
- **Objective:** We want to assess engagement automatically in order to predict results for students, so as that we may find those who may be at risk of failing a course, or dropping out, in order to attempt to prevent this occurring.
- **Methodology:** In this paper, we propose a framework for the identification of at-risk students using a combination of simple metrics, gathered from the domains of social network analysis and statistical analysis. We utilise three distinct methods to explore student engagement: - Social Network Analysis (SNA): SNA techniques enable us to explore relationships within our “network”, which consists of data contained within the LMS, such as forum postings and course resources, the student cohort, and connections from students to the data, i.e. a student may read a forum message posted by another student. The actors in the course are students who interact with each other via resources. They are related when two of the same actors have accessed the same resource. We can use absent connections to find those who are disengaged from the network and hence likely to be disengaged from the course; - Frequency of Access Analysis: analysing patterns of access for individual resources and student access patterns over time. We believe that students who are more engaged with a course will seek to access a wide array of materials and that they will access them frequently as required; - Measure of Distance Analysis - analysing patterns of access behaviour and similarity between student access patterns both visually and quantitatively; We will visualise these results using a “heat map”, which shows gradients of access frequencies. Of relevance to us for this research are resource and forum accesses, of which all possible actions are: view a course resource; view all available forums; view all discussion headings in a specific forum; view a particular discussion on the forum; add and update a post.
- **Dataset:** The data contains four entries for each “action” performed on the system, these are: Full Name,  Date, Access Type and Information (the name of the resource on which the action was performed). The data represents a course that contained 47 enrolled students, with 44 completing the course and receiving a final grade, and has 22,320 unique data entries logged on the Moodle system for this course.
- **Metrics:** High Distinction - A grade of or over 85%; Distinction - A grade of or over 75%; Credit - A grade of over 65%; Pass - A grade of or over 50%; Fail - A grade below 50%;
- **Case study:** We have implemented our framework using data from the Moodle LMS. The data which we have used is from a typical third year course run at the University of Adelaide.
- **Results:** We see the visualization of the network as a whole. This gives us a view of the engagement with the course by showing students linking with the materials We see that students on the outer edges of the network are those that are less engaged with the course, and are more likely to fail. We can pick out the students who are not well connected to the network. These are the students for whom intervention is needed, as they are not accessing materials. Students who access relevant materials, at the correct time, are those who are most likely to succeed. Credit students have a mean of around 40 and have a higher access rate than passing students. Failing students have the lowest mean number of resource accesses, with around 30. Lower achieving students have a lower rate of access than higher achieving students. Failing students access the fo- rums on around 30 days of a course that ran for over 100 days, in comparison to a mean of 45+ dates for passing students and 60 for Distinction students. The engagement with the forums and find if this has a bearing upon the final grade of a student. Lower grade students have more sparse access levels that high grade students. For example resource “46” has been accessed multiple times by students in the highest grade bands, however the same result is not seen for students who go on to fail the course. Students with a higher grade are accessing the forums more frequently. The access in forums has increased when assignments are due as well as the date of the final examination. Failing students have a lower rate of access.
- **Notes:** Students who are more engaged with the course perform better in terms of their final grade.

#### **Deconstructing disengagement: analyzing learner subpopulations in massive open online courses** - [10.1145/2460296.2460330](http://dx.doi.org/10.1145/2460296.2460330)
- **Source:** Proceedings of the Third International Conference on Learning Analytics and Knowledge - LAK '13
- **Problem:** The relatively low completion rates of MOOC participants has been a central criticism in the popular discourse.
- **Objective:** In this paper, we seek to strike a balance by identifying a small yet meaningful set of patterns of engagement and disengagement.
- **Methodology:** Our learning analytics methodology is designed to identify a small number of canonical ways in which students interact with MOOCs. The first step in our methodology is to generate a rough description of each student’s individual engagement in a course. For each assessment period, all participants are labeled either “on track” (did the assessment on time), “behind” (turned in the assessment late), “auditing” (didn’t do the assessment but engaged by watching a video or doing a quiz), or “out” (didn’t participate in the course at all). Once we had engagement descriptions for each learner in a course, we applied the k-means clustering algorithm to identify prototypical engagement patterns. Since we wanted to account for the random properties of k-means we repeated our clustering one hundred times and selected the solution with the highest likelihood. Though clustering was performed separately on all three courses, the process extracted the same four high-level, prototypical engagement trajectories: - ‘Completing’: learners who completed the majority of the assessments offered in the class; - ‘Auditing’: learners who did assessments infrequently if at all and engaged instead by watching video lectures; - ‘Disengaging’: learners who did assessments at the beginning of the course but then have a marked decrease in engagement; - ‘Sampling’: learners who watched video lectures for only one or two assessment periods; To evaluate the clusters produced by this methodology we tested that (1) the trends derived were robust to perturba-tions in the methodology, (2) the clusters that we arrived at had a healthy “goodness of fit” for the data, and (3) that the trends made sense from an educational perspective.
- **Dataset:**
- **Metrics:** For each assessment, labels was assigned for each student: - on track (did the assessment on time); - behind (turned in the assessment late); - auditing (didn’t do the assessment but engaged by watching a video or doing a quiz); - out (didn’t participate in the course at all); We assigned a numerical value to each label (on track = 3, behind = 2, auditing = 1, out = 0). Though clustering was performed separately on all three courses, the process extracted the same four high-level, prototypical engagement trajectories: - Completing: learners who completed the majority of the assessments offered in the class; - Auditing: learners who did assessments infrequently if at all and engaged instead by watching video lectures; - Disengaging: learners who did assessments at the beginning of the course but then have a marked decrease in engagement; - Sampling: learners who watched video lectures for only one or two assessment periods;
- **Case study:** Our analysis of learner trajectories is based on three computer science courses that vary in their level of sophistication: “Computer Science 101” covers high school level content (HS-level), “Algorithms: Design and Analysis” covers undergraduate level content (UG-level), and “Probabilistic Graphical Models” is a graduate level course (GS-level). In all three courses, the vast majority of active learners are employed full-time, followed by grad-uate and undergraduate students. Moreover, most learners in the UG-level and GS-level courses come from technology-related industries. The majority of learners in the UG-level course report to hold a Master’s or a Bachelor’s degree. Most learners are located in the United States, followed by India and Russia.
- **Results:** All three courses enrolled more male than female learners. Learner engagement groups are approximately equally distributed within age brackets, except in the GS-level course, where there were fewer elderly (65+) Completing and Auditing learners, and none under the age of 18. Learner engagement groups are approximately equally distributed within age brackets, except in the GS-level course, where there were fewer elderly (65+) Completing and Auditing learners, and none under the age of 18. As HDI (Human Development Index) increases, the proportion of Completing and Disengaging learners increases, while the proportion of Sampling learners decreases. To circumvent this issue, we analyzed the distribution of engagement patterns for the four most represented countries (US, India, Russia, and the UK) which happen to span over three HDI levels: the US and UK rank ‘very high’, Russia ranks ‘high’, and India ranks ‘medium’. Ratings of overall experience are highly significantly different between engagement groups in all three courses. In the HS-level and GS-level courses, the overall experience of Completing and Auditing learners is not significantly different fromeach other, but significantly above Disengaging and Sampling learners. The UG-level course exhibits a different pattern, with Completing learners having a significantly better overall experience than the other engagement groups. Forum activity varies significantly between engagement trajectories with medium to large effect sizes, with Completing learners participating at significantly higher rates than learners in other engagement trajectories. A consistent pattern in all three courses is an average Streaming Index (SI) above 0.5 for each engagement trajectory, which indicates that streaming is the dominant form of access to video lectures. In the HS-level course, the SI is consistently higher than the other courses across all engagement patterns: streaming accounts for around 88% of video consumption, compared 70% in the UG-level and 63% in GS-level courses.
- **Notes:** We recommend that they incorporate an understanding of the high level ways in which students engage.

#### **Dependencies between E-Learning Usage Patterns and Learning Results** - [10.1145/2494188.2494206](http://dx.doi.org/10.1145/2494188.2494206)
- **Source:** Proceedings of the 13th International Conference on Knowledge Management and Knowledge Technologies - i-Know '13
- **Problem:** Predict learning performance in educational activities.
- **Objective:** This paper analyzes correlations between usage variables and learning results in blended learning courses within an intensely used, web-based LMS platform.
- **Methodology:** We calculated Web Analytics indicators, mined browsing sessions and clustered them according to similar usage behavior, and identified seasonal effects in LMS usage data. A set of LMS usage variables was calculated for each student. Thereby we differentiated between usage within the course (“topic”) and outside the course (“other”). For each aspect we considered the following variables: - Sessions: number of sessions of the course participant, (sessions topic, sessions other); - Views: number of page views of a course participant (views topic, views other); - Duration: amount of time in seconds spent in a course (topic duration) and in other courses (other duration); - Days: number of days being active in the LMS platform (topic days, other days); - Excs: number of different exercises (provided for self-to monitor the learning the students evaluation of progress) solved by this student (excs topic, excs other); - Exam: number of different exam examples (provided to the students for the preparation for the final exam) solved by this student (exam topic, exam other); - Lecturecast duration: amount of time a participant has spent on lecturecasts (lecturecast topic duration, lecture-cast other duration); We implemented a Web Usage Mining process that includes the typical phases: - usage preprocessing: phase comprises the cleaning and filtering of the log-files; - pattern discovery: deals with calculating the statistics on the LMS usage; - pattern analysis: describes the phase in which the analysis takes place. We conducted Pearson correlation tests and ANOVA variance analysis;
- **Dataset:** Log-file with the following characteristics: C1: 2.3 Mio. user activities, C2: 1.2 Mio. user activities, C3: 1.6 Mio. user activities. In addition to the standard fields (IP address of client, remote user, HTTP user, timestamp, HTTP request, HTTP response, HTTP response size, HTTP referrer, user agent), we added LMS-specific data to the information for certain additional such as log-file entries, activities (e.g., exercises or exams) and the identifier of the user.
- **Metrics:** Grades clustering for each course: - Grade = 1: C1 >= 90%, C2 >= 80%, C3 >= 87%; - Grade = 2: C1 >= 80%, C2 >= 70%, C3 >= 80%; - Grade = 3: C1 >= 70%, C2 >= 60%, C3 >= 70%; - Grade = 4: C1 >= 60%, C2 >= 50%, C3 >= 60%; - Grade = 5: C1 < 60%, C2 < 50%, C3 < 60%; Students clustering: - Best learners: All students with the best grade (=1); - Good learners: Students with grade better than 3 (=1,2); - Positive learners: Students with a positive grade (=1,2,3,4); - Bad learners: Students with a negative grade (=5);
- **Case study:** Performed in the Vienna University of Economics and Business (WU) that uses a LMS called Learn@WU platform. The data for the empirical study were collected in the winter semester 2012/13. We chose a sample of three courses of different knowledge domains (law, business, IT) within the Common Body of Knowledge. The three courses are referred to as C1, C2 and C3. All three courses are offered as half-semester courses with durations of two months and end with a final test, which is a multiple-choice test in all cases. Only those students who attended the exam were considered in the data analysis.
- **Results:** Small correlation between the achieved final points in the exam and sessions, views, exercises and exams within each course (topic) and outside them (other). Our analysis exhibits that there are (small and medium) positive correlations between time-dependent usage variables (days, duration and lecturecast duration) and the final grades in all three courses. Students preparing between 7 and 11 days (of the 12 potential days for this exam) achieved the best results. The correlations between the final points and the topic duration has a different characteristic. For C1, after an online learning time of about 33 hours (120,000 sec), a peak is reached with no improvement up to about 61 hours (220,000 sec). A decline in final exam performance is observable beyond that time. It can be concluded that spending more than 61 hours for the exam preparation does not improve the results. In C1, the best learners and thus those with the highest grade, on average have spent almost the double amount of time on the LMS, in the course of the topic, than those students who failed in the exam. There is, however, not much difference between best and good students. It evidences that bad learners can be clearly distinguished from best and good learners. A comparison between best and bad learners shows a big gap regarding the exercises, where the best learners have solved a much broader variety of exercises (about 700) than the bad learners (about 200) on average.
- **Notes:**

### Elsevier
#### **A web-based intelligent report e-learning system using data mining techniques** - [10.1016/j.compeleceng.2012.09.011](http://dx.doi.org/10.1016/j.compeleceng.2012.09.011)
- **Source:** Computers & Electrical Engineering
- **Problem:** The use of e-learning systems has resulted in a need for monitoring and enhancing behavior patterns of all participants, with the aim of continuous improvement of the teaching process and ultimate results – education services.
- **Objective:** We aim to improve the LMS through the enhancement of courses by using data mining techniques. In this study include the following: - To identify features of data mining techniques to obtain predictions of user behavior patterns; - To allow teachers to have access to future patterns of behavior for their students; based on this information, adjustments can be made in Moodle courses; - To improve the existing system of reporting, such that the Moodle LMS will have web intelligence using data mining techniques.
- **Methodology:** The paper presents the creation of a web-based intelligent report e-learning system using data mining techniques with PDCA (Plan, Do, Check, Act).  PDCA is a method to improve web-based intelligent reports of an e-learning system as intelligent system. In this paper, data mining techniques are applied only to data from the Moodle server. The process of making an intelligent, web-based reporting system includes several stages: - In preprocessing columns that relate to additional information and to URLs are irrelevant in this research, and are therefore excluded from the log files; - Creation of the OLAP cube and dimensions; - Data mining models were then created.  In our study, a decision tree is used to predict the percentage of occurrence modules with selected input parameters. The use of artificial neural networks deal, In our study, with the impact of all attribute values that relate to the predictable state is given; - Data Mining Extensions (DMXs) to obtain predictive patterns. The SQL Server Management Studio accesses and checks the writing of DMX queries that are needed to obtain an adequate prediction of behavioral patterns. The request was written using the decision tree model; Intelligent reporting systems take records of user activities within the Moodle server. The data is processed and subject to execution in the corresponding DMX, depending on whether or not the user wants to see the predictive analysis. The results are displayed to the user within a web page and can then be executed during a new analysis.
- **Dataset:**
- **Metrics:**
- **Case study:** A case was performed in a course in Electrical Engineering. This case relates to a module system that predicts electronic modules within the course. The analyses are conducted at the beginning of the summer term (February 4th) by the ‘‘introduction’’ of courses in numerous fields, such as Electrical Engineering. The teacher can choose his/her own courses and an analysis technique according to the teacher’s interest.
- **Results:** Shows a tool to predicting some behavior based on previous log using decision tree and neural network.  The teacher receives display modules and activities associated with the percentage probabilities.
- **Notes:** Does not show features of data mining techniques to obtain predictions of user behavior patterns. It was not performed a validation using a huge dataset.

#### **Meaningful posts and online learning in Blackboard across four cohorts of adult learners** - [10.1016/j.chb.2013.07.021](http://dx.doi.org/10.1016/j.chb.2013.07.021)
- **Source:** Computers in Human Behavior
- **Problem:** What are the best predictors of successful learning and develop a training program for improving online patterns and behaviors.
- **Objective:** The purpose of this research is to determine the best predictors of successful learning and develop a training program for improving  online patterns and behaviors.
- **Methodology:** The present study investigates the correlates of online learning success in science students who represent millennial through older baby boomer cohorts from 2007 to 2011. The present study, 2011, includes three standardized questionnaires about personal responsibility locus of control as measured by Duttweiler’s Internal Control Index  and the ROPELOC. The assessments were given to health science graduate students in two 16-week longitudinal studies over a two-year period. Online measures of learning included the amount of cumulative meaningful posts during the course, the total number of posts, and knowledge gained, the graded performance.
- **Dataset:**
- **Metrics:**
- **Case study:** The present study includes 52 doctoral candidates in a health science program in the years 2011 and 2012 when a quarter of them are less than 28 years old, a quarter 29–39, 40–50 and 51– 61. Measures of online performance were collected longitudinally for 16 weeks and included total hits to the website, number of meaningful posts, points on first six assignments, and points on a larger homework assignment within a Blackboard learning system. All student volunteers were given three standardized assess-ments in the following order: ICI, PRM and ROPELOC. Learning outcomes and class points were calculated from a set of 22 homework assignments and one final article review.
- **Results:** Across all age cohorts, the most reliable  predictor of the main criterion variable, class points, was, the number of meaningful posts,  and not the total amount of online activity.  Chronological age was significantly correlated with online activity,  and marginally related to the number of meaningful posts. A linear regression model predicting class points showed that the number of meaningful posts alone reliably predicted 20% of the unique variance to class points.  No other variables made a significant contribution to classroom learning as measured by homework and test points awarded. The oldest cohort, (51–61 years of age in 2011) had reliably more meaningful posts, x = 5.5, (SD = 2.64), than the next youngest (40–50), x = 2.2, (SD = 1.83), the next (29–39), x = 3.1 (SD = 1.72), and the youngest cohort (<28), x = 3.0, (SD = 1.83).
- **Notes:**

#### **Examining students’ online interaction in a live video streaming environment using data mining and text mining** - [10.1016/j.chb.2012.07.020](http://dx.doi.org/10.1016/j.chb.2012.07.020)
- **Source:** Computers in Human Behavior
- **Problem:** The explosive growth in the amount of data in learning environments has created a need to automatically analyze the data using novel information technology. What are the major patterns of participation and interaction (social presence, teaching presence and cognitive presence) in LVS students’ online questions and chat messages? How is students’ participation and interaction in the LVS environment related to their learning achievement in the LVS course?
- **Objective:** To contribute to the understanding of educational data mining.
- **Methodology:** This study applied data mining and text mining techniques to explore students’ patterns of participation and interaction in a live video streaming environment by examining the data automatically collected by the LVS environment. We applied various data mining and text mining techniques to examine the two different data sets in order to gain insights about students’ participation and learning behaviors. We mainly used SPSS Clementine’s linguistic methods (extracting, grouping, indexing, etc.) to explore and extract key concepts, generate categories, and help us quickly gain insights from the textual data. We mainly used NVivo 9 software to conduct various query searches and clustering analysis. To understand the specific effects of students’ participation and interaction on their learning achievement (final grade), we focused on two courses with the most questions and chat messages respectively and conducted an in-depth analysis. We decided to narrow down the analysis and selected two courses for an in-depth examination in order to get more specific understanding how students’ participation and interaction may affect student learning outcomes. One course with the most questions (Course A) and one course with the most chat messages (Course B) were selected for the in-depth examination.
- **Dataset:** Two different data sets: - Online questions (student-to-instructor): 3678 questions (from 84 courses posted by 339 students); - Online chat messages (student-to-student): 29.828 chat messages (from 115 courses posted by 825 students). Each question or chat message in the data sets was associated with a course ID, a student name, a date and a time stamp. In addition, we obtained the final grade of these students from the university’s registrar’s office.
- **Metrics:** The unit of analysis in our clustering analysis is based on each post (a question or chat message) from a student. A post can have more than one sentence.
- **Case study:** This study was conducted in a large southeastern university. 1144 LVS students enrolled into 138 courses in a variety of subjects (e.g., accounting, computer engineering, information technology, human services, etc.) in the Fall semester of 2009.
- **Results:** After our data pre-processing, we identified that 298 students posted both questions and chat messages in their respective courses. We noticed that some students either posted questions or chatted with peers but failed to do both. Some students did not post anything in the LVS sessions. The average number of questions posted by a student is 10.8 (standard devia-tion = 13.8; range = from 1 to 118). The average number of questions in a LVS course is 44 (standard deviation = 89; range = from 2 to 548). The main categories/themes, their frequencies and simple descriptions are: - Check-in/check out: 41%; - Social and affective statements (Greetings, giving thanks, and emotional responses): 22%; - Learning/comprehension (Questions regarding course materials and assignments): 15%; - LVS technical issues (Reporting that technical problems): 10%; - Course logistics (Submission deadline, exam schedule, lab schedule, Exam format, assignment requirements, grading, office hours): 9%; - Others. LVS students in Health Sciences, Education and Arts & Letters clearly asked significantly more questions than students in other colleges (including Business & Public Administration, Engineering & Technology, and Sciences). The average number of chat messages posted by a student is 36 (standard deviation = 53.9; range = from 1 to 528). The average number of chat messages in a LVS course is 259 (standard deviation = 335; range = from 1 to 1664). Five main categories/themes were identified from these online chat messages. Due to a technical issue with clustering analysis, we were not able to get a percentage for these themes. - Social and affective statements; - LVS technical issues; - Course logistics; - Learning/comprehension; - Others; Many students used the chat tool to troubleshoot the LVS technical issues and help each other. Many students chose to ask their online peers first when they had LVS related issues such as audio or video problems in-stead of contacting the instructor or technical support. Each student in course A posted nearly 46 questions and 59 chat messages on average during the semester; each student in course B posted about 11 questions and 52 chat messages on average during the semester. No correlation was found between the number of questions and chat messages in the two courses. We found a positive correlation between the number of questions students asked and their final grade for both courses. A negative correlation was found between the number of chat messages and final grade for course B. Students were much more active in interacting with their peers than with their instructors by comparing the numbers of questions and chat messages they created during LVS sessions.
- **Notes:** A graphical LVS student tracking and monitoring tool is needed to extract the recorded data by the system and to generate graphical representations that can be easily explored by instructors to examine various aspects about LVS students. There is a need for instructors to adopt more strategies and ways to encourage participation and engage students to ask questions related to course content learning and comprehension.


## 2014

### ACM
#### **How video production affects student engagement: an empirical study of MOOC videos** - [10.1145/2556325.2566239](http://dx.doi.org/10.1145/2556325.2566239)
- **Source:** Proceedings of the first ACM conference on Learning @ scale conference - L@S '14
- **Problem:** Which kinds of videos lead to the best student learning outcomes in a MOOC?
- **Objective:** To present an empirical study of how video production decisions affect student engagement in online educational videos.
- **Methodology:** This paper presents an empirical study of students’ engagement with MOOC videos, as measured by how long students are watching each video, and whether they attempt to answer post-video assessment problems. The main data we analyze is a video watching session, which represents a single instance of a student watching a particular edX video and whether the student attempted an assessment problem shortly after watching the given video. We segmented the raw logs into video watching sessions based on these heuristics: Each session starts with a “play video” event for a particular student and video, and it ends when: - that student triggers any event not related to the current video (e.g., navigating to another page), that student triggers any event not related to the current video (e.g., navigating to another page); - that student ends the current login session; - there is at least a 30-minute gap before that student’s next event; - the video finishes playing. The edX video player issues a “pause video” event when a video ends, so if a student plays, say, a five-minute video and then walks away from the computer, that watching session will conclude when the video ends after five minutes. It was extracted four main properties from each video: Length; Speaking rate (spoken words by the total in-video speaking time); Video type (an ordinary lecture, a tutorial); Production style:   - Slides – PowerPoint slide presentation with voice-over; - Code – video screencast of the instructor writing code in a text editor, IDE, or command-line prompt; - Khan-style – full-screen video of an instructor drawing freehand on a digital tablet, which is a style popularized by Khan Academy videos; - Classroom – video captured from a live classroom lecture; - Studio – instructor recorded in a studio with no audience; - Office Desk – close-up shots of an instructor’s head filmed at an office desk; Note that a video can contain multiple production styles. To supplement our quantitative findings, we presented our data to domain experts at edX to solicit their feedback and interpretations.
- **Dataset:** Courses: - Intro. CS & Programming: 141 videos, 59.126 students, 2.218.821 watching sessions; - Statistics for Public Health: 301 videos, 30.742 students, 2.846.960 watching sessions; - Artificial Intelligence: 149 videos, 22.690 students, 1.030.215  watching sessions; - Solid State Chemistry: 271 videos, 15.281 students, 806.362 watching sessions; Total of  862 videos, 127.839 students and 6,902,358 watching sessions.
- **Metrics:** Engagement was measured by how long students are watching each video, and whether they attempt to answer post-video assessment problems. These assessment were designed to check a student’s understanding of the video’s contents. In order to comparing the time spent in watching each video, it was computed the median.
- **Case study:** We analyzed data from four edX courses and supplemented our quantitative findings with qualitative insights from interviews with six edX staff who were involved in producing those courses. We analyzed data from four courses in the first edX batch offered in Fall 2012. We selected courses from all three edX affiliates at the time (MIT, Harvard, and UC Berkeley) and strived to maximize diversity in subject matter and video production styles. The edX website logs user in-teraction events such as navigating to a page, playing a video, pausing a video, and submitting a problem for grading.
- **Results:** Students often make it less than halfway through videos longer than 9 minutes. The shortest videos (0–3 minutes) had the highest engagement and much less variance than all other groups: 75% of sessions lasted over three quarters of the video length. Students also engaged less frequently with assessment problems that followed longer videos. The videos for two of our courses (Intro. CS & Programming and Statistics for Public Health) were mostly PowerPoint slideshows and code screencasts. Students usually engaged more with talking-head videos. Students engaged for nearly twice as long on videos filmed informally with the instructor sitting at his office desk, when compared with a video filmed in a multi-million dollar TV production studio. In the first video the instructor seemed more comfortable seated at his office having a personal one-on-one, office-hours style conversation with the video watcher. In contrast, the another video the instructor looked farther removed from the watcher because he was lecturing from behind a podium in a TV studio. Students engaged for 1.5x to 2x as long with Khan-style tutorials. For videos preceding problems, 40% of Khan-style tutorial watching sessions were followed by a problem attempt, versus 31% for other tutorials. Students engaged more with pre-production videos, especially longer ones. Students generally engaged more with videos where instructors spoke faster. Students only watch, on average, 2 to 3 minutes of each tutorial video. Students re-watch tutorials more frequently than lectures.
- **Notes:** This paper have identified six basic types of video production style: 1) classroom lecture with instructor on the blackboard, 2) talking head of instructor at desk, 3) digital drawing board (Khan-style), 4) slide presentation, 5) studio without audience, and 6) computer coding session. The authors do not take in account important features as for example: teaching practice. The dataset could be better if this paper had used a dataset with the same teachers performing all the explored features. To improve external validity, these analyses should be replicated on additional, more diverse courses. The main limitation of some analysis is that they had only one pair of courses to compare, and they differed in instructors and subject matter.

#### **Clustering for improving educational process mining** - [10.1145/2567574.2567604](http://dx.doi.org/10.1145/2567574.2567604)
- **Source:** Proceedins of the Fourth International Conference on Learning Analytics And Knowledge - LAK '14
- **Problem:** The results of Educational Process Mining (EPM) can be used to get a better understanding of the underlying educational processes. However, we have found two problems when using EPM: 1) the model obtained cannot fit well to the general students’ behaviour and 2) the model obtained can be too large and complex for use or analysis by an instructor.
- **Objective:** To use clustering to improve both the fitness and comprehensibility of the obtained models by EPM.
- **Methodology:** We proposed an approach that uses clustering for improving educational process mining. On the other hand, this proposed approach firstly applies clustering in order to group students with similar marks or characteristics. It then applies process mining to discover more specific models of student behaviour. We propose two different clustering/grouping approaches: - Manual: To group students directly using only the students’ marks obtained in the final exam of the course. - Automatic: To group students using a clustering algorithm (provided by Weka) over the student’s interaction with the Moodle’s course. We used all the log data about the 84 students. In the second, we divided firstly the original log file into two datasets: one that contains the 68 students who passed and other with the 16 students who failed. We then applied process mining over the different datasets (all, pass/mark and clusters) in order to discover students’ process models and workflows in each one of these logs.
- **Dataset:** We used all the log data about the 84 students. In the second, we divided firstly the original log file into two datasets: one that contains the 68 students who passed and other with the 16 students who failed.
- **Metrics:** - Time Theory: Total time spent on theoretical contents; - Time Tasks: Total time spent on practical tasks; - Time Forums: Total time spent reviewing forums; - Days Theory: How long students wait to check the content; - Days Tasks: How long students wait to check the task; - Days "hand in": Taking time in hand in the task; - Number of Words in forums: Number of words in forum posts; - Number of sentences in forums: Number of sentences in forum posts;
- **Case study:** The datasets used in this research were gathered from a Moodle 2.0 course used by 84 undergraduate university students taking the Psychology degree in a university in the north of Spain. The study was conducted during two different semesters in the years 2011-2012 and 2012-2013. Students were asked to participate in an eLearning/training programme about “learning to learn”, related to the subject’s topic, completed entirely out of teaching hours.
- **Results:** We obtained three clusters with the following distribution of students: - Cluster 0: 23 students (22 pass and 1 fail); - Cluster 1: 41 students (39 pass and 2 fail); - Cluster 2: 20 students (13 fail and 7 pass); In the dataset “all students”, the students show different behaviour and only have some common actions because there are mixed different type of students (pass and fail students). In the datasets “students who failed and cluster 2 students”, the students show only some common behavioural patterns because this type of student participates/interacts little with Moodle.  In the datasets “students who pass, cluster 0 and cluster 1”, students show much more common behavioural patterns because these types of students are more active users of Moodle.
- **Notes:**

#### **Demographic differences in how students navigate through MOOCs** - [10.1145/2556325.2566247](http://dx.doi.org/10.1145/2556325.2566247)
- **Source:** Proceedings of the first ACM conference on Learning @ scale conference - L@S '14
- **Problem:** MOOCs cannot cater to individual and cultural variabilities in learning style, but instead require every student to be a “self-directed autodidact”.
- **Objective:** This paper contributes a study of students’ strategies for navigating through the learning content in MOOCs, and is, to our knowledge, the first to investigate demographic differences.
- **Methodology:** We analyzed student interaction log data from four different MOOCs provided by edX, comprising 140,546 students from 196 countries. Across all four courses, 78% of students filled out all demographic information. For analyses that require demographics, it was excluded students who did not fill out the relevant field. We conducted multiple linear regression analyses and report their ANOVA F statistics, p-values, and, when applicable, the unstandardized b coefficients for each independent variable in the regression. We do not analyze fine-grained navigation within a learning sequence.
- **Dataset:** - Intro. CS & Programming (6.00x): 65.475 students (50.581 filled out all demographic information), 5.758  certificate earners (4.155 filled out all demographic information); - Statistics for Public Health (PH207x): 31.851 students (28.363 filled out all demographic information), 4.915 certificate earners (4.528 filled out all demographic information); - Artificial Intelligence (CS188.1x): 24.517 students (17.066 filled out all demographic information), 1.900 certificate earners (1.232 filled out all demographic information); - Solid State Chemistry (3.091x): 18.703 students (14.152 filled out all demographic information), 2.072 certificate earners (1.575 filled out all demographic information); Total: 140.546 students (110.162 filled out all demographic information), 14.645 certificate earners (11.490 filled out all demographic information);
- **Metrics:** For each student, we extract the following variables that indicate their motivation and intent for enrolling in MOOCs: - Certificate earned (whether a student earned a certificate of completion or not.);  - Grade; - Coverage (The fraction of total learning sequences (lectures, problem sets, and exams) that the student visited.); - Discussion forum events; It was quantified the following kinds of non-linear navigation:  - Backjumps: The number of times that this student navigated backwards from a learning sequence to another one released earlier in the term (e.g., from Lecture 6 to Lecture 4), divided by the total number of sequences visited by this student; - Textbook events: The number of times that this student accessed the digital textbook associated with the course, divided by the student’s total number of courseware access events.
- **Case study:** The data we extracted from four courses in the first edX batch offered in Fall 2012.  We selected courses from all three edX affiliates at the time (MIT, Harvard, and UC Berkeley). To maximize diversity in subject matter and student population, we selected an introductory computer science course (6.00x), an advanced computer science course (CS188.1x), statistics for public health (PH207x), and solid state chemistry (3.091x).
- **Results:** The mean student age across all four courses in our data set was 28 years (sd=9.4). Most students (77%) were between 20 and 40 years old, with 13% under 20 and 10% over 40. The most common highest education level for students participating in these four courses was a bachelor’s degree (38%), followed by a high school diploma (28%). However, students who earned a certificate most commonly held a master’s degree (37%), followed by a bachelor’s degree (32%). Most students in these four courses were men. Students from all 196 countries (plus 10 additional dependent territories such as Guernsey) participated in the four courses. Students from 157 countries ended up getting certificates. Students from countries with lower student-teacher ratios seem to participate in MOOCs later in life than students from countries with higher student-teacher ratios (i.e., larger class sizes). One possible interpretation is that people from countries with high student-teacher ratios are more likely using MOOCs to supplement their regular education, whereas those from low student-teacher ratio countries – U.S., Canada, Western Europe – are more likely to be adult lifelong learners. Certificate-earning students viewed, on average, 67% of the learning sequences in 3.091x (solid state chemistry), 77% in PH207x (statistics for public health), 81% in CS188.1x (advanced computer science course), and 86% in 6.00x (introductory computer science course). Thus, students ignore, on average, 22% of the materials in those courses, yet still earn certificates. Certificate-earning students from countries with higher student-teacher ratios usually visit fewer learning sequences. In all four courses, students who viewed more materials were more likely to achieve higher grades, which makes sense because they had more opportunities to learn and to get assessed (i.e., higher “time-on-task”). Older students participate more in forums. Students with Ph.D. degrees participated up to 32% less in forums than non-Ph.D. holders. Certificate earners in our four courses performed an average of 1.04 backjumps for every learning sequence they visited. This behavior indicates that students apply non-linear navigation strategies on their way to earning certificates. In contrast, students who did not earn certificates performed only 0.3 backjumps per visited sequence. Students most frequently backjump from an assessment to a lecture, which shows that they might be opportunistically looking up specific information needed to answer assessment questions. The second most prevalent kind of backjump is between two lectures, potentially when students are re-watching old lectures for more in-depth learning. Older certificate-earning students backjump more frequently. Certificate earners above 40 years of age performed 5% to 12% more lecture-to-lecture backjumps than those under age 20. Also, those above 40 years old performed 2% to 11% fewer assessment-to-lecture backjumps than those under 20 years old. A higher student-teacher ratio corresponds to fewer backjumps. The country where students backjump most frequently is Greece, with a low student-teacher ratio of 10.  In comparison, students from Kenya (with the highest student-teacher ratio of 47) have a significantly lower number of backjumps. Older students are more likely self-selected to be independent, self-directed learners, so they might not conform to the general trends of the educational systems in their home countries. Men performed fewer backjumps than women. Older students and those from countries with a low student-teacher ratio perform more backjumps. One possible explanation is that this behavior is emblematic of more independent learning and their preferences for non-linear navigation.
- **Notes:** This paper found that most students employ non-linear navigation strategies through MOOCs, and it was observed differences between demographics, most notably age and country of origin. Furthermore, certificate earners in the four edX courses on average do not visit 22% of course learning sequences. Older certificate earners cover more course materials and repeat more lecture sequences than younger students. This behavior suggests that they follow non-linear, self-defined learning paths. Older students use more the course digital textbook than younger students. A central limitation of this study is that by just analyzing log data, we cannot directly measure students’ true motivations, engagement, intent for enrolling in a MOOC, or their knowledge gained after passing the course.

#### **Identifying Latent Study Habits by Mining Learner Behavior Patterns in Massive Open Online Courses** - [10.1145/2661829.2662033](http://dx.doi.org/10.1145/2661829.2662033)
- **Source:** Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management - CIKM '14
- **Problem:** Understanding how students interact with MOOCs is a crucial issue because it affects how MOOC engineers and instructors design future online courses.
- **Objective:** We propose to leverage topical N-gram models to (1) capture the typical combination of learning activities and (2) identify frequent learning activity sequence which can suggest learning strategies.
- **Methodology:** We apply topical N-gram models to our learning session modeling task where we treat each learning activity defined during pre-processing as a word and each session as a document. Then we can characterize learning sessions with topic proportions. We select at most the top 20 behavior patterns for each session type instead of using all the mined behavior patterns. We manually check the mined behavior patterns.
- **Dataset:** - Virtual Instruction: 20.372 users, 76.770 sessions and 2.112.857 clicks; - Financial Planning: 112.318 users, 469.238 sessions and 10.073.997 clicks.
- **Metrics:** We extract 18 types of learning activities from the trace data. In particular, we distinguish between submitted quizzes, assignments, peer assessments and those that are attempted but not submitted. We also differentiate passive activities such as browsing and active activities such as publishing a post in the forums.
- **Case study:** Our study was performed of two Coursera MOOCs, one is about virtual instruction(VI) and the other one is about personal financial planning(FP). They were both offered in 2013. Final grades in each course: - VI: 17.562 <= 10% (none), 1.418 between 10% and 60% (fail), 507 between 60% and 90%, 881 >= 90%; - FP: 71.522 <= 10% (none), 8.768  between 10% and 60% (fail), 1.025 between 60% and 90%, 2.918 >= 90%;
- **Results:** In both MOOCs, The None achievement users are characterized with a flat distribution across the five learning session topics, which means they are insensitive to course schedules and deadlines. The other three achievement groups reflect the course schedule in different levels. Towards the assignment deadline, which is the end of week 3, an even higher proportion of the sessions are Assignment and Forum. Similar trends can also be observed for peer-assessment and final quiz. It is interesting to compare the trends between Fail and Pass, they have similar trends except that Fail users mostly do not have the clear Peer-Assessment bump in week 4. This may largely be due to the fact that they did not finish their own assignments so they cannot do peer-assessment. If we compare Pass and Distinction users, we can see that Pass users tend to “procrastinate” towards a deadline, as much more of their Assignment or Final Quiz related sessions are in the later part of the week.
- **Notes:**

#### **What does enrollment in a MOOC mean?** - [10.1145/2556325.2567882](http://dx.doi.org/10.1145/2556325.2567882)
- **Source:** Proceedings of the first ACM conference on Learning @ scale conference - L@S '14
- **Problem:** The number 150.000 students was mentioned for both Stanford’s Artificial Intelligence course and MIT’s Circuits and Electronics. But what kind of enrollment do these large numbers really show?
- **Objective:** In this paper we will only focus on when and how often students showed up for the course.
- **Methodology:** By parsing the dates and event types of every user in the database, we are able to find out when, how often, and what they did in every website visit.
- **Dataset:**
- **Metrics:** When and how often students showed up for the course.
- **Case study:** For this study, the author was given access to anonymized data from two iterations of MITx 6.00x – Introduction to Computer Science and Programming. Each course ran over a 4-month period: Fall 2012 (Oct 2012 - Jan 2013); Spring 2013 (Feb - May 2013). The large number of sign-ups was estimated to more than 184.000 students.
- **Results:** The largest group of users (84,853 users or 46%) had a single visit to the website in Fall 2012, which corresponded to landing on the info page after clicking the “Sign-up” button. Another large group is that of “one-day visitors”. This group consisted of 19,035 (Fall 2012) and 21,615 (Spring 2013) such users, who spent a median time of eight consecutive minutes on the site. The majority—78% of all users (excluding “no-shows”)— visited between 1 to 10 days during the duration of 112 days. While the Fall 2012 offering attracted 184K users, only about 50K showed up in the first week of the course. Meanwhile, the Spring 2013 offering (only three weeks after the completion of Fall 2012), attracted 90K users. This suggests that successive offerings of the same course might be “less” massive. For a course such as 6.00X Fall 2012, with 184,234 signups, where 46% of users never showed-up, the number of students who attempted the final exam (7,559) is really small, only 4%. However, if we base calculations on who showed up for the course on at least 1/4 of its duration (on four different weeks during 16 weeks; there are 35,173 such users, refer to online plots), the completion rate increases to 21%. Real engagement might be found in the group of 7.161 students, who showed up for more than 40 days during the entire course duration, and all attempted the final exam.
- **Notes:**

#### **Educational technology approach toward learning analytics** - [10.1145/2567574.2567594](http://dx.doi.org/10.1145/2567574.2567594)
- **Source:** Proceedins of the Fourth International Conference on Learning Analytics And Knowledge - LAK '14
- **Problem:** (1): Total login frequency in LMS; (2): Total studying time in LMS; (3): Regularity of learning interval in LMS; (4): Number of downloads; (5): Interactions with peers; (6): Interactions with instructor; These variables predict learners’ academic achievement?
- **Objective:** The purpose of this study is to suggest more meaningful components for learning analytics to help learners improving their learning achievement continuously in terms of educational technology approach.
- **Methodology:** Variables for this study were computed by automatic data collection module embedded in the LMS. Final grades were collected as a dependent variable in this study. Data matching process between independent variables and final grades was executed automatically in the database system. Multiple linear regression analysis was conducted to develop a model for predicting students’ academic achievement.
- **Dataset:**
- **Metrics:** (1): Total login frequency in LMS (Adding up the number of individual student’s login time into the LMS); (2): Total studying time in LMS (Calculating the total amount of time spent between login and logout); (3): Regularity of learning interval in LMS (Calculating the standard deviation of average login time into the LMS); (4): Number of downloads (Adding up the numbers of course materials downloaded); (5): Interactions with peers (Counting the total number of student’s postings responding to peers); (6): Interactions with instructor (Counting the total number of student’s postings responding to instructor);
- **Case study:** This study were 84 undergraduate students in a face-to-face course entitled ‘Understanding of Science of Public Administration’. 20% of the final grade was assigned for online discussion participation in LMS. All participants are female students in a women’s university in South Korea.
- **Results:** The six-predictor model was able to account for 33.5% of the variance in final grade. Total studying time in LMS and interaction with peers had a significant (p < .05) correlation with final grade. In addition, (ir)regularity of learning interval in LMS and number of downloads had significant (p <. 10) partial effects in the full model. However, total login frequency in LMS (ß=-.131, t=-.980, p > .05) and interaction with instructor (ß=-.035, t=-.344, p > .05) did not predict final grade.
- **Notes:** Blended learning.

#### **Visualizing patterns of student engagement and performance in MOOCs** - [10.1145/2567574.2567586](http://dx.doi.org/10.1145/2567574.2567586)
- **Source:** Proceedins of the Fourth International Conference on Learning Analytics And Knowledge - LAK '14
- **Problem:** In the context of MOOCs, current access to visualizations of learners’ data through the more recent MOOC platforms – e.g. edX, Coursera – is somewhat limited.
- **Objective:** The two main purposes of the research were to develop more refined learning analytic techniques that can be used with MOOC data, and to design visualizations of the output that is meaningful to end users (instructors, researchers, students and administrators).
- **Methodology:** We have adopted an approach that involved an iterative analysis of analytics data. This process involves making a series of repeated passes at the data, with each pass involving further refinement in analysis at a finer level of granularity. On each pass, patterns are observed in the data and those patterns are used to refine the analysis focus. By repeating this process several times, it is possible to cluster either variables into groups and/or student users into subpopulations to gain further insight. We ran a linear regression model for each course considering the association between students’ marks at the two week point of the course and their final grade. For a consistent comparison between the courses, we elect to focus on the students’ marks at the two week point in both courses. Qualified students are students who obtained marks above 60% in the first two weeks of the course and they have invested the time required to complete the assignments. We investigate students’ temporal engagement with the course material. To investigate this question, we introduce another visualization, called a state transition diagram. We will show how the state transition diagram combined with an analysis of student subgroups can clearly illustrate the differences in students’ temporal engagement in the two courses. It is possible to study students’ interactions and engagement at both a macro-level (e.g. how many students completed an assignment) and a micro-level (e.g. a click-stream, tracking every interaction a student has with the learning platform). It was used the state transitions diagrams in the video view from both coursed by the qualified and non-qualified student groups.
- **Dataset:**
- **Metrics:** (1) Auditors, students who watched videos in a particular week, but did not participate in any assessments; (2) Active, students who participated in an assessment in a particular week; (3) Qualified, students who watched a video or participated in an assessment and met the two assessment qualification criteria;
- **Case study:** This paper examines the interactions of students in the inaugural sessions of two MOOCs developed at the University of Melbourne: - Principles of Macroeconomics: The inaugural session of this course attracted the interest of 54,217 students, of which 32,598 started the course, and 1,412 students received a certificate of completion for the course (a 4,33% completion rate); - Discrete Optimization: The inaugural session of this course attracted the interest of 37,777 students, with 22,731 starting the course, and 795 receiving a certificate of completion for the course (a 3,50% completion rate).
- **Results:** A preliminary assessment of the analytics in both courses indicates that (1) there are many more students viewing the videos than working on the assignments; (2) there is a noticeable and consistent decline in the number of students participating in the course every week; (3) of the subgroup attempting the assignments, the rate of overall success in the course is quite low, with more than half of the students in the first bar of the histogram. In the first two weeks many students experiment with the assignments but only a dedicated subgroup (less than 50% in these courses) continues to work on the assignments for the entirety of the course. Insights revealed: (1) The relative proportions of auditor, active, and qualified students are remarkably similar across both courses, despite the substantial differences in the designs of the courses and their target students; (2) Of the students who are still engaged in the course, the relative proportion of qualified students is broadly maintained over time, unlike the population of active students, which decreases steadily; (3) The reduced number of active students in weeks five and seven of Macroeconomics suggests that some students only complete the exams and skip the quizzes; and (4) The relative proportion of students who are discontinuing across both courses came from the Active group; In regard videos view: (1) Comparing across the qualified and non-qualified groups, we can see that the quantity of non-qualified students is greater than the qualified group. Hence, we have a very different insight into video viewing behavior when the qualified group is an-alyzed in isolation; (2) The transition behavior of the non-qualified group is remarkably similar across the two courses; and (3) When the qualified groups are compared across the two courses, it appears that these students switch between video topics in both courses more than non-qualified students, however, the switching behavior is more pronounced in Discrete Optimization; Comparing the assignment transitions between these courses some of the key observations are: (1) When qualified and non- qualified groups are compared, again we can see that the number of non-qualified students is far greater than qualified students. Once again, this allows for a very different insight into assignment submission behavior of students when the qualified group is analyzed in isolation; (2) The behavior of the non-qualified group of students is similar in the two courses. However, it is clear that a subgroup of non-qualified students in Macroeconomics prefer to skip the practice quizzes and only take the exams; Further, it seems that many students repeatedly revised their assessments.
- **Notes:** The main contribution is the visualization used in the analysis. However, there is no evaluation about it.

#### **A primary criticism of MOOCs is that their completion rate is very low.** - [10.1145/2556325.2566240](http://dx.doi.org/10.1145/2556325.2566240)
- **Source:** Proceedings of the first ACM conference on Learning @ scale conference - L@S '14
- **Problem:** A primary criticism of MOOCs is that their completion rate is very low.
- **Objective:** This paper addresses both how students’ goals at the course outset affect their completion and dropout rates as well as the effect of students’ skills on their success in the course.
- **Methodology:** During registration, students were asked to complete a questionnaire about their course goals and previous experience with skills addressed in the course. To assess whether students achieved the goal they established, we sent them an anonymous follow-up survey as well as conducted a clickstream analysis of their behaviors. The clickstream analysis enabled us to analyze the percentage of students who selected each goal and whether their behaviors indicated that they achieved, then we assessed how many videos they watched and how many activities they completed. To assess students’ perceptions of how well they achieved their goals, all 41,455 course registrants received an anonymous post-course survey not aligned to student identifiers in the rest of the course.
- **Dataset:** 97% of registrants (40,248 out of 41,445) provided a goal at registration.
- **Metrics:** The four categories we have observed include: - No-shows: students register for the course but never log in to the course to interact with the content; - Observers: want to see what an online course is like or how this one is taught; - Casual learners: want to learn one or two new things; - Completers: complete as many course elements necessary to complete projects and earn a certificate;
- **Case study:** Mapping with Google was created to teach the general public how to use Google’s mapping and Google Earth products more efficiently and effectively. Students could access instructional materials in the two-week period from June 10 through June 24, 2013. 41,455 students registered for the course; 21,837 students (53% of registrants) did something in the course other than register (e.g. watched a video, looked at a text lesson, attempted an activity, completed a final project). Students could choose whether to complete a final project to earn a Google Maps certificate of completion, a Google Earth certificate of completion, or both. Final projects required students to apply skills taught in the course to create a custom map or Google Earth tour.
- **Results:** We discovered that 52.5% of registrants intended to complete requirements to earn a certificate and  44.7% of registrants who supplied a goal preferred to learn a few new skills or explore the online course. 2.881 (7%) of registrants responded to the achieved survey. 2.258 (78%) of survey respondents indicated that they met the goal they had set at the beginning of the class. Of 20,977 engaged students (all categories except no-shows) a total of 11,348 (54.1%) met or exceeded their goal via behavioral analysis. Half of students who register for a course never return to the course (no-shows). More students completed activities than watched videos or read lesson text. We considered a student to complete an activity if she correctly answered the corresponding application activity questions. We verified with a Wilcoxon signed-rank test that there were significantly more unique students who completed activities than viewed lessons. Compared the number of students who possessed relevant skills at the beginning of class, we found differences of less than 1% in the completion rate; we confirmed that the differences were not significantly. Students without each specific skill and completed the activity had an overall greater rate of completing the course than students who merely watched videos or clicked on text lessons. It seems that completing activities is a greater predictor of students completing the course than what skills students possess when they enter the course.
- **Notes:** Understanding students’ goals enables course designers to change how courses are presented. An important result of this reasearch: "It seems that completing activities is a greater predictor of students completing the course than what skills students possess when they enter the course."

#### **Measuring learned skill behaviors post-MOOC** - [10.1145/2559206.2581180](http://dx.doi.org/10.1145/2559206.2581180)
- **Source:** Proceedings of the extended abstracts of the 32nd annual ACM conference on Human factors in computing systems - CHI EA '14
- **Problem:** Will the design innovations of MOOCS make a change?
- **Objective:** It were to deliver the course material to a large number of students and to measure the quality of the students’ learning.
- **Methodology:** Beyond to develop a MOOC, we analysed the successful completion. Students were invited to share their data from the moment of registration and for a two-week period after course completion. A proxy measure of advanced-search behavior data was collected for those registrants who agreed to have their search sessions analyzed in aggregate.
- **Dataset:**
- **Metrics:** The metric was a count of how many “advanced search features” (ASF) were used during a single search session. (Basically, the metric counts the use of features such as double-quotes, search filters site: and filetype:, along with UI features such as date restrict and color filtering.)
- **Case study:** We began developing our MOOC on May 15, 2012, and launched the class on July 10 and ran for 2 weeks, with class content released on Tues / Wed / Thurs of each week, with all work needed to be completed by end-of-day July 23, 2012. This MOOC, had the goal of teaching basic search skills to a broad audience of internet-search-interested people. We had 154,367 students sign up for the class.
- **Results:** The course retained 14.6% of the original registrants. Of the 27,278 students that took the midterm exam, 92.7% scored a passing grade of more than 70% correct (26,237). Anyone who enrolled in the course is already (pre-course) rated at 0.4 ASF. The non-completers have increased their advanced-search measures a bit, but quickly fall back down to their pre-course levels. Students who will go on to complete the course are on a rapid increase in their advanced search skills use. Their scores rise to 1.6 for the latter part of the course. The most rewarding part, however, is in the advanced-search metric for the two weeks post-course. For the completers cohort, the metric is 0.82 ASF— exactly double what they were measured at when they entered the course.
- **Notes:**

#### **Correlating skill and improvement in 2 MOOCs with a student's time on tasks** - [10.1145/2556325.2566250](http://dx.doi.org/10.1145/2556325.2566250)
- **Source:** Proceedings of the first ACM conference on Learning @ scale conference - L@S '14
- **Problem:** There is hope that it may be possible to find out which activities are the most useful for learning. Can we correlate increased expenditure of a student’s time on particular course improvement and resources with score, skill, relative skill learning?
- **Objective:** We are especially interested in how student study habits, such as time on task, influence skill change in our longitudinal study over the semester or class.
- **Methodology:** Time estimation for each participant involves measuring the durations between a student’s initial interaction with a resource and the time when they navigate away.  We accumulate durations calculated from each participant’s time-series for each separate course component (Homework, Book, Discussion Forums, etc.). IRT (Item Response Theory) was designed to measure student ability (skill) on standard tests. A student’s IRT ability is relative to “class average” and is measured in standard deviations of the class, using the z-score (equal to difference from class average divided by standard deviation of the class). An improvement over a series of weekly IRT scores for a particular student shows the student’s relative improvement (relative to the overall class average). We quantified this relative improvement, as the slope of the regression line fit to a student’s weekly combined skill on homework and tests/quizzes, where the class’ average weekly skill was 0. An improvement over a series of weekly IRT scores for a particular student shows the student’s relative improvement (relative to the overall class average). We quantified this relative improvement, as the slope of the regression line fit to a student’s weekly combined skill on homework and tests/quizzes, where the class’ average weekly skill was 0. We used two approaches to look at the relationships between initial skill, average skill, and relative improvement with course behaviors while taking into account demographics: (1) use multiple regression models including demographics and course behaviors; (2) calculate the correlations between the variable of interest for various cohorts. The demographic information included the highest degree earned, experience with the subject material, math background, whether the student was also a teacher of the subject, and whether the student worked through the course on his/her own, with a classmate or with an expert.
- **Dataset:** In total, 6.002x generated roughly 230 million logged interactions, while in 8.MReV there were roughly 17 million interactions. In the tracking logs, each interaction (click) contains the following relevant information: username, resource id, interaction details, and a timestamp. Interaction details are context-dependent, e.g., correctness of a homework problem submission, body text of a discussion post, page number for book navigation.
- **Metrics:** We don’t count durations less than 10 as activity. We don’t accumulate durations over 30 minutes, assuming that the user has disengaged from their computer.
- **Case study:** We investigate two courses: 6.002x (Circuits & Electronics), and 8.MReV (Mechanics ReView), a mechanics course for students with at least a high school level of preparation. Both courses were offered on the edX.org platform. There were 7519 certificate earners in 6.002X, of those 6060 were used in the analyses in this paper because they had completed more than 50% of the homework and exam questions. About the users, 35% a bachelor’s degree was the highest degree held, 38% had a masters degree or doctorate, for 25% the highest degree attained was high school; 12.6% taught electrical engineering at some level, 73% of the respondents had some exposure to electrical engineering. In 8.MReV a total of 1080 students completed more than 50% of the questions in the course, and of these 1030 received certificates of completion. 17.706 did not earn certificate. Of the 1080 students we analyzed, 750-900 students responded to the various demographic questions. Of the respondents, 25% were physics teachers, 35% held masters degrees or doctorates, 25% were high school seniors or college students, 76% had a college-level math background, 38% had a physics background equivalent to 8.MReV and 33% had a background in physics more advance than 8.MReV.
- **Results:** Those self-identifying as teachers had initial and average skills significantly greater than both non-teachers and those who did not respond to the question about teaching. Having a college math background was associated with better initial and average skills when compared with those without a college-level math background. Differences between the skills of PhD holders and all other groups, except those with less than a high school education were significant, probably due to small sample size (8%). Additionally, the initial and average skills of Master’s degree holders were significantly different than those with only a high school education, a Bachelor’s degree, or those who did not respond to the education question. Those who spent more time on any of the instructional activities showed lower skill average wherever there was a significant correlation. More time on any resource correlated with negative relative improvement, except that homework time correlated with a slight increase in relative improvement. Students with more skill can get enough points to obtain a certificate with less investment of their time. The more skillful students can finish problems and labs and obtain enough points to earn a certificate with significantly less expenditure of time. The negative correlation between spending time on the lecture videos or lecture questions and relative improvement may reflect that students who are struggling to keep their skill level constant revert to the lecture videos and questions in an attempt to prevent falling further behind. It was noted that 92%of the visits to the discussion forum are made by students who simply read existing threads and that such visits constitute the most frequently consulted resource of students doing homework problems suggesting that the discussion forum functions as a way to obtain instruction and information about solving the homework and labs.
- **Notes:**

#### **Is the LMS access frequency a sign of students' success in face-to-face higher education?** - [10.1145/2669711.2669912](http://dx.doi.org/10.1145/2669711.2669912)
- **Source:** Proceedings of the Second International Conference on Technological Ecosystems for Enhancing Multiculturality - TEEM '14
- **Problem:** In base of all the former, the research question in this paper focused on finding out to what extent students’ interactions with a LMS can provide an evidence to predict their success or failure at the end of the course.
- **Objective:** The goal of this study is to find out whether one or more variables can predict academic marks, discovering what variables may be related.
- **Methodology:** Having selecting “view actions” as the  activity more correlated with the grades, the second step focused on the longitudinal analysis of these “view actions” for all the students in the data set, dividing the dataset according to the academic year. The longitudinal analysis builds up time series of the students’ access to the platform and applies alignment of time series and clustering, in order to classify the students according to their temporal behaviour. In our experiment, the clustering is done comparing time series (frequency of students’ access over time), whose similarity built up the pairwise similarity matrix. We construct this matrix by applying the well-known Dynamic Time Warping (DTW) method. We selected Pearson correlation to calculate the correlation between the total number of access and the final grade of each student in an academic year. We have divided the events into three groups: events related to see some contents or class notes, events related to the forums activities and events related to quizzes. Then, we have calculated the correlations between each group of events and the final marks for each year. We apply a time series alignment and hierarchical  clustering for the longitudinal analysis of “View Contents” in the platform. We have look for a temporal pattern of these groups (clusters) using temporal series decomposition. To detect students who are in risk of failing the subject, we consider students in three groups: students whose final grade is higher than 6, students whose final grade is between 4 and 6, and students whose final grade is lower than 4.
- **Dataset:** We collected: final grades and interactions. In our dataset, we have collected 173.890 interactions in 2011/2012, 225.018 in 2012/2013 and 205.140 in 2013/2014, i.e. 604.048 interactions in total.
- **Metrics:** Frequency of students’ access over time.
- **Case study:** The exploratory analysis of a real dataset obtained from the official e-learning Moodle-based platform at the University of Vigo. This dataset gathers all the online activities, during three consecutive academic years, of full-time students at the 2nd year in a telecommunication subject performed to course of the Telecommunication Engineering Degree. This face-to-face course (where the LMS acts as a  complement of in-person classes) is annually given in 20 weeks (from September to January), considering two weeks without classes because of the Christmas holydays. We have collected data of three consecutive academic years, from 2011 to 2014. A total of 419 students have enrolled this course during the three academic years. The students who dropped the course were not considered because the reasons behind this decision are totally unpredictable. In this specific course, the forum is only used by professors or tutors to announce some news, so in this case, the interaction among students and student-professor using the forum is not feasible.
- **Results:** Just using Pearson correlation,  we noted that there is not any event which has a correlation upper to 0.5, so there is not any event which could serve as a clear predictor of the final grade. We can observe that the most correlated group of events is the “View Contents” one. In our course, the events related with forums have the lowest correlation rate with the final grade. This can be explained by the fact that only professors can write in the forum, which is used as a noticeboard. On the other hand, this course only has mandatory “Quizzes”, so all students are obliged to fill in them. For this reason, quizzes events do not provide valuable information. There is a clear difference between the temporal series of students with the highest marks (more than 8 of final grade) and the temporal series of students with the lowest marks (less than 2 of final grade). Some peaks on the number of access that occurring some days before the test day. This peaks usually appear before in the successful students graph than in the unsuccessful study. Students with highest grades enter to the platform more times than students with lowest grades. Students with similar behavior would have similar trend component, no matter if there were a bit difference between the day and/or the number of access of both students. We present a trend graph of groups whose final grade is higher than 6, students whose final grade is between 4 and 6, and students whose final grade is lower than 4. The trend is very different in each group.
- **Notes:**

#### **National differences in an international classroom** - [10.1145/2567574.2567602](http://dx.doi.org/10.1145/2567574.2567602)
- **Source:** Proceedins of the Fourth International Conference on Learning Analytics And Knowledge - LAK '14
- **Problem:** Researchers and policymakers who work with developing countries have criticized the capacity for MOOCs to deliver learning opportunities. They note that MOOCs presume a stable internet connection and bandwidth to access video links, that they cater to students who already have the wherewithal to succeed in formal educational institutions, and that the perspective of developing country leaders is lacking. “Are there significant differences in the overall performance of students from different countries?”
- **Objective:** We investigate country-level differences in performance and the distribution of achievement for those students who participated in the first edX course.
- **Methodology:** Our attention on four groups of countries with high participation—English-speaking high-income countries, Hispano—and Lusophone middle-income countries, low-middle income countries in South Asia, and non-English high-income countries. We observe students from 194 countries with a diversity of backgrounds and prior experiences. We then conduct ANOVA of student achievement and proportion of certificate earners by region and by the four country groups of interest. We apply a robust test of the equality of variances to investigate the distribution of performance and completion overall and between our four country groups of interest, which we illustrate graphically. We show that differences persist in a predictive model; We include country characteristics such as GDP per capita and a dummy variable for the country being predominately English-speaking, and we control for individual behaviors such as time spent on online homework problems or lecture videos.
- **Dataset:** The data we analyze come from the clickstream logs of students’ interactions with the site.
- **Metrics:**
- **Case study:** Our sample comprises students who participated in the first edX MOOC, “Circuits and Electronics,” which was offered in the spring of 2012.
- **Results:** We confirm that there is a great deal of heterogeneity within each national group of students in 6.002x. We also observe significant differences in overall achievement and in the proportion of students who receive certificates between countries, whether or not we exclude students who do not score any points. There appear to be systematic differences between country groups in the mean, variance, skew, and kurtosis of achievement in 6.002x as well as in the performance of those students who score any points and those who earn a certificate. This systematic difference is statistically significant. Overall achievement has no significant relation with GDP per capita. Higher achievement is related, counter-intuitively, to being in a non-English speaking country. This result persists even when high-income, high-scoring OECD countries such as France are excluded. This suggests that the profile of students in English-speaking countries such as group (1) are more diverse, while those in non-English speaking countries group (2) come from an already- high achieving group of students who know about and can engage with a college-level class entirely in English. Further, students from group (4) show a larger, flatter spread of scores with a notably higher mean.
- **Notes:**

#### **Visual analytics of MOOCs at maryland** - [10.1145/2556325.2567878](http://dx.doi.org/10.1145/2556325.2567878)
- **Source:** Proceedings of the first ACM conference on Learning @ scale conference - L@S '14
- **Problem:** Low success rates of MOOCs in general.
- **Objective:** We wanted to better understand student behavior and its relationship to student success.
- **Methodology:** Students identified four prototypical trajectories of engagement, summarized as “auditing” (engaged in the course, but not submitting assessments), “behind”, “on track”, or “out”.  Using an interactive tool such as Tableau enabled us to experiment with many variations of what data to plot and how to visually arrange them. We used “multiple miniatures”, created heat maps, etc.
- **Dataset:**
- **Metrics:** Students identified four prototypical trajectories of engagement, summarized as “auditing” (engaged in the course, but not submitting assessments), “behind”, “on track”, or “out”. We explore correlations with course final grades.
- **Case study:** We use visual analytics to explore participation in five MOOCs at the University of Maryland. We analyzed the standard Coursera data available to each course instructor. Here, we show illustrative data from the course titled “Genes and the Human Condition (From Behavior to Biotechnology)”, which had 23,564 registered students in its first offering during April/May, 2013.
- **Results:** There are three distinct clusters of students. We hypothesize that these clusters correspond to the “auditing” (at the bottom), “behind” (in the middle) and “on track” (on top) prototypical trajectories. The second pattern is that the shape of the clusters varies by student activity. One receives a good grade without submitting quizzes. We also see that students receive good grades without participating in the forums. The middle plot shows that students receive good grades with widely varying numbers of lecture views. These results show that there are multiple ways to be successful in a course, and it is perhaps not necessary to “do everything” – at least, not for everyone. Based on a quantitative analysis of student activities, we did notice some subtle differences in certain types of activity. For example, active forum participants are much more likely to be hobbyists and academics than students, industry professionals, or research scientists. And consistent with the literature, forum participants are over twice as likely to be men than women.
- **Notes:**

#### **Engaging with massive online courses** - [10.1145/2566486.2568042](http://dx.doi.org/10.1145/2566486.2568042)
- **Source:** Proceedings of the 23rd international conference on World wide web - WWW '14
- **Problem:** There have been relatively few quantitative studies of MOOC activity at a large scale, and relatively little understanding of differentways in which students may be engaging with MOOCs
- **Objective:** In this paper we propose a framework for understanding how students engage with massive online courses.
- **Methodology:** We find first of all that each student’s activities in a course can be usefully described by one of a small number of engagement styles. Our second main focus in this paper is the course forums, and how to increase engagement in them. We computed the assignment fraction for each student and presented in histograms. These way, five styles of engagement were found: Viewers, Solvers, All-rounders, Collectors, and Bystanders. Our analysis of engagement styles also relates to the issue of students’ performance, grades and use of forums. In order to shift the level of student activity, we introduce badges into the third offering of Coursera’s Machine Learning class (ML3). Our badges were based on milestones for reaching certain activity levels based on contributing to threads, reading content, and voting on content. When a student reached one of these milestones she received a badge. We awarded simple cumulative badges for reading certain numbers of threads, and voting on certain numbers of posts and comments, but instead of cumulative badges for authoring posts and threads, we instead implemented great achievement badges to incentivize high-quality content, which were awarded for authoring posts or threads that were up-voted a certain number of times by other forum members. We also created cumulative great achievement badges to reward users who consistently authored high-quality content, as judged by their peers. The “Early Bird” badge was given for participation in the forums in the early days of the class, to help the forums get off to a good start, and the “All-Star” badge was given to users who were active on the forums during every week of the class. We were also interested in whether different ways of making the badges salient to the students could produce different incentives and hence different levels of forum activity, so we developed a randomized experiment that presented the badges differently to different sub-populations of the students.
- **Dataset:**
- **Metrics:** We consider the two fundamental activities of (i) viewing a lecture and (ii) handing in an assignment for credit. Our classification of students into five engagement styles is based on the total number of assignment questions they attempted and lectures they consumed. It is striking that the histogram for each course has three natural peaks: - Viewers, in the left mode of the plot, primarily watch lec tures, handing in few if any assignments; - Solvers, in the right mode, primarily hand in assignments for a grade, viewing few if any lectures; - All-rounders, in the middle mode, balance the watching of lectures with the handing in of assignments; There are two other types of students who are not apparent from the three modes of the histogram: - Collectors, also in the left mode of the plot, primarily download lectures, handing in few assignments, if any. Unlike Viewers they may or may not be actually watching the lectures; - Bystanders registered for the course but their total activity is below a very low threshold.
- **Case study:** Based on quantitative investigations of student behavior in several large Stanford University courses offered on Coursera:  three successive offerings of Machine Learning (which we name ML1, ML2, and ML3) and three successive offerings of Probabilistic Graphical Models (which we name PGM1, PGM2, and PGM3).
- **Results:** We find in all the classes that most students receive a grade of zero; however, the fact that many Viewers spend a non-trivial amount of time watching lectures means that a grade of zero should not be equated with a failure to invest any effort in the course. We see an intriguing difference between the ML and PGM classes; the PGM classes, which are more challenging, include students who do all the coursework but have wide variation in their grades, whereas in the ML classes we find that a student’s grade has a more linear relationship to the number of assignments and lectures. Our analysis shows that threads in the course forums proceed in a relatively “straight-line” fashion; each thread grows much more through the arrival of brand- new contributors than through repeated interaction among the initial contributors. Moreover, we find an interesting pattern in which the activity level and average grade of the student making the initial post is substantially lower than that of the students making subsequent posts. This suggests that we may be observing a dynamic in which better students are helping out others in the class by joining the threads they initiate. The use of badges in ML3 course had significantly higher engagement compared to earlier offerings of Machine Learning (ML1 and ML2) in which badges were not used. We analyze the distribution of the number of actions users took on ML3, and compare it to the same distributions in previous runs of the class, ML1 and ML2, neither of which had a badge system in place. If badges incentivized users to be more engaged in the forums, then we should observe a shift in the distribution of the number of actions in ML3 toward a heavier tail. So, we cannot necessarily conclude that the differences exhibited by ML3 relative to ML1 and ML2 are the result of the badge system, but a close look at the distinctions between the courses provides evidence for the impact of badges. Notice that the grade distributions in the courses are heavily skewed; out of the 60,000 students who registered for ML2, two-thirds of them (≈40.000) get a final grade of 0, 10% of students (≈5.000) achieve a perfect score, and the remaining 20% of students (≈10.000) receive a grade in between these two extremes. The distribution of grades in PGM2 is even more skewed: there is no large contingent of students with a perfect grade (only two students achieve this distinction), and only 10% (3.300 out of 35.000 students) achieve a non-zero grade. In both classes there is a large number of students who achieve a score of zero in the course. However, this doesn’t mean that these students are not engaging or putting effort in the course. The grade is generally proportional with their activity (assignment submissions, quiz submissions, lectures viewed, forum thread views). However, that students with perfect grades (100%) are not into the group of biggest forums readers. This is consistent with the explanation that the population of near-perfect students is composed of two subgroups: Solvers, who perhaps already know the material, and All-rounders, who diligently watch the lectures, finish the quizzes, and do assignments. It is striking that 90% of All-rounders are forum readers, meaning that the two populations heavily overlap. It is apparent that a large majority of the most engaged students are on the forum.
- **Notes:** Students in these courses can engage in a range of activities—watching lectures, taking quizzes to test their understanding, working on assignments or exams, and engaging in a forum where they can seek help, offer advice, and have discussions. We find first of all that each student’s activities in a course can be usefully described by one of a small number of engagement styles.

### Elsevier
#### **Analyzing the students’ behavior and relevant topics in virtual learning communities** - [10.1016/j.chb.2013.10.001](http://dx.doi.org/10.1016/j.chb.2013.10.001)
- **Source:** Analyzing the students’ behavior and relevant topics in virtual learning communities
- **Problem:** Loneliness and demotivation of e-learning students are the first cause of failure of online courses. (Q1) What are the students’ behavior patterns during their interaction and participation in the asynchronous virtual discussion forums of the virtual learning community? (Q2) What are the most relevant topics and subtopics in the asynchronous on-line discussion forums of the on-line learning community? (Q3) Could they be characterized in an automatic way?
- **Objective:** The analysis of the students’ behavior and discovering of the most relevant topics in a course is propose in this work.
- **Methodology:** (1) we provide an extensive analysis of the students’ behavior in an on-line learning community. In the first phase, messages were collected and stored in a local database. In our first approach to reveal the students’ behavior, a number of statistical indicators were calculated over the forums’ messages. For the semantic analysis needed to detect topics we use the Apache Lucene library. Each message is split in basic tokens. Among these words we can find stop-words (e.g., the, at, which, or on). After removing the stop-words, the remaining tokens are processed to obtain their stem using the Porter algorithm in order to detect the plural of words or the conjugation of verbs, which allows us to record the frequency of tokens based on their stem. (2) we propose a set of algorithms to characterize the most relevant topics (and their subtopics) of the community during two academic years. The peak of a topic depends on how relevant topics are. Hence, it can be concluded that a topic is represented by a set of associated stems. Thus, topics should be classified: - Chatter topics, new thoughts on chatter topics are published all days at an educational community and some members can react to previously posted ideas. For example, evaluation elements, exams, literature for the course; - Spike topics, which are induced by LMS external events, produce sharp rises in postings. A new event happening (exam date, scores publications, or task assignments) may cause an unpredictable reaction in the community; We propose a way to characterize relevant topics in our discussion forums. For that, we have analyzed three weight functions for each stem: Basic frequency, Inverted frequency and Weighted frequency. (3) these relevant topics and subtopics are characterized in an automatic way. For this purpose, Algorithm 1 deals with the computation of chatter stems (mostly chatter). As a further step, the spiky chatter stems are detected by using the Algorithm 2, and just spike stems will be removed from the set of relevant topics. This second algorithm is not intended to be fully automatized. Its execution and results should be supervised by an expert in order to be useful;
- **Dataset:** Considering all the forums, participants created 514 messages during the 2010–2011 academic year. On the other hand, in the 2011–2012 academic year, 1024 messages were published overall.
- **Metrics:** We base our study on the following statistical indicators for each participant: - Number of published messages (N); - Number of replies (Rm); - Number of initiated conversations (Ti); - Number of initiated conversations without replies (Ti1); - Number of conversations where the participant has posted a message (Ct); - Number of forums where the participant has posted a message (Ft);
- **Case study:** This study is focused on ‘‘Network Services Management in Operating Systems’’ (NetServicesOS) subject belonging to the ‘‘Communication, Networks, and Content Management’’ post-degree program in the Faculty of Computer Science at Spanish University for Distance Education (in Spanish, Universidad Nacional de Educación a Distancia – UNED). The number of enrolled students in our subject for each academic year is around 30. The duration of the subject are 15 weeks in the first semester of the academic year. This study is conducted using data from two academic years, 2010–2011 and 2011–2012, where these data consist on the messages published in the forums of our LMS. For the 2010–2011 academic year, 32 students were enrolled in the NetServicesOS subject. For the next academic year, 36 students were enrolled. For both academic years, three different forums were created: Forum Student for students; Forum Activities 1–6;  Forum Activities 7–11;
- **Results:** (Q1) Students can be classified as producers and consumers, depending on their pattern of behavior. A producer is a student who posts messages and can be considered as a proactive or reactive. A proactive student not only replies their partners, but she initiates threads expressing his/her feelings and opinions. In contrast, a reactive student only takes an active part by answering already created threads. (Q2) We find that the Weighted frequency is the best relative importance metric for each stem, where the best threshold for it is 0.6. (Q3) Algorithm 2. Although the main part of Algorithm 2 can be automated, this final search must be supervised by an expert, in our case, faculty.
- **Notes:** Over the last years, adaptive hypermedia has been widely used for the development of customized Web-based courses. Therefore, the students’ learning process was guided, adapting both pedagogical resources and learning ways to specific user’s features.

#### **Developing early warning systems to predict students’ online learning performance** - [10.1016/j.chb.2014.04.002](http://dx.doi.org/10.1016/j.chb.2014.04.002)
- **Source:** Computers in Human Behavior
- **Problem:** How can data mining techniques accurately predict student learning performance based on activities in a fully online course? With the inclusion of time-dependent variables, how early in the semester can the early warning system accurately predict student learning performance? Which data mining technique offers superior predictive power regarding learning performance, when a fully online class is in progress?
- **Objective:** To understand the effects of time-dependent variables on academic performance. Time-dependent variables denote those variables that varied during the learning activity processes (i.e., change with time).
- **Methodology:** We collected online course time-dependent variables to build reliable prediction models for an early warning system using C4.5, classification and regression tree (CART), logistic regression (LGR) and adaptive boosting (AdaBoost) as part of our data mining strategy. To build a classification model for the early warning system, each student in the dataset is assigned a particular label, i.e., pass or fail the course. A student passes the course if the average score for midterm and final examinations is greater than, or equal to, 60 points, otherwise, the student is classified as fail. The study recruited 300 students, of which 284 students passed the course, and 16 students failed the course. We generate three datasets based on different periods of study. The three datasets T4W, T8W, and T13W. We generate three datasets (NT4W, NT8W, and NT13W) by removing all time-dependent variables from the original datasets. After, an early warning prototype system was developed. The proposed system comprised three main modules: a data mining engine, a knowledge base, and an inference engine. The data mining engine was responsible for generating early warning rules; The knowledge base is a repository for the knowledge that has been verified by domain experts; The inference engine, also known as the rule interpreter, is a computer program for determining learning outcomes (i.e., pass or fail) based on a student’s learning portfolio. During the semester, the earning warning system automatically judges at-risk students and sends warning alert through emails. At the same time, instructor also gets the warning alert email showing those at-risk students. When a student logs into the LMS, his/her current learning status can be displayed by the dashboard. When a learner clicks on one of the key performance indicators (KPIs) on the dashboard, the learning history will display in the right of the screen. A learner can compare his or her learning history with class-wide statistics regarding the best, the average, and the worst values of the selected KPI.
- **Dataset:** Each student record includes four types of input variables, these are variables regarding: login behavior, the use of online course materials, assignment status, and discussion status in the forum. In the collected dataset, 5.33% of students failed the course, resulting in a serious class imbalance problem.
- **Metrics:** A student passes the course if the average score for midterm and final examinations is greater than, or equal to, 60 points, otherwise, the student is classified as fail.
- **Case study:** We analyzed a fully online undergraduate information literacy course offered at a national university in Taiwan during September 2009 to June 2010 by the same lecturer. The analysis included 330 student learning portfolios.
- **Results:** Both C4.5 and CART provide an overall accuracy of greater than 93%, significantly better than the accuracy of the LGR method. Additionally, although CART exhibited slightly greater accuracy than C4.5 did. These findings show that compared to LGR, the DT-based early warning system provides greater accuracy in characterizing student learning performances. The accuracies of C4.5 in NT4W, NT8W, and NT13W were 0.941, 0.944, and 0.913, respectively. Compared to the corresponding results in T4W, T8W, and T13W, we found that considering time-dependent variables produced significantly more accurate forecasts. On the other hand, the CART experimental results were similar to those of C4.5. Compared to datasets without time-dependent variables, applying CART to T4W, T8W, and T13W increased accuracy by approximately 1–4%, which is statistically significant. Our results also confirm that, we can increase the accuracy of an early warning system in identifying at-risk students by considering time-dependent variables. We consider two classifier ensembles, AdaBoost + C4.5 and AdaBoost + CART. The best classifier identified by this study was provided by combining CART with AdaBoost.
- **Notes:** The objectives of these EDM studies include: understanding learner behavior determining the effectiveness of learning systems identifying academically at-risk students and developing an early warning system and decision support system for instructors. This current paper presents goods results about methods to predict student performance. Then, a system based on the methods of predicting of performance student was developed to help teachers adjust their teaching methods and students to improve their performance. However, it was not evaluate both if the early-warning affects the teaching methods performed by the teacher and if the dashboard affect the behavior or performance of the student.

### IEEE
#### **Telescope, a MOOCs initiative in Latin America: Infrastructure, best practices, completion and dropout analysis** - [10.1109/FIE.2014.7044103](http://dx.doi.org/10.1109/FIE.2014.7044103)
- **Source:** 2014 IEEE Frontiers in Education Conference (FIE) Proceedings
- **Problem:** Most participants in MOOCs are not prepared to control their own learning. The most recurrent criticism is the low completion rates. MOOCs have relatively few active users, that user “engagement” falls off dramatically—especially after the first 1-2 weeks of a course—and that few users persist to the course end.
- **Objective:** Analyze the demographic aspects, characteristics, general behaviors, and focus on the course completion and drop out reasons.
- **Methodology:** To validate the general information and activate the user was required to complete a form with; gender, age, academic level and country; in this way we obtained the demographic data. IP addresses were not collected; therefore all available data on geographical origin comes from the form mentioned. To analyze the behavior of students during the first week of the course, it was created an analytics report for access to the MOOC’s. About the students with at least one login, we assignment consider all those who reviewed the reading material of the first week, but not exactly delivered the first assignment. To perform the drop out analysis, were published a desertion-questionary at the end of the course.
- **Dataset:**
- **Metrics:**
- **Case study:** In 2012 we create the project with the aim of sharing knowledge in the Latin America region. The experiences presented correspond to the MOOCs implemented during 2013 with over 15.000 enrolled students, and learners from over 15 countries, including Spain, Mexico, Guatemala, Colombia, Peru, etc. The MOOCs were designed in units, distributed and duration depending upon one per week typically objectives. During the course, students accessed the content, interacted with their peers and tutors using collaborative forums and completed learning activities and assignments using different tools. A peer review system was integrated to the platform, the system enabled the student to evaluate one or more of their fellow students through evaluation parameters provided random and anonymous.
- **Results:** On average 34.33% of the students are placed in the category "No-Shows". The learner behavior in each course were similar. 60.59% of completion students were male; with on average M= 81 final grades. The 57% of students have a Bachelor’s degree and 30% report education beyond a Bachelor’s degree. 35.40% of students and solving asking questions forums participated in questions to their peers. However, 11.32% said they never entered the forum system; and only 20.73% admitted only to read the comments. In our MOOC experience for dropout student showed the average age was M=37 years; 60.91% of drop-out students were male and the 56% of students have a Bachelor’s degree. The 64.85% of participants that have not finished the MOOC’s indicated that primary reason they drop-out was related to "job responsibilities changed during the MOOC duration". In terms of the academic reasons for leaving the MOOC 47.96% participants indicated that it was “too hard to work full-time and master the online course”. This was followed by 21.65% for category “other” which included issues such as “I didn´t have time to complete assignments”, “slow internet connection”, and “too many forums which caused confusion” . 7.12% raised the problem that they were “Not able to perform well in the course”; 4.85% indicated, “Not technically prepared for this program”, 3.50% indicated that the “academic program too difficult/ demanding”, and in contrast 3.29% emphasized that the “academic program was not challenging”. The 3.47% that “course had been poorly created”, and finally 1.07% found that they are “Course were poorly taught”.
- **Notes:**

#### **Learning about Social Learning in MOOCs: From Statistical Analysis to Generative Model** - [10.1109/TLT.2014.2337900](http://dx.doi.org/10.1109/TLT.2014.2337900)
- **Source:** IEEE Transactions on Learning Technologies
- **Problem:** How can we leverage the large-scale, extensive data that has emerged in recent months to better understand MOOC forum activities? These forums suffer from the following major problems: - Sharp decline rate: The amount of interaction rapidly drops soon after a course is launched; - Information overload: As a course reaches a larger audience, its forum is often flooded by discussions from many students. Thus, it quickly becomes infeasible for anyone to navigate the discussions to find course-relevant information.
- **Objective:** In this paper, we study both problems through a comprehensive data set that we obtained by crawling the discussion forums of all courses that were offered on Coursera during the summer of 2013.
- **Methodology:** - Statistical analysis (Linear Regression) to understand the factors that are associated with student participation on MOOC forums. We first use regression models to understand what variables are significantly correlated with the number of posts (or users) on the forums in each day for each course; - Identifying the information overload problem. In the first few days, the forum is often flooded with “small-talk,” such as self-introductions. The primary goal in this stage is to classify these threads and filter them out. Beyond the first few days, of small-talk often begins to drop. At this point, most of the threads are valuable, so it is important to be able to give a relevance ranking of new threads over time; - Generative models. We propose a unified generative model for thread discussions that simultaneously guides (i) the choice of classifiers, (ii) the design of algorithms for extracting important topics in each forum, and (iii) the design of a relevance ranking algorithm based on the resulting topic extraction algorithm; In order to know how many small-talk posts exist for each course, we use a machine learning approach  (SVM) to classify the threads, using the labels from MTurk as training data.
- **Dataset:** In total, our data set consists of approximately 830K posts. There are 171.197 threads, 831.576 posts, and 115.922 distinct users in our data set.
- **Metrics:** For the purpose of comparison across course types, we categorize a course as  quantitative (If a course requires the students to carry out mathematical or statistical analysis, or to write computer programs), and vocational (If a course’s material is directly relevant to jobs that require high school or college degrees, or it is plausible to see the course offered in a typical university’s continuing education division). We use these categories to partition the data into three groups: a course could be (1) vocational, (2) science or applied science or (3) humanities and social sciences. The discussion threads can be roughly categorized into the following groups: - Small-talk conversations that are not course-related; - Course logistics such as when to submit homework; - Course-specific discussions that can range in scope from highly specific to open-ended; Our dependent variables are Yi,t and Zi,t, where Yi,t refers to the number of posts on the t day in the i course, and Zi,t refers to the number of distinct users that participate in discussion on the t day in the i course. We consider a student active if she made at least two posts in a course and inactive otherwise.
- **Case study:** We focused on all 80 courses that were available in the middle of July and that ended before August 10, 2013. Seven of these 80 courses became inaccessible while we were crawling or coding the data, and thus the 73 courses for which we have complete records were used for statistical analysis. We added five more courses that ended shortly after August 10.
- **Results:** The percentage of small-talk is high across different categories, and then it drops over time. For humanities and social sciences courses, on average more than 30 percent of the threads are classified as small-talk. Small-talk is a major source of information overload on MOOC forums. While teaching staff active participation in the forum is associated with a higher volume of discussion (for every additional post by the teaching staff, there are on average 6.05 additional posts in the forum each day). An increase in staff participation is correlated with a higher decline over time. While the presence of peer-reviewed homework is associated with 88,29 additional posts per day on average, it is also correlated with a higher decline rate. Quantitative and vocational courses are associated with a smaller volume of distinct users on the forums initially, but also with smaller decline rates over time. Teaching staff participation is associated with an increased number of distinct users on the forums, but also with higher decline rates and the presence of peer-grading is associated with an increase in the total number of distinct users, but also with higher decline rates.
- **Notes:**

#### **Monitoring student performance using data clustering and predictive modelling** - [10.1109/FIE.2014.7044401](http://dx.doi.org/10.1109/FIE.2014.7044401)
- **Source:** 2014 IEEE Frontiers in Education Conference (FIE) Proceedings
- **Problem:** How does teacher cluster e-learning users based on their answers? Are there factors that influence the tutor’s help in the VLE? What behavioral variables does the teacher need to understand to improve the virtual class in VLE?
- **Objective:** Modeling of user clustering based on information about the user performance in VLE. Prediction of student performance based on their features in VLE.
- **Methodology:** First, it is necessary for approach collect educational data for applying preprocessing techniques of EDM, after students interacted with VLE activities. In this moment, it removes incomplete, data of the sample. Secondly, it noisy and inconsistent highlights two main steps: clustering and prediction. The clustering process aims for identification of student’s group according to his performance. After understanding the groups, it predicts specific behaviors of student in each cluster (group), which were defined in the last step. Our paper believes that an effective inference’s evaluation should involve two main perspectives: individual and group performance. It was defined that the minimum linear value of k value (number de clusters) is five. We opt to use the k-means algorithm to identify these five clusters. This research defined the necessity to compare to the scores of the variable “Avg Assistance Score” among the groups mentioned on previous step. For the execution, it used the Stepwise Backward Regression. For this, we choose the Spearman test, because we are dealing with a linear, homoscedastic and normal sample. We only used Strong correlations in the regression models, such as more than 0.7.
- **Dataset:** Composed by 120 students, 6592 steps, and 11394 transactions.
- **Metrics:** Log has: Row (R): Student Id; Latency (L): Time spent on aswers resolution; Hints (H): Hints gave by the VLE when student request; Incorrect (I): Number of incorrect answers; Correct (C): Number of correct answers; Avg Correct (Ac): Average score of correct answers; Steps (St): Number of steps on resolution answer; Avg Assistance Score (Avg): Average assistance score (The tutor provides learning assistance to user when they have a doubt); Correct First Attempts (Cfa): Number of times that the first answer was correct; In the current paper, we defined that the score of tutor’s help (“Avg Assistance Score”) is the most interesting factor for our questions, because of its influence on the student assistance on VLE. The academic performance was defined by following answer labels: Expert; Regular; Criticism; Good; Bad.
- **Case study:** This research uses Open Learning Data from Pittsburgh Science of Learning Center (PSLC). Our paper selected data of an English e-learning course. The analyzed project called Intelligent Writing Tutor, composed by 120 students, 6592 steps, and 11394 transactions. The users need to complete edit or menu article tutor. Students could put articles in, take them out, or change articles and had to both find and correct article mistakes. Students are randomly either high-assistance or low-assistance assigned to be in groups. Students in the high- assistance groups use the menu-based tutors, while students in the low-assistance group used the controlled-editing tutors.
- **Results:** Clusters: (1) Expert: It has high scores of accuracy and the student almost does not need to the tutor assistance; (2) Regular: Presents a score of accuracy higher than the median and required support; (3) Criticism: Lots of mistakes, attempts and errors are detected on their answers that were not constructed on suitable time (detected by the high score of “Latency” and low score of “Correct”); (4) Good: A set of good answers. This group interacted with tips provides by the tutor; (5) Bad: Answers, which are based on a minimal number of hints and tutor’s assistance; Regression: The cluster 3 and 5 did not present dependency with the variable “Avg Assistance Score”. It means that the tutor’s help did not influence. It is possible and discouraged when students were these expected because constructing their answers. The cluster 1, 2 and 3 could advance in the proposed approach. It was observed that the variables “Incorrect” and “Correct First Attempts” belong to three regression models. However, there is an inverse relationship among the variables. If a student has a good score of “Correct First Attempts”, he will probably reach a low “Avg Assistance Score”. It happens because the student has the confidence and knowledge to answer the questions without tutor interventions. We can detect that the number of “Steps” depends inversely on “Avg Assistance Score” in the cluster 1. It is an evidence that the tutor acted when the student remained in the same state (the number of steps is low or do not change) in the learning interactions. When we investigate the cluster 2, we noted that new criteria appeared in the regression model: “Hints”. It means the tutor’s monitoring and intervention when the student uses many hints in a question. The criteria “Hints” appeared on cluster 2 because useful tips are fundamental for the student’s performance. Question 1: How do we cluster e-learning users based on the quality of answers? Answer 1: We group the answers according to the set of attributes. These attributes are responsible for describing the students’ behavior on the learning interactions. It is important to keep in mind that the groups are unstable because it is possible to obtain different results depending on the statistic clusters when we influenced our technique. The quality analyzed several features to define the learner’s expertize when he answered the questions. Question 2: Are there factors that influence the tutor’s help in the VLE? Answer 2: Overall, we can highlight two main variables based on our prediction step: “Incorrect” and “Correct First Attempts”. However, sometimes we observed other variables influencing the “Avg Assistance Score”, such as “Hints” and “Steps”. Question 3: What behavioral variables does the teacher need to understand for good performance of the virtual class in VLE? Answer 3: The main variable in the sample that helps the teacher monitoring is “Avg Assistance Score”. We believe it is because the tutor’s help is a fundamental tool when the teacher needs to provide more support on VLE. The data confirms this behavior, because we found interesting information when investigating the regression results.
- **Notes:** Normally, human tutors or teachers get information from observing what students (users) say and do on VLE. A few challenges interfere on this analytical process because most teachers or human tutors are not a statistical expert or do not receive training to extract key information. Researchers investigate several Data Mining methods in order to assist teachers and improve e-learning systems, which we called Educational Data Mining (EDM). We can detect that there are threats to the validity of our experiment. The main problems are: - The lack of randomization of subjects, since the experiment is a case study low statistical power because the samples are small in the new course design; - The cluster groups are mutable, because of K-means algorithm. This feature can change the groups’ sequence in replications process.

## 2015

### ACM
#### **"Scaling up" learning design: impact of learning design activities on LMS behavior and performance** - [10.1145/2723576.2723600](http://dx.doi.org/10.1145/2723576.2723600)
- **Source:** Proceedings of the Fifth International Conference on Learning Analytics And Knowledge - LAK '15
- **Problem:** Learning design is widely studied in the Higher Education sector, but no study has yet empirically connected learning designs of a substantial number of courses with learning behavior in Learning  Management Systems (LMSs) and learning performance.
- **Objective:** This study will begin to overcome this gap in learning analytics research by combining three different sources of data from 40 blended and online modules involving a total of 21.803 learners.
- **Methodology:** A team of learning design specialists reviewed all the available learning materials, classifies the types of activity, and quantifies the time that students are expected to spend on each activity. Academics had the opportunity to comment on the data before the status of the design was finalised. In other words, each mapping was at least reviewed by three people, which enhanced the reliability and robustness of the data relating to each learning design. Learning performance was calculated by the number of learners who completed and passed the module relative to the number of learners who registered for each module. As a first step, we analyzed the underlying structures and collective patterns of the seven learning design activities by using cluster analysis of the 87 modules. We then tested solutions for 2-5 clusters using K-means cluster analyses. As a second step, we merged the learning design data with the LMS and learner retention data based upon module ID and year of implementation. Workload is ‘the number of hours that students objectively spend on studying’. In this study, workload was calculated by the learning design team as part of the module mapping process. We linked the learning designs of 32 modules followed by 19,322 learners with their LMS data. On a total of 2,186,246 occasions, the LMS was visited by 19,322 students. We visually analyzed whether the four clusters lead to different LMS usage over time. We linked the learning design metrics with learning performance.
- **Dataset:** We linked the learning designs of 32 modules followed by 19,322 learners with their LMS data. On a total of 2,186,246 occasions, the LMS was visited by 19,322 students.
- **Metrics:** The learning design taxonomy, which identifies seven types of learning activity: (1) Assimilative activities: relate to tasks in which learners attend to discipline specific information. These include reading text (online or offline), watching videos, or listening to an audio file; (2) Finding and handling information: for example on the internet or in a spreadsheet, learners take responsibility for extending their learning, and are therefore engaged in active learning; (3) Communicative activities: refer to any activities in which students communicate with another person about module content; (4) Productive activities: draw upon constructionist models of learning, whereby recent research has indicated that learners who build and co-construct new artefacts learn effectively; (5) Experimental activities: develop ‘students' intrinsic motivation and industry-relevant skill transfer’ by providing learners with the opportunity to apply their learning in a real life setting; (6) Interactive activities: endeavor to do the same, but in some fields this is not possible: for example, in medicine, such activities would have health and safety implications for either the learner or the person that they interact with; (7) Assessment activities: encompass all learning materials focused on assessment, whether enabling teaching staff to monitor progress;
- **Case study:** This study took place at the Open University UK (OU), the largest higher education provider of online distance education in Europe. In total 87 modules were mapped by the learning design team in the period January-August 2014. Two different types of LMS data in Moodle were gathered per module: total number of visits to the LMS; and average time spent on LMS.
- **Results:** In Cluster 1 modules, which we label as constructivist modules, seemed to have a strong emphasis on assimilative activities, as 58% of learning activities fell into this category, Students undertook assimilative activities such as reading module materials, watching videos and YouTube materials, reviewing core concepts and approaches. For example, a first-year science introductory module focused on understanding principles and concepts in a range of topics. 22 (25%) modules were positioned in Cluster 2, with a strong focus on assessment such as formative assessment for learning (e.g. write, present, report, demonstrate) and summative assessment of learning. We label Cluster 2 as assessment-driven. For example, an introductory history course focused on providing a historical perspective of a particular region in the UK. The 24 (28%) modules in Cluster 3 had a more or less equal balance between assimilative and assessment learning design activities, with a relatively high focus on experiential activities. For example, the health and social care module used a mix of understanding basic concepts as well as applying these concepts using case-studies, self-reflections and collaborative approaches. We label Cluster 3 modules as balanced-variety. The 16 (18%) modules in Cluster 4 seemed to use more a learner-centered learning design approach, whereby relatively more time was devoted towards communication, productive and interactive activities. For example, in a foreign language module, a range cognitive, skills-based, reflective and application tasks are assessed using a mix of technology tools and blended tuition. Cluster 4 modules as social constructivist. We found that assimilative activities were negatively related to all of the other six learning design activities, indicating that focusing more on cognition and content reduces the focus on other activities. On average, students spent 122.71 minutes per week online during each of the first 10 weeks of the module. LMS visits were positively related to communication activities and total (planned) workload, and negatively related to assessment activities. Average time spent in the LMS correlated positively with finding and handling information activities, communication activities, and total workload, whilst, again, a negative relation was found with assessment activities. The only significant (negative) correlations between the seven learning design activities and learning performance were with assimilative activities. Furthermore, positive correlations were found between productive and assessment activities and pass rates, although these were not statistically significant. No significant correlations were found between our LMS indicators and learning performance (not illustrated). Our most important finding is that learning design and learning design activities in particular strongly influence how students are engaging in our LMS. Further, learning design seems to have an impact on learning performance.
- **Notes:**

#### **Correlation of Topic Model and Student Grades Using Comment Data Mining** - [10.1145/2676723.2677259](http://dx.doi.org/10.1145/2676723.2677259)
- **Source:** Proceedings of the 46th ACM Technical Symposium on Computer Science Education - SIGCSE '15
- **Problem:** Teachers cannot take care of so many students in detail in the classroom.
- **Objective:** This paper presents a study that applies text mining techniques to students’ comments to predict their grades.
- **Methodology:** In this study, we employ Latent semantic analysis (LSA) and Probabilistic latent semantic analysis (PLSA) models to grasp students’ learning attitudes and learning situations. LSA constructs a conceptual vector space in which each comment is represented as a vector in the space. PLSA finds some aspects of words known as “topics” that can deeply distinguish comments with different meaning. We introduce PLSA to improve the results of LSA. We create prediction models based on comments analyzed by PLSA and LSA using ANN and SVM methods and compare the accuracy of the models. We evaluated the prediction performance of our proposed models by 2-fold cross validation. To achieve highly accurate prediction results from student comments with LSA method, we examined the different number of k-dimensions with ANN and SVM techniques from lesson 1 to 15, with P, C and N comments.
- **Dataset:**
- **Metrics:**
- **Case study:** Comment data were collected from Goda’s courses consisting of 15 lessons. The main subject from lesson 1 to 6 is computer literacy, giving information on how to use some IT tools such as word processors, spread sheets, and presentation tools. From lesson 7 to 15, students begin to learn the basics of programming. Comment data were collected from 123 students in two classes: 60 in Class A and 63 in Class B.
- **Results:** The average accuracy results for C-comments from lesson 1 to 6 using ANN had the higher rate between k=18 and k=22. It scored 50%. The highest rate scored 54% with k =20. From lesson 7 to 15, the higher rate scored from k=3 to k=12. It scored 46%. The highest rate scored 48% with k =10. On the other hand, PLSA automatically determines the number of topics and the number of words in each topic. For example in lesson 7 and after choosing C-comments, 7 topics and 30 words in each topic were obtained using the EM algorithm. We manually chose the number of topics and the number of words in each topic for PLSA*. In this case, we chose 3 topics and 20 words in each topic after some trials.  The average accuracy results were 50%, 56% and 63% for LSA, PLSA and PLSA*, respectively. A with PLSA and PLSA* were better than those with LSA. Predicting student grades using the PLSA* model had the highest accuracy and F-measure.
- **Notes:** The result is poor. The best result was 63% of accuracy.

#### **Am I failing this course?: risk prediction using e-learning data** - [10.1145/2808580.2808621](http://dx.doi.org/10.1145/2808580.2808621)
- **Source:** Proceedings of the 3rd International Conference on Technological Ecosystems for Enhancing Multiculturality - TEEM '15
- **Problem:** This paper could be classified inside the predicting students’ performance challenge.
- **Objective:** We propose a system that detects the students at risk of failing a blended course based on their interactions with the e-learning platform.
- **Methodology:** Firstly we based our study in the students’ interaction with the e-learning platform instead of using their grades or other characteristics. Secondly, our system predictions are based on the behavior and results of students of previous academic years, instead of on the previous results of the target student, as the papers mentioned above. Thirdly, we use time series and temporal decomposition to study the students’ interaction with the platform, detecting that the trend component can be used as performance predictor. To predict the students’ results of one academic year, we need at least the data of one previous academic year, which is used as a training data. In our case, we use the academic year 12/13 as training data for the academic year 13/14. For each student of the test course: 1. The algorithm calculates the trend component of the time series formed by the daily number of interactions with the e-learning platform of this student until the control point day; 2. The algorithm calculates the vector Euclidean distance of this student’ trend component and each of the trend components of the four groups; 3. The student is assigned to the group that has the minimum Euclidean distance; After applying the algorithm, the students of the test course will be divided in four groups: risk of withdrawal, high risk of failing, low risk of failing and not at risk of failing. We have set up three control points: - 8th week: At this point the students have already done the first laboratory assignment and the first applied assignment; - 12th week: At this moment the students have already done the second problem and laboratory assignments; - 14th week, just one week before the end of the course: This last point is used as a last emergency call for professors and students at risk.
- **Dataset:**
- **Metrics:** We set up four groups: (i) students that withdrawal the course, (ii) students whose final grade was less than 3 points, (iii) students whose final grade was between 3 and 6, and (iv) students whose final grade was higher than 6.
- **Case study:** We collected the data from the official e-learning Moodle-based platform of the University of Vigo. We centered our study in a blended course of the second year of the telecommunication engineering. In this course the Moodle platform acts as a complement of traditional education, where students can download the class notes, do some mandatory tests, receive information and news about the course, etc. The assessment mechanism of this course is based on several mandatory assignments distributed along the course and a final basic knowledge test that will take place the second-to-last week of the course. - Laboratory: To determine if the student has acquires all the knowledge and skills corresponding to the laboratory practices; - Applied: To determine if the student knows how to apply the knowledge of the course to solve problems; Each quiz is graded between 0 and 10. To pass the course the student should pass the three parts: - Laboratory: The average of the three quizzes must be greater or equal to 5; - Applied: The student should get 5 points or more calculated as the weighted average of each quiz (25% the first one, 35% the second and 40% the last one); - Basic knowledge quiz: The student should get 5 or more points; The students pass the course with a final grade of 5.
- **Results:** We have detected the failing risk successfully in the 84,37% of the students in the first control point (8th week). At this point, both the student and the failing risk successfully in the 84,37% of the students in the first control point (8th week). The success rate increases until reach the 93,75% in the third control point (14th week). More than 80% of withdrawals are detected in the second control point. Furthermore, only 1.4% of the students that are not detected as at risk of failing in the second control point finally give up the course. Many of the students that finally fail the course with less than 3 are detected as at risk of withdrawal. This may be because the behavior between some students that finally drop out the course and some students that finally decide attend to the exams but fail with such a low grade is minimum. Only the 16,67% of the students that fail the course with more than a 3 was not detected as at risk in the third control point. This is a very good result taking into account that these students are very close to pass the course.
- **Notes:**

#### **Probabilistic Use Cases: Discovering Behavioral Patterns for Predicting Certification** - [10.1145/2724660.2724662](http://dx.doi.org/10.1145/2724660.2724662)
- **Source:** Proceedings of the Second (2015) ACM Conference on Learning @ Scale - L@S '15
- **Problem:** Modeling student behavior. Can Latent Dirichlet Allocation (LDA) serve as an unsupervised approach for discovering the behavioral trends of MOOC participants? Can the mixed-membership model from Latent Dirichlet Allocation (LDA) predict certification?
- **Objective:** We make this problem more tractable by adapting the approach of Latent Dirichlet Allocation (LDA) to predict certification.
- **Methodology:** To model behavior, we represent students as a bag of interactions with the courseware. Each of the static resources in the course has a unique module identifier. In 8.02x, there were 1,725 unique module identifiers. Each student is represented as a bag of interactions with the courseware, where we only consider browser issued events. To quantify these interactions, we used time spent in seconds on each course module. Breaks over 30 minutes long were discarded. The bag of interactions model was tested based on its ability to accurately predict whether or not a student would earn a certificate. We separately generated each of the interaction representations using the logs from the beginning of the course to the end of the given week. The performance of each representation was quantified by 5-fold cross validation of a Support Vector Machine (SVM) classifier for certification, where Different Error Costs (DEC) compensated for the class imbalance.
- **Dataset:** These enrollees led to 37.394.406 events being recorded in the edX tracking logs.
- **Metrics:** - Shopping use case; - Disengaging use case; - Completing use case;
- **Case study:** Our application involves one MITx MOOC, an introductory physics course called 8.02x: Electricity and Magnetism from the spring of 2013. Between January, 17 and September 8, enrollment reached 43.758 people, from around the world with a wide range of educational backgrounds. The resources in 8.02x included a variety of videos, problems, textual content, and simulation activities.
- **Results:** Only 4.2% of registrants earned a certificate in 8.02x making the classes of non-certificate earners and certificate earners extremely imbalanced. Using the time spent on modules as the underlying representation, 5-fold cross-validation of log-perplexity per interaction is displayed by a number around 50, however, it is unclear from cross-validation alone how much of effect such a large number of use cases has on our ability to interpret the model. With the 3-use case model as a baseline, we describe the resulting behavioral patterns. The use case concentrated the majority of its probability on videos from the first week of the  courses. Probability distributions from a 3-Use Case Model of 8.02x over all released content during the 18 week running. A course structure visual aid is below the lowermost probability distribution. Each bar is a set of resources, where color and length represents the type of resource and its weight toward final grade, respectively. Orange - lecture videos, black - lecture questions, gray - homework, green - simulations, red - exams, and blue - problem solving videos. These three use cases were evident from the very beginning of the course. The shopping use case remained virtually unchanged after the first two weeks of the course, while the disengaging and completing use cases slowly spread their distributions out, as new material was released. At the peak of 50 use cases, a SVM classifier with DEC achieves 0.81±0.01 accuracy at predicting certification with just one week of data. Even with only 3 use cases the, prediction accuracy is still at 0.71±0.01 with only one week of data.
- **Notes:**

#### **Connecting the Dots: Predicting Student Grade Sequences from Bursty MOOC Interactions over Time** - [10.1145/2724660.2728669](http://dx.doi.org/10.1145/2724660.2728669)
- **Source:** Proceedings of the Second (2015) ACM Conference on Learning @ Scale - L@S '15
- **Problem:** We consider the problem of “grade sequence prediction” or “grade labeling”: to classify a students’ grade points scored in multiple MOOC courses into one of the several defined categories.
- **Objective:** Our interest is in making “long-term” predictions on grade points that a student achieves, by leveraging this aspect of the interaction.
- **Methodology:** We first operationalize three kinds of “burstiness” indices from students’ interactions for all their MOOC courses taken. For all the 3 indices, a higher value denotes more “burstiness” and vice versa. We discretize the above burstiness indices into four categories based on equal frequency: Low (L), Medium (M), High (H) and Very High (V). We then use a generic sequence of tokens representation for our input to the Conditional Random Fields (CRF) framework. CRF follow a discriminative modeling approach to capture the conditional probability distribution p(y|x) of label sequences (y1, y2...yn), given the input sequences (x1,x2...xn), without requiring calculation of potentially dependent features in p(x) that are not required for classification. We utilize the following 4 feature templates for each of the 3 “burstiness” features: a)1I: current feature value, previous feature value, next feature value (input ±1) and independence assumptions among y, b)2I: current feature value, previous 2 feature values, next 2 feature values (input ±2) and independence assumptions among y, c)1D: current feature value, previous feature value, next feature value (input ±1), combinations of previous output token (grade label) and current output token (dependence assumptions among y), d)2D: current feature value, previous 2 feature values, next 2 feature values (input ±2), combinations of previous output token (grade label) and current output token (dependence assumptions among y). Evaluation of the grade sequence tagging is done using precision, recall and weighted F-score measure to account for label imbalance. As baselines, we also employ Logistic Regression and Sequential Minimal Optimization (SMO).
- **Dataset:**
- **Metrics:** (1) Bursty number of play video events/clicks (Bursty #videoplays) is defined as the total number of play video events within a course, divided by the number of unique days student interacted with the particular course. (2) Bursty number of chapters (Bursty #chapters) is defined as the total number of chapters (within the Courseware) with which the student interacted, divided by the number of unique days student interacted with the particular course. (3) Bursty number of discussion forum posts (Bursty #forumposts) is the total number of posts to the discussion forum, divided by the number of unique days student interacted with the particular course.
- **Case study:** Our study context comprises of de-identified data from the first year (Academic Year 2013: Fall 2012, Spring 2013, and Summer 2013) of 13 MITx and HarvardX courses offered on the edX platform. Our study is confined to ≈ 10000 students who took at least 2 or more courses on the edX platform and had some form of interaction with the particular MOOCs.
- **Results:** The 1I CRF feature template configuration out-performs these baseline approaches that are myopic about the impact of current decision on later decisions. CRF improves predictability by allowing for a much richer input feature set.
- **Notes:**

#### **The Impact of Students And TAs' Participation on Students' Academic Performance in MOOC** - [10.1145/2808797.2809428](http://dx.doi.org/10.1145/2808797.2809428)
- **Source:** Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015 - ASONAM '15
- **Problem:** Relationship between forum and academic performance.
- **Objective:** In this paper, we study the course forum of MOOC. We try to find out the relationship between students’ academic performance and their participation in the course forum. We also analyzed the content of their posts and TAs’ posts in order to further understand their behavior in forum.
- **Methodology:** We correlated the assignment data with the forum data, in order to find the relationship between students’ academic performance and their participation in the forum. We also conducted a semantic analysis of the forum data. The semantic analysis used a word segmenter from Stanford Natural Language Processing Group. We built a data analysis system for MOOC data analysis. The front end is based on Lift web framework and Scala. And the back end is a distributed system and based on Hadoop and Hive. We define students’ participation as the average number of threads initiated and the average number of replies they wrote. We correlate the average number with the total score of all the assignments. We consider the relationship between non-negative posts and negative posts. Non-negative posts can be considered as related to the course and involve discussion. Negative posts can be considered as just chatting and unrelated to the course.
- **Dataset:**
- **Metrics:**
- **Case study:** We conducted the data analysis on a sociology course offered by Shanghai Fudan University on zhihuishu.com. We conducted the data analysis on a sociology course offered by Shanghai Fudan University on zhihuishu.com. The forum data has two types. The first type of data records the posts that one initiated. The second type of data includes replies to the original poster. We also used the data of students’ academic performance in their assignments.
- **Results:** Most students did a quite good job in their assignments. Those who appear more active in forum discussion tend to achieve higher scores. Those who earn less than 10 points in their assignments almost didn’t participate in forum discussion. The second (10~20 points) to the third (20~30 points) range don’t have any posts. Students that earn lower scores write fewer posts than those earn higher scores. And those earn higher scores write more replies than starting new threads. It can be concluded that students that earn lower scores are less active in forum discussion than those who earn higher scores. There are quite a lot of students that do not take the discussion in the forum very seriously. Their posts in the forum are probably not related to the content in the class. We try to find out within each range of score, what is the ratio of non-negative posts and negative posts. Students whose score is under 30 rarely write posts in the forum, so their ratio is not calculated. And students whose score is higher than 30 write more posts. Students that earn higher scores have lower ratio than those who earn lower scores. The highest ratio appears in the range ’30 ~ 40’.
- **Notes:**

#### **Exploring the Effect of Confusion in Discussion Forums of Massive Open Online Courses** - [10.1145/2724660.2724677](http://dx.doi.org/10.1145/2724660.2724677)
- **Source:** Exploring the Effect of Confusion in Discussion Forums of Massive Open Online Courses
- **Problem:** Previous explorations of student affect in educational contexts have demonstrated a strong connection between affect, engagement, and learning.
- **Objective:** The focus of ourwork is to better understand the student participation experience as they struggle, express confusion, and ultimately stop participating in a MOOC course.
- **Methodology:** In order to offer the opportunity to identify students who are experiencing confusion, we built machine learning models to automatically identify the level of confusion expressed in students’ posts in the course discussion forums. We extract the input features both from students’ click behaviors and their posting behaviors, including clicking patterns in courses, presence of domain-specific content words, and high-level linguistic features. The output is a numerical value indicating the degree of confusion expressed in each post. We began our work by creating a human annotated corpus for developing our automated measure of student confusion. Second, we represented each post as a set of features as input for our machine learning models. Finally, we constructed a statistical model based on the hand-coded data and evaluated its performance. We used Amazon’s Mechanical Turk (MTurk) to construct a reliable, hand-coded dataset to automatically measure student confusion. For each post, Turkers judged the level of confusion contained in the message on a 1-4 Likert scale ranging from “No Confusion”, “Slightly Confused”, “Moderately Confused” to “Seriously Confused”. We provided them with explicit definitions and examples to use in helping their judgments. Each post was labeled by five different workers. Based on clickstream data, we distinguish instances where students are taking quizzes (quiz), watching lectures (lecture), participating in forums (forum), and viewing other course materials (course). The top twenty click patterns are then extracted as predictors of confusion. For example, we observed a student having a specific click pattern such as “quiz-quiz-forum” could reflect that the student encountered problems during the quiz and turned to the forum to seek help from peers, another pattern “quiz-lecture-quiz-lecture” might indicate that this student referred to the lectures for clarification and help after working on a quiz. We incorporated into our machine learning models may be categorized into the following: (1) Pronouns : “I, we, you, she/he”, impersonal pronouns; (2) Sentiment : affective processes (e.g. “happy, cried, abandon”), positive emotion, negative emotion (“anxiety, anger, sadness”); (3) Spoken Language: negation (e.g., no, not, never), disfluencies (e.g., rr, hm, umm), insight words (e.g., “think, know, consider”), assent (“agree, ok, yes”), adverbs (e.g., “very, much”), certainty (e.g., “always, never”), discrepancy (e.g., “should, would, could”). We included all the active students who contributed at least one post to the course forums in the two courses. The time interval is defined as student participation weeks. We considered the timestamp of the first post by each student as the starting point for that student’s active participation in the course discussion forums, and the date of the last post as the end of participation unless it is the last course week.
- **Dataset:** Algebra had 2.126 active users (active users refers to those who post at least once in a course forum) and 7.994 forum posts; Microeconomics had 2.155 active users and 4.440 forum posts. Algebra has 8.686.230 student clicks, and Microeconomics has 2.709.053 clicks.
- **Metrics:** - Dropout: we consider a student dropping out from participation in the course community if that student has no activities in the course forum (even if they may engage in other forms of engagement with the materials); - TotalPost: This is the number of posts a student contributes to the forums in one week; - Starter: Students who start a discussion thread tend to have a higher probability of being confused compared to those who participate in discussions initiated by others; - Expressed Confusion (ExprConfusion): This measures the average confusion per post a student has expressed in a week; - User Exposed Confusion (UserExpoConfusion): This measures the average confusion per post a studentwas exposed to by averaging confusion scores of posts in the threads that student initiated during the time period; - Others Exposed Confusion (OthersExpoConfusion): This measures the average confusion a student was exposed to by averaging the measured confusion of posts in all the threads he/she participated in those he/she initiated; - Confusion Resolved (Resolved): This variable indicates how many threads are initiated by a student and are later resolved; - Reply (Reply): This variable indicates how many threads a student initi- ated that have received a response from others;
- **Case study:** West Coast state university, which provided the data from two Coursera MOOCs. Our dataset for this paper consists of two Coursera courses: one mathematics course, “Algebra” and one economics course “Microeconomics”. The duration of each of the two courses was 12 weeks. Besides forum records, each student’s interaction (student clicks) with the course materials is also recorded in a clickstream.
- **Results:** We found that students expressed more confusion in Algebra and less confusion in Microeconomics. For example, 28% posts in the Algebra course have an average confusion score larger than 0.90, while that score is 6% in Microeconomics. Besides, 54,6% posts in the Alegebra course have expressed confusion (having a confusion score larger than 0,5) and that of Microeconomics is 35,1. We found that females tend to express more confusion than males; young people have the highest expressed confusion; and middle-aged people are the least likely to express confusion. In addition, students who achieved higher education degrees shared a relatively low level of confusion compared to students whose highest education were primary school or no completion. The hazard ratio (HZ) for TotalPost is 0.60, indicating that students who contribute a standard deviation more posts than average are 40% more likely to survive compared to students who have lower post counts. HZ 1.29 for Starter indicates that survival rates are 29% more likely to dropout for those who have started one standard deviation more thread starters than average. This finding is consistent in Microeconomics. Confusion exposure on threads started by other students has the strongest influence (HZ=1.85) on dropout compared to students expressed confusion (HZ=1.17). Students whose ExprConfusion X Resolved are one standard deviation higher than average are 22% more likely to dropout. Students whose ExprConfusion X Reply are one standard deviation higher have a Hazard Ratio of 1.14, which means that they are one quarter as likely to drop out as the full set of students who express confusion at a level of 1 standard deviation higher than average (1.64). Students who are measured at 1 standard deviation higher expressed confusion are 14% more likely to dropout. Quiz confusion has the biggest impact on dropout in the Algebra course while confusion towards the course more generally leads to the highest dropout in the Microeconomics course. Our results demonstrate that: (1) the more students express their confusion and are exposed to confusion in the MOOC forums, the less likely students are to remain active in the learning community; (2) helping resolving or providing responses to student confusion reduces their dropout in the courses; (3) the extent to which different types of confusion affect dropout is determined by specific courses.
- **Notes:** This study is also subject to the limitation that, all measures of student expression, including expressed confusion, exposed confusion, as well as interaction effect of confusion with reply or resolution, are all judged by Turkers who are not participants and possibly have no experience in MOOC course forums.

#### **Predicting success - how learners' prior knowledge, skills and activities predict MOOC performance** - [10.1145/2723576.2723593](http://dx.doi.org/10.1145/2723576.2723593)
- **Source:** Proceedings of the Fifth International Conference on Learning Analytics And Knowledge - LAK '15
- **Problem:** Understanding of learners' online experiences, processes, and outcomes.
- **Objective:** This paper presents an investigation of how students’ preparedness for MOOCs impacts on their participation and success in the course. This paper considers how students’ prior content knowledge in an advanced area of computer science (Discrete Optimization) and their problem solving skills related to this area of computer science impact on their patterns of engagement as measured through learning analytics and their learning outcomes or performance.
- **Methodology:** The analyses conducted as part of this investigation employed two samples. The first sample included all participants (n=6,635) who were active in the MOOC as determined by submitting at least one assignment. The second sample – a subset of the first – only included participants who passed the course (n=774). These two samples were investigated as it was expected that, given the focus on prior knowledge and skills, distinct patterns may emerge for those who passed the course and those who did not. A stepwise multiple regression was conducted to determine the degree to which these variables were able to predict total points.
- **Dataset:** The Coursera platform records: Course-wide statistics which provide an aggregated overview of activity for the entire class; a grade book which provides a summary information about each learner’s performance; and an event log, which tracks every interaction the learners have with the platform. The grade book and the event log data formed the basis for the learning analytics used in this investigation.
- **Metrics:** The measures used were assigned to each problem solved by student: - Knapsack: Students could score between 0 to 60 points; - Graph Coloring: Students could score between 0 to 60 points; - Assignment Submissions: it was a measure of the total number of times a student submitted any assignment during the course. This value can range from 0 to infinity, however if a student submitted each assignment just once he or she would have an Assignment Submissions value of 37; - Active Days: it was a measure of the total number of days a student was actively submitting assignments in the course. As the course was nine weeks long, this value could range from 0 to 62; - Assignment switches: it was a measure of the number of times a learner switched from submitting one assignment to a different assignment; - Total Points was a measure of the students’ overall performance in the course and was calculated as the cumulative points earned by the learner across all assignments on the final day of the course. The value range for this measure was 0 to 396;
- **Case study:** Data were collected from the first session of Discrete Optimization offered in June 2013, a MOOC provided by the University of Melbourne on the Coursera platform. The inaugural session of this course attracted the interest of 37.777 learners, with 22.731 starting the course, 6.635 active in the assignments, and 774 receiving a certificate of completion for the course. The remaining assignments increased in difficulty and required students to show progressively advanced knowledge and skills in computer programming, computer science, and problem solving. From the 6,635 learners who participated in the previous analysis, 774 passed the course.
- **Results:** Strong positive correlations were seen among knapsack points, graph coloring, active days, and assignment switching, particularly between graph coloring and active days and assignment switching, and between active days and assignment switching. While positive, assignment submission was weakly correlated with all other variables. Knapsack points was entered into the regression model and was significantly related to total points, F (1,6633) = 7401.77, p<.001. This model accounted for approximately 53% of the variance of total points (Adj. R2 = .527). At step 2 of the analysis graph coloring was entered into the model and was also significantly related to total points, F (2,6632) = 15982.75, p<.001. This model accounted for approximately 83% of the variance of total points (Adj. R2 = .828). Total points was primarily predicted by graph coloring, and to a lesser extent by knapsack points. Finally, at step 3 of the analysis the three remaining variables were entered into the regression model and active days and assignment switching were statistically significant, F (5,6629) = 13044.39, p < .001. This model accounted for approximately 91% of the variance of total points (Adj. R2 = .908). Total points were primarily predicted by graph coloring, assignment switches, and active days, and to a lesser extent by knapsack points. Assignment submission’s contribution to the model was not significant. There was a moderate positive correlation between graph coloring and knapsack points, and assignment submission and assignment switching. There was a weak positive correlation between assignment submissions and both graph coloring and active days; and assignment switching with graph coloring and active days.  At step 1 of the analysis knapsack points was entered into the regression equation and was significantly related to total points, F (1,772) = 102.87, p<.001; and accounted for only about 12% of the variance in total points (Adj. R2 = . 116). The second step of the model, in which graph coloring was entered, was significant (F (2,771) = 262.56, p<.001) and accounted for approximately 40% of the variance of total points (Adj. R2 = .404). Total points were primarily predicted by graph coloring, and to a lesser extent by knapsack points. Finally, at step 3 of the analysis assignment switching, assignment submission and active days were entered into the regression model which was statistically significant, F (5,768) = 121.67, p < .001. This model accounted for approximately 44% of the variance of total points (Adj. R2 = .438). Total points were primarily predicted by graph coloring, and to a lesser extent by assignment switching and knapsack points. The contributions of assignment submissions and active days to this model were not significant.
- **Notes:**

#### **Examining engagement: analysing learner subpopulations in massive open online courses (MOOCs)** - [10.1145/2723576.2723606](http://dx.doi.org/10.1145/2723576.2723606)
- **Source:** Proceedings of the Fifth International Conference on Learning Analytics And Knowledge - LAK '15
- **Problem:** Although cMOOC and xMOOC have different pedagogies, a problem that almost all of them experience is the large difference between numbers registering and numbers completing. The high drop-out rates on most MOOCs still provide a cause for concern.
- **Objective:** We investigate whether the same patterns of engagement are found in MOOCs that employ social constructivist pedagogy, or if other patterns of engagement apply.
- **Methodology:** Our initial approach followed the one applied in the Coursera study as far as possible, in order to investigate whether previous findings could be replicated in a different context. That study adopted a methodology designed to identify a small number of ways in which learners interact with MOOCs. Kizilcec and his colleagues began by computing a description for individual learners of the way in which they engaged in each assessment period of the course (typically a week), and then applied clustering techniques to find subpopulations in these engagement descriptions. (1) Once we had created engagement profiles for each learner in this way, we followed the method of Kizilcec’s team and applied the k-means clustering algorithm to partition the learners into a small number of groups. We assigned numerical values to each label (On Track = 3, Behind = 2, Auditing = 1, Out = 0), calculated the L1 norm for each engagement pattern, and used that as the basis for 1-dimensional k-means clustering. We also followed their practice in repeating clustering 100 times and selecting the solution with the highest likelihood, because k-means has random aspects. The method produced two clusters that were very similar to the ones they found. We found a cluster strikingly similar to their ‘Completing’ group: learners who completed almost all the assessments. We also found a cluster similar to their ‘Sampling’ group: learners who visited only once or twice, and did not attempt any assessment. (2) We developed a new classification, in order to reflect the importance of discussion in FutureLearn MOOCs. For each content week, students were assigned the value 1 if they viewed content, 2 if they posted a comment, 4 if they submitted the week’s assessment late and 8 if they completed the final assessment before the end of the week in which it was set (i.e. early, or on time). These values were added up to give a total for each week. We used the k-means algorithm to extract clusters from the engagement profiles directly, as a 6- or 8-imensional vector for each learner, to allow for the possibility of clustering by time of activity, as well as by total activity. Two methods suggested that seven clusters should be extracted. The mean silhouette width reached a local maximum at k = 7.
- **Dataset:**
- **Metrics:** As in the Coursera study, students’ activity in each week was assigned to one of four categories: - ‘T = on track’ if they undertook the assessment on time; - ‘B = behind’ if they submitted the assessment late; - ‘A = auditing’ if they engaged with content but not with the assessment; - ‘O = out’ if they did not participate;
- **Case study:** In order to investigate patterns of engagement within MOOCs employing social-constructivist pedagogy, we used data provided by FutureLearn. This is a company owned by The Open University that is currently in partnership with 39 universities, and organisations such as the British Library and the British Museum, to deliver free online courses. Four MOOCs ran soon after the launch of FutureLearn, at a point when the majority of marketing was taking place within the UK, so the majority of participants on each course were based in the UK. People who were active before the start date were removed from the dataset entirely, because the only people with access at that point were educators or FutureLearn staff. Two of the MOOCs had substantive structural differences from the others. MOOC2, the life sciences course, ran for six weeks, rather than eight. MOOC3, the arts course, ran for eight weeks, but included only three assessments, rather than one every week. Learners on MOOC3 posted substantially more comments than on the other courses.
- **Results:** Five of the seven clusters were found in substantially identical form in all four MOOCs. MOOC2 and MOOC3 between them generated three clusters that did not match those developed on MOOC1, and they are appended to the list as supplementary clusters. (Cluster I) Samplers: Samplers made up the largest cluster in all four MOOCs, accounting for 37%-39% of learners (56% on MOOC4). They typically visited about 5% of the course, with a few Samplers (11%-24%) visiting only a single step, although only 1% of Samplers on MOOC3 did so. They were active in a very small number of weeks, often including week 1, but not always – 25%-40% joined the course after week 1. Very few Samplers posted comments (6%-15%), and very few submitted any assessment, although the handful that did so typically did this in week 1; (Cluster II) Strong Starters: These learners completed the first assessment of the course, but then dropped out. Strong Starters made up 8%-14% of learners. All of them submitted the first assignment, but then their engagement dropped off sharply, with very little activity after that. A little over a third of them posted comments (35%-38%), and those who did so did not post very many (1.7-4.0), except on MOOC3, where 73% of learners posted an average of 13.7 comments; (Cluster III) Returners: These learners completed the assessment in the first week, returned to do so again in the second week, and then dropped out. Returners made up 6%-8% of learners, with the exception of those studying MOOC3, where this cluster did not appear. This is almost certainly because there are only three assessments in that course, with at least one week between each, so this pattern of activity was not possible. Almost all the learners in this cluster (>97%) submitted the assessment for week 1 and in week 2. This does not mean that they all visited in both weeks; some of them did the week 1 assessment late. No Returners explored all the course steps; the average amount of steps visited varied from 23% to 47%. After the first two weeks, there was very little activity indeed; (Cluster IV) Midway Dropouts: These learners completed three or four assessments, but then dropped out about half way through the course. Mid-way Dropouts made up 6% of learners on MOOC1, and 7% of learners on MOOC4. This cluster did not appear for MOOC2 and MOOC3, because their unusual structure (shorter, fewer assessments) meant that there were not enough assessments other than the final one for them to have this profile. These learners visited about half of the course (47%, 59%), and roughly half posted comments (38%, 49%), posting 6.3-6.5 comments on average; (Cluster V) Nearly There: These learners consistently completed assessments, but then dropped out just before the end of the course. Nearly There learners accounted for 5%-6% of learners on all four MOOCs. They typically visited over three-quarters of the course (82%-80%) and submitted assessments consistently (>90%) until week 5, and mostly on time (40%-75%), after which their activity declined steeply, and few completed the final assessment (3%-17%), none on time. Many of them posted comments (48%-65%), and those who did posted an average of 5.7-8.3 – except for MOOC3, where 80% of learners posted an average of 21.8 comments each; (Cluster VI) Late Completers: This cluster includes learners who completed the final assessment, and submitted most of the other assessments, but were either late or missed some out. Late Completers accounted for 6%-8% of learners, except on MOOC3, where only 0.2% of learners fell into this cluster. Each week, including the final week, more than 94% of this cluster submitted their assessment. The average proportion submitting late varied from 16% to 59%. However, more than three quarters of them submitted the final assessment on time (78%-90%). Fewer than half of these learners posted comments (40%-43%), apart from on MOOC3 where 76% did so, and those that posted made an average of 7.9-15.0 comments; (Cluster VII) Keen Completers: This cluster consists of learners who completed the course diligently, engaging actively throughout. Keen Completers accounted for 7% to 13% of learners, apart from on MOOC1, where 23% of learners fell into this cluster. All learners in this cluster completed all the assessments, including the final one, and almost all of them on time (>80%). On average, the Keen Completers visited >90% of the course content. They were also assiduous commenters. About two-thirds of this cluster (68%-73%) contributed 20.8-24.4 comments on average, although again MOOC3 stood out, with all learners commenting, posting an impressive average of 53.7 comments each;
- **Notes:** The original MOOCs, developed by Siemens and Downes, employed a connectivist approach to learning. This approach is signaled by references to them as cMOOCs. Connectivism considers learning to be social, technologically enhanced, distributed within a network and associated with the recognition and interpretation of patterns. Knowledge is developed as a result of experience and the self-organizing nature of appropriately designed networks. In turn, xMOOCs, the defining letter ‘x’ did not refer to a specific pedagogy, but to the role of these MOOCs as an extension to a previous offering. However, in many cases, this ‘extension’ version took the essential elements of education to be content and assessment, with input from educators bundled as part of the content. This led to an instructivist approach to teaching and learning in which ‘learning goals are predefined by an instructor, learning pathways structured by environment and learners have limited interactions with other learners’. In Siemens’ view, ‘cMOOCs focus on knowledge creation and generation whereas xMOOCs focus on knowledge duplication’. This paper is based on 10.1145/2460296.2460330.

#### **Collaborative multi-regression models for predicting students' performance in course activities** - [10.1145/2723576.2723590](http://dx.doi.org/10.1145/2723576.2723590)
- **Source:** Proceedings of the Fifth International Conference on Learning Analytics And Knowledge - LAK '15
- **Problem:** The problem of identifying students that are at risk of failing a course in order to allow for taking corrective actions can be addressed through analyzing historical data for students’ academic performance that is collected by the various Colleges and Universities.
- **Objective:** In this work we investigate the effectiveness of a class of collaborative multi-regression models for predicting the students’ performance at various course activities.
- **Methodology:** We wish to learn a model that predicts the student grades within the different course activities, like assignments and quizzes, given some input features. We achieve this using a collaborative multi-regression model inspired by 10.1145/1871437.1871674 and 10.1145/1557019.1557029. This approach learns a small number of linear models that capture the performance patterns of the different student groups and thus it has an advantage over learning a different model per student as it makes use of the similarities among the students (with respect to performance) and can better handle data sparsity issues. In this model, grade students in activity a is estimated. For each of the sixMoodle interaction features, we created five different features that measured the specified interaction at various time intervals prior to the activity due date. The dataset was randomly split into 80%-20% train-test subsets. The model was trained on the training set and evaluated on the test set. This process was repeated 4 times and the obtained results on the test set were averaged and reported. The model is evaluated in terms of the root mean squared error (RMSE) between the actual and predicted grades of the test set.
- **Dataset:** The dataset spans two semesters including 11.556 students, 832 courses belonging to 157 departments. Each student has registered in at least 4 courses. There is a total of 114.498 assignment submis-sions, 75.143 quiz submissions, and 251.348 forum posts.
- **Metrics:** Each student-activity pair is associated with a feature vector Features fall into three categories: (1) Student performance-specific features: – cumGPA: the GPA accumulated over the courses previously taken by the student; – cumGrade: the average grade achieved over all of the pervious activities in the course; (2) Activity and course-specific features: – activity type: is either quiz or assignment; – course level: describes the course’s difficulty; (3) Moodle interaction features: – n-init-disc: number of discussions initiated by the student; – n-engaged-disc: number of times that the student has posted to an open discussion; – n-read-posts: number of forum discussions that has been read by the student; – n-viewed-mater: The number of times the student has viewed some course material; – n-add-contrib: number of times the student has contributed to the course by adding something to the course page (e.g., a wiki-page); – n-other-accesses: number of times the student has made any other kind of access to the course pages;
- **Case study:** We experimentally evaluated the performance of these models on a large dataset extracted from the University of Minnesota’s Moodle installation. This study was performed with 6 MOOCs.
- **Results:** Results shows that the prediction accuracy improves as the number of regression models increases. A larger number of linear models with student-specific memberships allow the models to capture relations among the features that better describe different subsets of students. Using ten regression models, the obtained RMSE falls to 0.145. The results also show that the Moodle interaction features do provide predictive signals about students’ performance. The model that uses student, activity and Moodle features saturate faster than the incremental gains achieved by the other model. The course bias contributes more than the student bias to the prediction accuracy. We believe this is because the contributions of the student bias can be captured by the membership weights. For the case of two linear models, the RMSE obtained with and without the non-negativity constrains are 0.165 and 0.160, respectively. The binary features representing the departments were omitted as well as the features with zero or very low importance values. The features related to the forum activities had very low importance values as they only appear in a small fraction of the training data (10 to 25% of the training instances). The features describing viewing of the course material and students previous performance contribute the most to the predicted grades. The quiz, number of attempts and course level are important under M1 and not M2. However, features related to viewing the course material have higher importance under M2. The quiz, number of attempts and course level are important under M3 but not under M1 or M2, whereas the assignment and most of the features related to viewing the course material have higher importance under M1 and M2. The fact that we have some models concerned with assignments and others concerned with quizzes reflects that the activity type and properties can impact student performance.
- **Notes:**

#### **Analysis of Interactive Features Designed to Enhance Learning in an Ebook** - [10.1145/2787622.2787731](http://dx.doi.org/10.1145/2787622.2787731)
- **Source:** Proceedings of the eleventh annual International Conference on International Computing Education Research - ICER '15
- **Problem:** Learning to program can be difficult and failure rates can be quite high, especially in large classes at the university level. Our challenge is to provide high school teachers effective and efficient learning opportunities in computer science. Our research questions were 1) would readers use the new features, 2) would some features be used more than others, and 3) were the practice problems at the right level of difficulty?
- **Objective:** In this paper, we describe the activities we are using in our ebook and the educational psychology principles that support their use. We expected readers to be familiar with multiple choice questions and solve those within a few tries. We wanted to see if readers would attempt the Parsons problems and how many attempts it would take them to get them correct.
- **Methodology:** We use an observational study and log file analyses to design principles for more effective use of the activities. Our research group believes that electronic books (ebooks) have the potential to help prepare a large number of teachers to teach programming at a low cost.  We added additional interactive features based on educational choice psychology principles: audio tours of code, multiple questions with different feedback for each possible answer, and Parsons problems. We problems (multiple choice questions and/or added practice Parsons problems) after each worked example (Active Code and/or Code Lens). Audio tours were added to the Active Code feature. They highlight one or more lines of program code as the audio explanation of the line(s) plays. To examine the use of our added features within the ebook, we were an observational study of four teachers working through one chapter of the ebook after working through the earlier chapters at their own pace in order to observe teacher use of the features as they worked through the chapter to determine what difficulties they encountered that would not be obvious from a log file analysis. Another study was a log file analysis of use of the ebook by undergraduate and high school students to determine how many features were actually used, how many students attempted each practice problem, and how many solved each practice problem. We asked the teachers to complete the first three chapters of the ebook at their own pace and then notify us before starting the fourth chapter on turtle graphics and loops. We did the remote observations using the webinar software BlackBoard as the teachers worked through the chapter. We took notes on our observations, but also recorded the sessions so that we could check our notes against the recordings. We instructed the teachers to talk aloud as they worked though the interactive elements and asked what they were thinking if they were silent for more than a few seconds.
- **Dataset:**
- **Metrics:**
- **Case study:** We recruited teachers with less than six months of textual programming experience (in languages like Python or Java) to participate in an observational study with the ebook. We had 18 teachers fill out the registration and sign the consent forms. We disqualified two teachers due to too much textual programming experience (we allowed longer drag- and-drop programming experience). It was analysed logs from users of a open ebook. This ebook aims to teach Python. We received an anonymized version of the log file data in the fall of 2014 contained data for all student use of the ebook from May 17, 2012 to May 26, 2014.
- **Results:** All four teachers interacted with all of the worked examples (Active Code and Code Lens) and practice problems (multiple choice and Parsons problems). Only one of the teachers watched all of the videos. Another teacher said that he had watched the videos in the other chapters, but realized that they covered much the same material as in the text and said he thought that he could get through the practice problems without watching them. Only two of the four teachers listened to any of the audio tours. When we asked the other teachers why they hadn’t listened to the audio tours, they told us that they hadn’t noticed them. The teachers solved all of the Parsons problems in 1-2 tries. In regard the dataset received from the open ebook, across all Parsons problems, the average percent of people that eventually achieved a correct solution was 96.5%. The lowest percentage for a correct solution was students in the High School course. Parsons problems were easier to solve if they gave the structure of the solution. Most students correctly solved most of the Parsons problems after only a couple of attempts. The number of attempts it took for 50% and 75% of the people to get the problem correct, disaggregated by each Parsons problem. We see a drop off in the number of people who perform each activity over the course of the chapter. By the end of the chapter, students do less of everything. They do more of the lower cognitive load practice activities (e.g., multiple choice questions and Parsons problems) than the higher cognitive load (and more traditional) computer science practice activity, editing code. This is interesting because readers were explicitly told to modify the code after the first, second, and last Active Code examples, but less than half of the students did this. One of the interesting results of the data analysis is that the videos were not viewed as often as expected. Only 51% of the people who ran the first Active Code example in the chapter also watched the first video. By the third video in the chapter only 18% of the people watched the video compared to the number that ran the next Active Code example. Perhaps the students in the four courses we analyzed were not likely to watch the videos because they also had face-to-face lectures. The audio tours also had very low usage rates. Only 383 people played the first audio tour compared to 2,173 people that ran the first Active Code example. One possibility is that the user interface made it difficult for people to notice the audio tours and thus they did not use that feature.
- **Notes:**

#### **Educational Data Mining and Learning Analytics in Programming: Literature Review and Case Studies** - [10.1145/2858796.2858798](http://dx.doi.org/10.1145/2858796.2858798)
- **Source:** Proceedings of the 2015 ITiCSE on Working Group Reports - ITICSE-WGR '15
- **Problem:** The majority of the studies focus on simplistic metric analysis and are conducted within a single institution and a single course.
- **Objective:** This working group report provides an overview of the body of knowledge regarding the use of educational data mining and learning analytics focused on the teaching and learning of programming. We introduce a novel taxonomy to analyse replicating studies and discuss the importance of replicating and reproducing previous work. The main objectives of this work have been to: (i) systematically identify and analyse relevant works in the literature; (ii) meaningfully categorize these studies; (iii) identify discipline-specific challenges related to replication and reproduction.
- **Methodology:** We identified and surveyed existing literature in this area to gain an understanding of the different approaches being used. We analyzed the critical commonalities and differences in the tools and strategies being used to collect and analyze data from student work processes and artefacts, and examined issues and challenges related to reproducing prior studies and results.The researchers first labeled the extracts individually, after which the labels were discussed jointly to form a common consensus on the main themes and goals. The type of research was classified into one of six categories: case study; constructive research; experiment; study; survey research; or other. We performed a quality assessment of the reviewed pa-pers by selecting 15 questions to evaluate each paper on.
- **Dataset:**
- **Metrics:**
- **Case study:**
- **Results:** We analyzed 76 papers distributed over the time period from 2005 to 2015. The rate of papers being published on this topic has increased rapidly over the past ten years. There is a general increase in interest in educational data mining, in addition to the subfield of educational data mining of programming data. Three overall categories emerged: student, programming, and learning environment. - Articles that were placed within the student category were concerned with, for example, predicting student performance, student affect and estimating students’ knowledge; - Articles in the programming category were concerned with for example, identifying programming behaviour and strategies and errors made in programming; - While the learning environment category was related to (students’ use of) tools and automated testing, grading and feedback, and the like; Of the papers, 59 (78%) were considered as a study, where results reported data collected in a natural setting, such as in class activity. This is in contrast to only 11 (14%) that were classified as experimental research, where formalised experimental environments were set up in order to collect the data. This suggests that researchers prefer to collect programming process data in situ, possibly hoping to understand student programming behaviours as they occur in more natural environments. It is also important to highlight that 15 (20%) were categorised as constructive research, suggesting there is a perceived importance placed on reporting in detail the construction of tools to aid in the collection of programming process data. When considering the breadth of the research and data presented, of key interest is that the majority of studies (62 studies, 81%) presented work conducted within just a single institution. Only 15 (20%) studies presented work that could be considered longitudinal in nature, where data was collected from students over multiple offerings of a course or courses. More positively, 26 studies (34%) presented data captured across multiple courses. This however is still low, and while proof of concept studies are important, it highlights that for rigour of work in the area, more studies must be undertaken that seek to capture data from more than just a single offering of a course. Only 5 (7%) were studies that sought to replicate or reproduce previously published work. it was curious that many studies did not report the course context of their study. The minimum number of students recorded was 10, however one study reported having collected data from 265,000 students (although these may not be different students). The spread of numbers was fairly even. 17 studies (22%) reported having less than 100 students from which data was gathered. The largest proportion of studies had between 100 and 500 students involved (32 studies, 42%). Only 2 studies (3%) had between 500 and 1000 student participants, however 12 studies (16%) reported having process data from over 1000 students. 13 studies did not report specific numbers. In 47 of the studies (62%) the automated data collection was conducted by instrumenting the programming environment or the computer that the students used so that the programming environment automatically logs students’ actions for research purposes. In 21 studies (28%) there was no instrumentation of the programming environment. In these cases the research data consisted of materials that the stu-dents explicitly submitted. Eight of the studies (10%) did not clearly state if the programming environment was instrumented for data collection. The most often mentioned data collection instruments were WebCAT (nine studies), BlueJ or an extension of BlueJ (eight studies), and CodeWrite or its plugin (five studies). Descriptive statistical methods were most common (63 studies, 83%), with most studies at least reporting basic counts and percentages. 22 studies (29%) also conducted more detailed statistical analysis, such as inferential, Bayesian, t-test, and the like. 14 studies (18%) conducted some form of exploratory statistical analysis, such as correlation, regression, or factor analysis. Interpretative classification, such as classifying based on an existing classification scheme or one that is refined during the analysis, was present in 6 studies (8%). Interpretive qualitative analysis, such as the data-driven formation of qualitatively different categories, appeared in 12 (16%) papers. Automated classification was accounted for in 11 studies reviewed (14%). Finally, 3 papers did not present any formal analysis of data at all. In examining individual assessment questions, we found several items which the group of papers did particularly well on, and a number which were sorely lacking. The most-often-met quality metrics were “The hypotheses/aims/objectives of the study are clearly described”(with 79% of the papers meeting the metric) and “The main findings of the study are clearly described” (also 79%). This shows that the majority of papers had clear goals, were suc-cessful at demonstrating what they had achieved, and ade-quately shared their findings. The least-often-met quality metrics include “Confounding factors have been acknowledged and discussed” (25%), “Threats to validity are analysed in a systematic way and countermeasures were taken to reduce threats” (22%), and “Ethical issues were acknowledged and discussed” (3%). The first two items demonstrate that the studies do not often consider factors outside of the collected data, despite the fact that most studies did not collect or present demographic information (which could lead to many confounding factors). The last item demonstrates that the field has not yet engaged in a general discussion of the issues related to privacy and ethics that other branches of data mining have dealt with. A number of papers were also driven by a post-hoc analysis approach, such as course results and their connection to variables extracted from the context. Whilst such studies are valuable to the field, this also provides evidence of the relative immaturity of the field, as longitudinal studies, where the effect of an intervention would be measured across a number of courses or institutions, are virtually non-existent.
- **Notes:**

#### **Motivation as a Lens to Understand Online Learners: Toward Data-Driven Design with the OLEI Scale** - [10.1145/2699735](http://dx.doi.org/10.1145/2699735)
- **Source:** ACM Transactions on Computer-Human Interaction
- **Problem:** There has not been a systematic approach to identifying learners’ motivations or how they relate to subsequent behaviors. (RQ1) What motivates learners in MOOCs? (RQ2) How pronounced are individual differences in motivations between demographics or across courses? (RQ3) Which motivations are predictive of behaviors in MOOCs, and how predictive are they?
- **Objective:** We developed the Online Learning Enrollment Intentions (OLEI) scale.
- **Methodology:** To address RQ1 and RQ2, we require an appropriate framework for characterizing motivations. This work primarily relies on self-reported preferences among media users, and we follow this model in developing a self-report measure for enrollment intentions. To address RQ3, we define behavioral outcomes that reflect variation in activity among MOOC participants. An iterative process of response option development was based on open-response answers from learners in three different MOOCs. To efficiently categorize large numbers of responses with multiple raters, the process was crowdsourced using Amazon Mechanical Turk. To investigate what motivates learners in MOOCs (RQ1), we compared the percentages of learners in each course who reported each enrollment intention. We examined individual differences in learner motivations by gender, education, and age (RQ2). We investigated the relation between enrollment intentions and learner behavior in 10 MOOCs using the OLEI scale to capture enrollment intentions. The 10 courses are a subset of those described in the previous section, because some courses were ongoing at the time of analysis and some course surveys were anonymized to the extent that they could not be linked with behavioral course data (RQ3). To generalize across courses and platforms, which varied in terms of course length, requirements, and norms of engagement, we considered relative progress milestones, such as watching more than half of the video lectures in the course. Progress in the course was quantified by three milestones for the proportion of watched video lectures and the proportion of attempted assignments: the learner attempted more than 10% (50%, 80%) of assignments available in the course (excluding in-video quizzes); the learner attempted more than 10% (50%, 80%) of lecture videos available in the course. A robust measure of performance was particularly difficult to find given the large variation in assessment types and grading procedures. Instead, as a measure of satisfactory performance on the assessment tasks, we used whether the learner earned a certificate of completion. Social engagement was quantified by two measures of activity on discussion forums and a measure of endorsement by the learner community: the learner authored one or more posts/comments on the discussion forum; the learner authored over half as many posts/comments as the most prolific forum posters in the course; and the learner received one or more net votes on the forum. Among numerous candidate methods for analyzing these data, we opted for a boot-strapped linear regression with main effects for each enrollment intention and each course.
- **Dataset:** 71,475 responses were collected on the survey.
- **Metrics:**
- **Case study:** The OLEI scale was included in an optional course survey in 14 MOOCs offered by Stanford University through the Coursera and OpenEdX platforms. Between fall 2013 and spring 2014. The surveys were distributed in the first few weeks of the course to sample from an active learner population. Links to the survey were sent out via email and announced on the course Web site.
- **Results:** Two insights into learner motivation emerged from the overall distribution of motivations and across different courses. First, we gain a novel perspective on learners’ motivations for taking MOOCs using a formally constructed instrument. Earning a certificate of completion is the goal that is commonly emphasized in current MOOCs and expected to motivate many learners. And yet, in a typical course, fewer than half of the learners (45%) indicated an intention to earn a certificate. A quarter of learners expressed an intention to meet new people in the course. Another social motivation reported by close to 20% of learners in a typical course is to take the course with friends or colleagues. In a typical course, 38% of learners were motivated to enroll because the course was relevant to school or their degree program, and 35% because of relevance to their academic research. More than half of learners in a typical course (56%) reported being motivated due to relevance to their job, and more than a third (36%) were motivated by aspirations to change careers. More than a quarter of learners in a typical course (28%) were motivated to improve their English. More than half of the learners in a typical course (57%) reported being motivated by the prestige of the instructor or institution. Almost half of the learners in a typical course (44%) were motivated by their curiosity to experience an online course. Two thirds of learners in a typical course (66%) were motivated by an expectation to have fun and be challenged. Almost 9 out of 10 learners reported general interest in the topic (89%) and a desire for growth and enrichment (86%) as a motivating force. For instance, many learners enrolled in a course on mathematics for fun and to be challenged (80%), whereas fewer learners indicated this intention for enrolling in a course on quantum physics (10%). Across all 14 courses, gender differences in motivation were small—less than 5% point difference. Learners who held a college degree or a more advanced degree (more schooling) more frequently reported enrolling due to relevance to their job than those with some college or less schooling (11.2% point difference). However, learners with less schooling more frequently reported enrolling due to relevance to school (14.9% pt. difference), to experience an online course (9.5% pt. difference), improve their English language skills (9.1% pt. difference), and to earn a certificate (8.2% pt. difference). Consistent with this hypothesis, learners who intended to earn a certificate were more likely to watch most video lectures and attempt most assessments in the course. Learners with the intention to earn a certificate were additionally more likely to actively engage on the discussion forum, which was previously found to be associated with behaviors that are on the pathway to earning a certificate. The intention to earn a certificate was not predictive of an increased likelihood of earning a certificate. The intention to meet new people was a strong predictor of active engagement on the discussion board and even of receiving social recognition from peers in the form of votes. However, these learners were also less likely to engage with lectures and assessments, compared to learners who did not express the intention to meet new people. The intention to take the course with colleagues or friends were more likely to watch at least 10% of lecture videos and attempt at least 10% of assessments. Moreover, these learners were even more likely to earn a certificate. However, learners with this intention were less likely to heavily engage in the discussion forum, possibly because they communicated via other channels with their course colleagues. Learners who enrolled due to job relevance were more likely to watch at least 10% of lecture videos but were less likely to actively engage on the forum. In contrast, learners who enrolled because they aspired to a career change were more likely to watch more than 80% of video lectures and complete more than half of the assessments in the course. Whereas learners who enrolled due to job relevance seemingly sought to learn new skills or better understand a topic by watching a few lectures, those who enrolled for career change appeared to be more committed to learning a new skill or understanding new concepts to serve them on their new career path. Learners with the intention of improving their English skills were less likely to heavily engage on the discussion forum or receive votes from peers on their posts. They were also less likely to watch more than 10% of lecture videos. This may be a sign of a language barrier. Learners who enrolled out of general interest in the topic and those with the intention to gain personal growth and enrichment were more likely to attempt at least 10% of assessments. These learners appear to be less invested in fully engaging with the topic. In contrast, enrolling for fun and challenge was predictive of attempting more than half of the assignments and watching most lecture videos. Learners who enrolled for fun and challenge probably include lifelong learners who strive to learn new topics and learners who were already immersed in the topic and received pleasure from exploring it further.
- **Notes:** It is worth explicating certain limitations of the approach taken in the current work. First, data on learner motivations was collected for learners who responded to optional course surveys. Given that learners did not report their enrollment intentions at exactly the time  of enrollment, it cannot be ruled out that learners were affirming or rationalizing the behavior they had already practiced in the course. Another limitation of the current work is that conclusions are based on data that was exclusively collected in MOOCs.

### Elsevier
#### **E-learners’ personality identifying using their network behaviors** - [10.1016/j.chb.2015.04.043](http://dx.doi.org/10.1016/j.chb.2015.04.043)
- **Source:** Computers in Human Behavior
- **Problem:** Personality is a learner attribute that affects on learning achievement. The relationship between personality and academic performance has been examined in the context of distance and online education. There is no research in literature to identify learners’ personality based on their network behaviors in e-learning environment.
- **Objective:** A fuzzy system, that named ALPIS, is designed to identify learners’ personality based on their behaviors.
- **Methodology:** This research employs the Big Five Personality factors to measure learner personality. In this paper, we use network behaviors as another source to identify personality. We propose an automatic identification system based on online learner’s behaviors on Learning Management System (LMS). Fuzzy sets theory is a framework to handle vague and uncertain data. We try to build a fuzzy system based on learner’s online behaviors. We use NEO-FFI questions and relate its questions to some learner’s network: - Neuroticism: Number of friends (NF), Number of entrance to system in a week (NES), Delay in assignment delivering (DAD); - Extroversion: Number of friends (NF), Participation in chat rooms and forums (PICF), Adding posts in forums (APF); - Openness: Number of friends (NF), Difficulty level of exercises and examples (DLAE), Dedicating time for reading concepts and theories (DTCM), Dedicating time for reading examples and cases (DTEC); - Agreeableness: Number of group exercises (NGE), Participation in chat rooms and forum (PICF), Participation in troubleshooting groups (PTG); - Conscientiousness: Delay in assignment delivering (DAD), Number of entrance to system in a week (NES), Scores (SR), View of lesson news (VLN); Based on logged data from the underlying LMS, we find the minimum and maximum of each behavior. Using these data and lesson’s teacher assistances opinion we define three linguistic variables named ‘‘low’’, ‘‘medium’’, and ‘‘high’’ for each behavior. We use Spearman correlation coefficient to evaluate the relation between behaviors and personality dimensions. We implemented ALPIS system using MATLAB 2010 fuzzy tool. We categorized learners in same classes based on ALPIS scores and then compared NEO-FFI and ALPIS results to evaluate the ALPIS accuracy in personality prediction. The results were shown using 10-fold cross validation (precision, recall, F-value and accuracy) of three-class classification problem for five personality dimensions.
- **Dataset:**
- **Metrics:** Big Five Personality (BFP) factors is the best description of personality that referred as the ‘‘Big 5’’ or ‘‘NEO’’ including neuroticism, extroversion-intro version, openness, agreeableness, conscientiousness: - Neuroticism is a dimension of personality defined by stability and low anxiety at one end as opposed to instability and high anxiety at the other end; - Extroversion defined as a trait characterized by a keen interest in other people and external events, and venturing forth with confidence into the unknown and denotes activity, energy, and vigorousness; - Openness refers to how willing people are to make adjustments in notions and activities in accordance with new ideas or situations. It is associated with creativity, intelligence, imagination, and autonomy; - Agreeableness measures how compatible people are with other people, or basically how able they are to get along with others. It is associated to the tendency to be friendly, flexible, and cooperative; - Conscientiousness refers to how much a person considers others when making decisions. Conscientiousness represents characteristics such as being thorough, in a task-oriented way, systematically and carefully;
- **Case study:** The proposed fuzzy system was evaluated in a real e-learning environment. We considered 53 learners in a graduate online course. We used logged files from Tarbiat Modares University’s LMS (moodle2). We collected the behavior data of 8 sessions in one month.
- **Results:** We notice that the total consistency is at least 0.8 for neuroticism dimension, 0.91 for extroversion, 0.90 for openness dimension, 0.87 for agreeableness and 0.89 for conscientiousness dimension that means the consistency of ALPIS is being in meaningful range. It is seen that ALPIS can identify the learner’s personality in acceptable accuracy. Recall, precision, F-value and accuracy are in reasonable range.
- **Notes:** Identifying learners’ personality will help the designer create customized educational materials tailored to each individual. Analyzing outer behaviors can be used to personality analysis since behavior is the expression of personality. A common methodology in behavioral science is to use self-report questionnaires. Most of these instruments depend on the use of self-report because it offers an efficient way to collect information. These instruments are a cheap and quick way, in terms of time and cost, to obtain data and can be easily implemented. However some disadvantages are associated with the use of self-report instruments. For example, they affect by social desirability biases. People tend to present themselves in a favorable manner, especially when they make judgments about attitudes that are negatively valued such as irresponsibleness. Some people often believe that problems that they encounter, especially interpersonal conflicts, are not due to any fault of their own. In fact there are many reasons why questionnaire may not be entirely valid, such as: - Honesty: self-report questionnaires are relying on the honesty of their participants; - Introspective ability: if we suppose participants are hones, they may lack the introspective ability to answer an accurate response; - Understanding: the participants understanding may also varying regarding their interpretation of particular questions; - Most self-report measures in behavioral science relate to constructs, such a personality measures, that cannot be measured in interval units and are therefore always ordinal;

#### **Tap into visual analysis of customization of grouping of activities in eLearning** - [10.1016/j.chb.2014.11.001](http://dx.doi.org/10.1016/j.chb.2014.11.001)
- **Source:** Computers in Human Behavior
- **Problem:** ELearning is commonly supported by Learning Content Management Systems (LCMS) or Course Management Systems (CMS). This kind of systems generally store a trace of the students’ course activities and interactions in a database. However, this information is usually provided in a way that poses several problems, such as the high level of information detail and its detachment from the context of the courses and/or from the whole institutional eLearning platform. Is it possible to find a relationship between student participation and student performance by using visual analysis tools? If so, how can the information provided by visual analytics be used to improve learning processes?
- **Objective:** Levels of participation can be detected from numerical indicators. Our work builds on these relationships, and our approach is that of a visual analytic perspective of such relations. This study focuses on the analysis of the following cases: - Temporal relationship between the types of activities and student performance; - Discovery of patterns from frequencies activities based on their type and their relation to student performance;
- **Methodology:** A multidimensional interactive visualization is proposed. This representation uses the Semantic Spiral Timeline (SST) enhanced by the implementation of dynamic and interactive filtering of grouped activities. The main focus of the visual representation is the spiral timeline, which, in its simplest form, is just a sequence of color-coded events. These are ordered clockwise in chronological order starting from the center, where the earliest event is depicted the center of the spiral and the outermost event represents the most recent event. In the central part, the spiral is divided into seven sectors; this division corresponds to the filter chosen by the user.
- **Dataset:** The courses included a total of 91 students and almost 55,000 events. Collection of data spanned over different periods of time (course 1 from May 2011 to August 2011; and courses 2 and 3 from November 2011 to March 2012) to control for possible seasonal effects.
- **Metrics:**
- **Case study:** In this study, the SST was applied to courses from a Moodle platform, using several years of data stored in the CMS’ database. Our case study uses data from three different courses on different subjects, including technical and non-technical subjects (courses 1–3).
- **Results:** Independently of each course and each grouping activity, in most cases the days with higher activity for each grouping are different depending on the students’ grade (i.e., groups of students with the same grade have different peak activity patterns than those of students sharing a different grade). The higher the grade of students, also the number of days of highest activity; for instance, in course 1 students with ratings of 7–9 had interacted more on Mondays and Tuesdays, regardless of the type of activity, but students with a final grade of 10 extended their interactions from Monday to Friday, varying the way they interacted with the platform depending on the day of the week. Regardless of the time of the year in which the courses were held, the type of the courses and the students, all three courses seems to follow the same patterns. The findings from this study confirm the validity of visual analytics for the analysis and interpretation of learning processes. This study offered insights that might help answering the second research question.
- **Notes:** Visual analytics is the science of analytical reasoning supported by interactive visual interfaces. Visual analytics has some overlapping goals and techniques with information visualization and scientific visualization. Visual analytics, together with visualization tools, provides users with the capability to make meaningful decisions based upon the interpretation of the underlying data, since create visualization tools may help maximizing the human capacity to perceive and understand complex and dynamic information. The importance of providing teachers with "real-time data" about learning activities is essential to inform instructors about the modifications needed to optimize eLearning activities. The tools based on visual analytics techniques exploit the human visual capabilities of achieving rapid understanding of the correlations found in the data or the comparison of data elements and the free scan by means of visual tools.

#### **Discovering usage behaviors and engagement in an Educational Virtual World** - [10.1016/j.chb.2014.11.028](http://dx.doi.org/10.1016/j.chb.2014.11.028)
- **Source:** Computers in Human Behavior
- **Problem:** Tracing behavior patterns and measuring engagement.
- **Objective:** The goals of this research were to measure engagement indicators specific to the Educational Virtual World, identify user behavior patterns, determine the relationship between engagement factors and the behavior patterns identified in the proposed case study, and relate the observed rules and patterns to possible actions and decisions by Educational Virtual World managers.
- **Methodology:** To achieve these goals, this research aims to conduct an exploratory analysis of a dataset retrieved from an Educational Virtual World to identify usage patterns, engagement factors and user behaviors to provide insights into the perceptions and motivations of users about a technology such as an Educational Virtual World. A panel of experts in the creation and use of learning scenarios in Educational Virtual Worlds was interviewed to obtain some general heuristic rules and their application and validation or rejection based on trends and relevant data. The expert panel comprised five professors from the University of Salamanca. The exploratory analysis of data retrieved from the Virtual World was performed using Microsoft Excel software. Once the proposed factors for engagement and usage were measured, they were counted, sorted, and clustered in groups of users, relating each user with performance and the engagement factor measures.
- **Dataset:**
- **Metrics:** A behavior pattern can be established by observing significant overlap in the interaction and use measures by at least 15% of the users of the system. Significant overlap is established when a difference or deviation occurs in the measurements between users and other of at least 30% of the measurement.
- **Case study:** We used an Educational Virtual World developed by the University of Salamanca. This Virtual World, called USALSIM is designed to provide virtual practices and immersive learning experiences through a 3D environment. The data used were collected in two months, between November and December 2012, during testing prior to deploying this Virtual World to the general student population at the University of Salamanca. The tests involved 75 users, and data were collected on various aspects that might indicate usage characteristics, behavior and engagement indicators in the Virtual World. These 75 users were classified as follows: One system administrator; ten teachers; sixty-five students; Data concerning four of the key features and options in Educational Virtual Worlds were retrieved: voice chat-based features, text communication-based features (between users, messages between objects, etc.), session information (time in each, total number, average time, etc.), and movement inside the Virtual Worlds (between dif-ferent lands, islands, etc.).
- **Results:** Users who participate in more sessions use more resources and tools in the Educational Virtual World. Users who logged more times in the Virtual World often used a greater amount of resources. The number of resources used increases exponentially for users who connect often. Users who spend more time in the Virtual World tend to have more interactions with objects than with other users. Teachers use a greater amount of voice resources. Six teachers were among the 20 users with the greatest use of the voice service within the Virtual World.  There is no relationship between the number of sessions and total time spent in the Virtual World. Users who refuse this technology stop using it very early. Of the 20 users who spent the users who spent the lease amount of time in this environment, 16 had spent less than 3 min online. In summary, and in terms of engagement, the experts group believed the most relevant patterns were the following: users with more sessions use more resources and users who spend more time in the virtual world tend to interact more with objects than with other users.
- **Notes:** Engagement is related to the involvement of students in their learning process or tools. This involvement is evident in many ways, such as the time users spend learning, the number of activities undertaken, the relationship between results quantity and quality, etc.

#### **Applying social learning analytics to message boards in online distance learning: A case study** - [10.1016/j.chb.2014.10.038](http://dx.doi.org/10.1016/j.chb.2014.10.038)
- **Source:** Computers in Human Behavior
- **Problem:** The difficulty for teachers to keep track of students’ progress and activity in the course. (RQ1) Are social network parameters of the different actors related to student outcomes in online learning? (RQ2) Are global social network parameters related to overall class performance? (RQ3) Can visualizations from social network analysis provide additional information about visible and invisible interactions in online classrooms that help improving the learning process?
- **Objective:** We are interested in observing the emerging visible and invisible social network structures from student and teacher interactions in online classrooms and their relation to student performance.
- **Methodology:** For the Social Network Analysis and data visualization, we used Gephi 0.8.2, an open-source software for network visualization and analysis. Information about each student’s final continuous assessment grade was added to the dataset; grades were assigned a numerical value following the values provided by the UOC assessment guide. We then proceeded to calculate the different network parameters and the network visualizations in Gephi. We created two different networks: the first one included only post reading activity (the ‘‘read’’ network) while the second one included only replying activity (the ‘‘reply’’ network); for each network, we observed two scenarios: one including the interactions between all agents, and another which removed all teacher-related interactions – i.e. replies from/to teachers, reading of teachers’ posts, etc. – so that the effect of teachers’ activity could be taken into account in the analysis. The network was modeled as a directed network – each interaction between two nodes was assigned a value of one; therefore, in addition to degree, weighted in-degrees and out-degrees were also calculated. The data obtained from Gephi was later introduced in SPSS 18 for statistical analysis in order to calculate the correlation between centrality parameters and final grade, as well as differences in network parameters across the different classrooms for both the read and reply networks. We performed Pearson's correlation analysis and Spearman's analysis.
- **Dataset:** The data block comprised a total of 114.756 records of academic nature from 656 students – distributed along 10 classrooms, 10 consultant teachers and 1 coordinating professor.
- **Metrics:** Grades: A = 9.5; B = 7.5; C+ = 5.5; C- = 3.5; D = 1.
- **Case study:** We selected the semester-long course ‘‘Introduction to financial information’’ from the different degrees in Economics and Business Studies – Business Administration, Marketing, Work Relationships and Tourism Degrees – at the Open University of Catalonia (UOC). We used data extracted from the learning system’s activity log, essentially, all the interaction between students and teachers – and among students. The UOC uses a proprietary system, and therefore there is currently no plug-in or tool available that allows performing social learning analytics directly from the data stored in the database system; The number of students who passed the course is slightly below 60% – almost 90% of them with grades equal to or higher than B, with more than 40% of the students not passing the course. Almost a third of students abandon before the end of continuous assessment. In particular, in two classrooms – 8 and 9 – there were more than 60% of students who failed to pass the continuous assessment; classroom 4 was also remarkably exceptional since every student who completed the continuous assessment, except for one, passed the course. The best performing classrooms were 5 and 7, with a success rate higher than 70%.
- **Results:** We confirmed significant but low overall positive relation between the different centrality measures and final grade. We found a significant negative relation between closeness centrality and student outcomes. We found no correlation between any centrality measure and academic performance in classes 1–4 but we found significant and medium to high correlations (between 0.35 and 0.59) between weighed out-degree and final grade for classes 5–10. The results do not show significant differences between the best and worst performing classrooms. However, when differences between network parameters in the teacher-included/teacher-excluded scenarios are observed, the courses with higher drops in degree-related and path- related parameters correspond to those with higher drop-out rates. The positive correlation between closeness centrality and final grade – negative in the ‘‘read’’ net-work. When we performed the correlation analysis on a per class-room basis, the results were similar to those in the ‘‘read’’ network, with non-significant correlations for classes 1–4, and significant, moderate to strong – 0.28 to 0.59 – correlations between number of replies – and weighed-out degree – and final grade in classrooms 5–10. Students who initiated more new threads tended to achieve better results than those who just gave an occasional reply. Students who reply to other students tend to get higher grades than those who interact more with the instructor. Classrooms with the most active consultant teachers – higher number of posts and replies – correspond to the worst performing groups; but lack of instructor’s activity did not assure better group performance, since the classrooms where the teacher intervened the least had high drop-out rates – e.g. classrooms 3, 4 and, to a lesser extent, 10; it must be noted, however, that students who got more replies from consultant teachers tended to get higher grades. (RQ1) Although the overall results showed a moderate relation between centrality measures and academic performance across the whole sample we actually obtained mixed results when the analysis was performed on a per classroom basis. (RQ2) The results showed no evidence of that relation. However, we found that higher drops in degree-related and path-related parameters when teachers’ activity was excluded were more typical of classrooms with higher drop-out rates. Besides these findings, we could not find any valid variable which helped us to group classrooms with similar network behaviors. (RQ3) In virtual classrooms with high number of students it may be difficult to explain the results of SNA, but we showed how different combinations of SNA visualizations can provide very rich and varied information at a glance and facilitate the identification of relevant agents (i.e. disconnected or at-risk students, experts, etc.), and how the instructor can use this visual information to change the course dynamics if needed. By including the ‘‘read’’ network in our analysis, we also were able to extract useful information on listening behaviors (i.e. lurkers) which are generally ‘‘invisible’’ but that also contribute to knowledge building
- **Notes:** Learning dashboards, which integrate the different educational data and represent it graphically, may offer support for students in their autonomous learning as well as help teachers to perform student tracking and understand ‘‘what is happening in their online classes’’. We have obtained mixed results suggesting that the sole use of SNA parameters for predictive purposes might not be adequate, and that additional research is needed in order to effectively design predictive systems based on SNA – such as additional variables or conditions under which SNA can be a valid predictor of academic performance. However, the visualization of the ‘‘read’’ and ‘‘reply’’ networks have highlighted the potential of social learning analytics to identify relevant actors and roles in the learning process, as well as to assess the role of teachers in online distance education.

#### **Fuzzy cognitive mapping of LMS users’ Quality of Interaction within higher education blended-learning environment** - [10.1016/j.eswa.2015.05.048](http://dx.doi.org/10.1016/j.eswa.2015.05.048)
- **Source:** Expert Systems with Applications
- **Problem:** Higher Education Institutions are facing the need of constant monitoring of users’ interaction with LMS, in order to identify key areas for potential improvement.
- **Objective:** This study addresses the hypothesis that the structural characteristics of a Fuzzy Cognitive Map can efficiently model the way LMS users interact with it and, further, evaluate it by estimating their Quality of Interaction within a b-learning context.
- **Methodology:** Under the Fuzzy Cognitive Map (FCM) approach, a given system is defined as a collection of concepts that possess influential interconnections, expressing cause-effect relationships, which are quantified through a weighting correspondence. The user interacts with the LMS Moodle and 110 metrics are acquired. These metrics are then categorized into 14 categories, denoted as C1;C2; ... ;C14. These 14 concepts are considered the inputs of the FCM within the FCM-QoI (Quality of Interaction) model and the additional FCM-QoI concept is considered its output The implementation of the whole analysis of the FCM- QoI model was carried out in Matlab using custommade programming code.
- **Dataset:**
- **Metrics:**
- **Case study:** The LMS Moodle data for the validation of the proposed FCM-QoI model were drawn from a b-learning environment related to five undergraduate courses (Sport Sciences, Ergonomics, Dance, Sport Management and Psychomotor Rehabilitation) offered by the first Author’s affiliated Higher Education Institutions (HEI).
- **Results:** In an effort to further deep in the derived results, a static analysis of the derived main FCMs under the time-independent scenario. Focusing at the professors’ case, the main concepts are C1;C4;C5; C6;C7;C12 and C14 which are directly connected with the proposed model. C12: {Edit/Delete (E/D)} is the concept that negatively affects most the P ?QoIFCM, whereas C1: {Journal/Wiki/Blog/Form (J/W/B/F)} is the one that positively affects most the model, closely followed by C4: {Course Page (CP)}. With regard to the Students’ case the main concepts are C2; C3; C5; C7 and C14, which are directly connected with the model. C3: {Submission/Report/Quiz/Feedback (S/R/Q/F)} is the concept that negatively affects most the model, whereas C7: {Resource/Assignment (R/A)} is the one that positively affects most the model. Each identified feedback cycle was allocated a positive or negative sign which was determined by multiplying the signs of the arcs present in each feedback cycle. In fact, positive (negative) feedback cycle behavior is that of amplifying (counteracting) any initial change, leading to a constant increase (decrease) in case an increase is introduced to the system. In regard Professors’ case, this cycle could interpreted as follows: the involvement of professors in C6: {Post/Activity (P/A)} could affect the model and then lead to the engagement of C5: {Module (M)}, which could evoke C1: {Journal/Wiki/Blog/Form (J/W/B/F)} activity in the related area; hence, involvement of C4: {Course Page (CP)}, leading back to more C6: {Post/Activity (P/A)}. Whereas Students’ case involves the , C7: {Resource/Assignment (R/A)} and C12: {Edit/Delete (E/D)}, implying that, Resource/Assignment affects the model, which then influences the content alterations (Edit/Delete), which could affect back the way Students could handle the R/A. The negative sign indicates that in this cycle, the increase in R/A will not necessary lead to a self-sustained action, but as an overall, it would negatively be affected by the changes in the model and and C12; hence, careful design in the students’ R/A activities should be considered, without boosting the need for any unnecessary alterations (E/D).
- **Notes:**

#### **Investigating student motivation in the context of a learning analytics intervention during a summer bridge program** - [10.1016/j.chb.2014.07.013](http://dx.doi.org/10.1016/j.chb.2014.07.013)
- **Source:** Computers in Human Behavior
- **Problem:** Retention has been seen as a critical issue in higher education for decades. (Q1) To what extent, if any, do students’ motivational orientations change throughout the course of a summer bridge program? (Q2) What factors predict the changes in motivation, if any, that occur over the course of a summer bridge program? (Q3) What is the relationship between advisors’ use of a learning analytics-powered Early Warning System (EWS) and their students’ academic performance during a summer bridge program?
- **Objective:** This study investigates students’ motivational orientations and how assessment of those orientations can inform a learning analytics-based intervention employed during a summer bridge program to support data-driven decisions and actions of the academic advisors.
- **Methodology:** We collaborated closely with the leaders and staff of a summer bridge program to iterate the system design of our learning analytics-powered early warning system (EWS) intended for academic advisors. This EWS utilizes information from the institutional learning management system (LMS) to inform just-in-time advisor interventions designed to identify student’s maladaptive academic behaviors and study habits before students fall further into academic jeopardy. Two online surveys were distributed to Bridge students, one at the start-of-term (12 items) and one at the end-of-term (22 items). To measure Bridge students’ motivational orientations, we used the Patterns of Adaptive Learning Scales to measure students’ achievement goal orientations in both surveys. While the two surveys described above represent the main data sources described in this paper, our overall design-based research program investigates how academic advisors in Bridge and other at-risk student programs use our EWS, called Student Explorer. This system was designed to provide advisors information about their students’ engagement and performance to facilitate timely interventions. Students’ weekly progress updates were presented through a dashboard that provided representations of student engagement and performance that allowed the advisors to readily identify students who were succeeding in any given course, beginning to show signs of falling behind, or struggling with their coursework. Log data captured from academic advisors’ use of Student Explorer and corresponding data from the student meeting appointment system were matched with student survey data.
- **Dataset:**
- **Metrics:** Our investigation of the relationship between advisors’ use of the Student Explorer EWS and students’ academic performance as measured by course grades.
- **Case study:** This study focuses on the Summer Bridge Program that is situated within a large four-year, more selective, lower transfer-in, and primarily residential university in the Midwestern United States with very high research activity. Bridge was made as an effort to assist non-traditional students’ transition from high school to college by providing highly structured introductory coursework. Two hundred and sixteen students completed Bridge in the Summer 2013 term (3 students dropped from the program during the term).
- **Results:** (Q1) There were no significant differences between pre-bridge performance-approach scores and post-bridge performance-approach scores. There were also no significant differences between pre-bridge performance-avoid scores and post-bridge performance-avoid scores. There was statistically significant decrease, however, in students’ reported pre-bridge mastery scores and post-bridge mastery scores. (Q2) we specified a multiple regression model of students’ change in mastery. Students’ incoming mastery was positively associated with their outgoing mastery. However, students’ self-reports of how often their advisors showed them their Student Explorer data negatively predicted the change in mastery over the course of bridge. Advisors’ logged (actual) use of the Student Explorer EWS before, during, or after meetings with each student was not a significant predictor of the change in mastery. (Q3) We used multiple regression to model which factors would predict student outcomes in each of courses (English course; Math A: a remedial intermediate algebra course; Math B: a college-level intermediate algebra course), and controlled for demographic characteristics (e.g., gender and athletic status), academic achievement measures (e.g., ACT scores, math pre-tests, and math midterms), high school experiences (e.g., perceived quality of high school teachers in general, and math/ English preparation, specifically), and encouragement by family and friends. The results indicated that athletic status negatively predicted English course grades, while students who stated that they had excellent high school teachers positively predicted English course grades. These variables, however, were not predictive of math course grades. For students in the remedial Math course (A), mastery orientation negatively predicted course grade. The extent to which Bridge advisors viewed students’ data via Student Explorer after meeting with students also negatively predicted students’ Math A course grades. For students in the college-level Math course (B), perceived family encouragement negatively predicted course grades. The midterm score positively predicted the final course grade for both Math courses. Advisors spending more time outside of meetings trying to help the students whose formative course performance was already flagged by Student Explorer as far below the course average.
- **Notes:** The size, selection biases of an at-risk student population, and short duration of the Bridge program are all factors that limit the generalizability of this study. As future research: (A) what habits and behaviors students bring to their higher education contexts, (B) how those habits and behaviors mediate students’ goals, performance, and effort, (C) what suggestions advisors make to remediate students’ unproductive habits and behaviors based on the data presented in systems like the EWS, and finally, (D) what actions, if any, students take in response to their advisors’ suggestions.

### IEEE
#### **Predicting student personality based on a data-driven model from student behavior on LMS and social networks** - [10.1109/ICDIPC.2015.7323044](http://dx.doi.org/10.1109/ICDIPC.2015.7323044)
- **Source:** 2015 Fifth International Conference on Digital Information Processing and Communications (ICDIPC)
- **Problem:** Although the same learning resources are provided to all students, each student is unique in their individual the learning in preferences, learning styles, their progress process and their individual background knowledge of the course material.
- **Objective:** The main objective of this study is to improve the quality of teaching and learning processes in the e-learning system by means of information and communication technologies.
- **Methodology:** The data can help us create a model that predicts and classifies the student’s personality and preferences using data mining tools, different in which researchers can test our model with classification methods. WEKA was used for classification. The first phase is to predict student personality type (PT); we defined and initialized our counter of 8 preferences measured in the Myers-Briggs Type Indicator (MBTI) model. The second phase is to predict student dominant preference (DP) which is based on the student PT; every student personality type has one of 4 dominant preferences. The experiment used 10 classification algorithms in order classification accuracy. The determine the highest to algorithms were NaiveBayes, BayesNet, Kstar, Random forest, J48, OneR, JRIP, KNN/IBK, RandomTree, and Decision Table. In order to measure its accuracy, precision, recall, F-value, true positives (TP) rate, and false positives (FP) rate, which are defined by equations by means of a confusion matrix.
- **Dataset:**
- **Metrics:** Based on the Jungian’s psychological types, Myers-Briggs evaluates personality types and preferences through four aspects of personality: (1) Extraversion (E) or Introversion (I); (2) Sensing (S) or Intuition (N); (3) Thinking (T) or Feeling (F); (4) Judging (J) or Perceiving (P); The combined sets of these different preferences provide 16 different personality types and are typically symbolized by four letters to represent a person’s movement on the four scales. Students’ dataset attributes: N_VisitedPages; Total_TimeSpent; N_Comments; N_Likes; N_Shares; N_Posts; N_ChatSessions; N_Early Assignment Submission; N_Late Assignment Submission; MBTI Type (student Myers-Briggs Type Indicator (MBTI) 16 types); Dominant Preferences (student Dominant preferences (class S, class F, class N, class T).
- **Case study:** E-learning uses the social network as one of its tools to provide and improve the communication in the learning process. Instructors are able to improve the to create closed groups for each class communication between the instructor and students and among the students themselves. We selected Facebook for the social present study as it is the most popular form of networking amongst the student population. In this study, the data was collected from the Business College of the German university in Cairo (GUC) that serves over 1900 students in Egypt. The data was collected from the LMS (Moodle) provided by the college as well as Facebook groups created by the instructors for their students. The data was collected for the academic year 2014-2015 from 240 students. Each of these students took about 6 courses per semester.
- **Results:** The classification accuracy using OneR technique had the highest value of 97.40%, followed by Random forest and then J48, achieving accuracy of 93.23%, and 92.19%, respectively.
- **Notes:**

#### **Can student engagement be measured? And, if so, does it matter?** - [10.1109/FIE.2015.7344077](http://dx.doi.org/10.1109/FIE.2015.7344077)
- **Source:** 2015 IEEE Frontiers in Education Conference (FIE)
- **Problem:** How a student participates, to what degree will it be possible to identify a suite of measures that can quantify engagement so we can identify underperforming students earlier in the semester? how can we use learning analytics to improve the pedagogical value of the large lecture hall? In other words can we identify what it is that successful students do in class that is different from less successful students?
- **Objective:** To measure student engagement.
- **Methodology:** (1) Collecting Intrinsic Data. Students could use LectureTools, which allowed the students to: - Type notes synchronized with the lecture slides; - Answer questions posed by the instructor; - Indicate confusion; - Pose questions to the instructor and view responses; - Print lecture slides and notes for off-line review; For this study the data collected during the semester was separated into three periods representing student participation between the beginning of the semester and the first exam, between the first exam and second exam, and between the second exam and third. (2) Collecting Extrinsic Data. It is reasonable to expect that there are influences on student performance that are unrelated to the conduct of the course. These could be the incoming knowledge base and confidence level of the student, their level of emotional or physical wellness and/or their level of interest in the content of the course. All the data were extracted and then linked to students’ hourly exam grades. Processing was accomplished using Microsoft Excel for categorical frequency analyses and Weka for statistical clustering, categorization and linear regression.
- **Dataset:**
- **Metrics:** Data collected in this class was divided between those extrinsic and those intrinsic to the course: - Extrinsic Measures: incoming grade point average, interest in course content; - Intrinsic Measures: attendance in class, participation in class activities, correctness in class activities, Taking notes, posting questions, Indicating confusion;
- **Case study:** The course, Extreme Weather, was offered at the University of Michigan in the winter 2014 semester with 168 students. The course was both streamed live and recorded every class day, so students could have the choice of physically coming to class or viewing lecture remotely and synchronously.
- **Results:** The data from this class do show a relationship between incoming Grade Point Average (GPA) and exam grades though this also illustrates a fair amount of variability in the grades regardless of GPA. The incoming GPA of the students explains between 23 to 31% of the variance in exam grades. This information may be of interest to an instructor as a potential harbinger of difficulty a student’s GPA. About 150 of the 160 students in the course used LectureTools and of those about 130 attended every day of class either in person or remotely. The results show little relationship between missed classes and grades, but the numbers of students missing class four or more times was relatively low for each exam (11, 13, 16, respectively). This would suggest that on the whole attendance was a poor indicator of student outcomes for this course. The number of slides for which each student took  notes was calculated. The average grades  associated with various bins of notated slides generally increase with the more slides containing notes. The hypothesis is that if students “attend” class but are not answering questions they are likely disengaged. There is a positive relationship between number of questions answered and resulting grades with an explained variance (r2) of 0.20, 0.23 and 0.33 for Exam 1, Exam 2 and Exam 3, respectively. This suggests that simply “attending” class is less predictive than participating. This is an  important distinction as many instructors use attendance as a measure of participation and even grading.
- **Notes:**

## 2016

### ACM
#### **Macro Data for Micro Learning: Developing the FUN! Tool for Automated Assessment of Learning** - [10.1145/2876034.2893422](http://dx.doi.org/10.1145/2876034.2893422)
- **Source:** Proceedings of the Third (2016) ACM Conference on Learning @ Scale - L@S '16
- **Problem:** Researchers have discussed the analysis of learning events at the level of log data to understand learning that is ubiquitous, heterogeneous, and complex. Unfortunately, while the field of education data mining supplies methodologies for analyzing these data, the analytic techniques are often difficult for researchers without advanced data science experience.
- **Objective:** Make a tool to provide access to an increasing amount of data from digital learning environments, would benefit from a tool to help manage and analyze all of this data.
- **Methodology:** We created the FUN! tool. The FUN! tool automates the elements of the analysis that are repetitive and also makes the other elements (such as adapting to different data sources, applying different analysis techniques, and reporting results in a variety of ways) simple to change. The FUN! tool is essentially an engine that automatically moves data through a workflow process which is divided into four parts: adaptors, selectors, measures, and reporters.
- **Dataset:**
- **Metrics:**
- **Case study:** We created FUN! tool and applied our initial version of it to three real project data sets from several different digital learning environments (e.g., Scratch).
- **Results:** The FUN! tool allowed us to (1) organize our workflow process from start to finish, (2) record log data of all of our analyses, and (3) provide a platform to share our analyses with others through GitHub.
- **Notes:** Poor work!

#### **Applying classification techniques on temporal trace data for shaping student behavior models** - [10.1145/2883851.2883926](http://dx.doi.org/10.1145/2883851.2883926)
- **Source:** Proceedings of the Sixth International Conference on Learning Analytics & Knowledge - LAK '16
- **Problem:** Detecting undesirable learner behaviors, and profiling learners. Differences in learners’ behavior have a deep impact on their performance. There is a need to detect and identify these differences and build suitable learner models accordingly.
- **Objective:** In this paper, we present a method for dynamic student behavioral modeling based on the analysis of temporal, student-generated trace data. The goal was to unobtrusively classify students according to their time-spent behavior during assessment processes.
- **Methodology:** We explored a large range of supervised learning classification algorithms (SLA) (namely Artificial Neural Networks-ANNs, Support Vector Machines-SVMs, Naïve Bayes-NB, k-Nearest Neighbors-kNN and treeBagger) on a dataset consisting of time-based data (including total time to answer correctly, total time to answer wrongly, total idle time, effort and goal expectancy) using as target values (class labels) the students’ performance during a Computer-Based Assessment (CBA) process. We configured a testing mechanism and a tracker that logs the students’ temporal data. For the purposes of the examination, we used 34 multiple choice questions. Each question had two to four possible answers, but only one was the correct. The participants could skip or review the questions and/or alter the submitted answer. Finally, the participation to the midterm exams procedure was optional. As an external motivation to increase the students’ effort, we set that their score would participate up to 30% to their final grade.
- **Dataset:**
- **Metrics:** A method commonly used to evaluate the performance of a classifier is cross validation. We used a stratified k=10-fold cross validation with n=100 iterations for estimating the misclassification (test) error. F-score (or F- measure) is a measure of a test's accuracy. It considers the precision and the recall of the test to compute the score.
- **Case study:** Data were collected from a total of 259 undergraduate students (108 males [41.7%] and 151 females [58.3%], aged 20-27 years old (M=22.6, SD=1.933, N=259) from the Department of Economics at University of Macedonia, Thessaloniki, Greece. 12 groups of 20 to 25 students attended the midterm exams of the Computers II course.
- **Results:** These results demonstrate that all methods achieve high classification performance, since the true test error varies from 0.24 (ENS method) to 0.28 (kNN method). Further to that, the sensitivity measure is close to 1 in most cases (0.95-0.96) and the F-score is also high (0.85-0.88). ENS method provides better classification results compared to the other methods, while the kNN and NB methods also achieve satisfactory results. We examined how the highly performing methods (ENS, kNN and NB) change their output when applied to more input variables (predictors). ENS does not seem to be affected by the additional features, providing results similar to the previous ones. The performance of the other two methods is slightly reduced when the number of predictors increases.
- **Notes:**

#### **Teaching Programming: Understanding Lecture Capture YouTube Analytics** - [10.1145/2899415.2899421](http://dx.doi.org/10.1145/2899415.2899421)
- **Source:** Proceedings of the 2016 ACM Conference on Innovation and Technology in Computer Science Education - ITiCSE '16
- **Problem:** The effects of lecture capture in the area of computer programming is a relatively under researched area
- **Objective:** The goal of this action based research project was to capture and quantitatively analyse the viewing behaviours and patterns of a series of video lecture.
- **Methodology:** The video capture of lecture was presented as 10 – 30 minutes snippets that consisted of the audio of the lecturer and video of on-screen projected content. A comparative study of the general performance of the videos was made using viewing (hits) figures. Additionally an analysis of each of the pedagogically different teaching formats enabled a study of the comparative performance of each section of the video.
- **Dataset:**
- **Metrics:** Videos with following formats: - Slides (Theory); - Slides (Code); - Code (Walkthrough); - Code (Development);
- **Case study:** This research was conducted across two full Java programming modules over two semesters within a computing degree in Queen’s University, Belfast. The modules employed a series of video lecture captures in which the audio of lectures and video of on-screen projected content were recorded and made available to the students via a closed YouTube channel. The channel had analytics which offered engagement reports for the videos. The study was conducted with a cohort of 80 post graduate students. A total of 55% of the students in the study stated that with first time views of Lecture Capture videos they watched the whole way through without stopping regardless if they had attended the lecture or not. However with subsequent views only 14% of the students in the survey watched the complete video.
- **Results:** The Audience Retention (AR) decreases with video length. The observed AR suggests that an optimum time for a video would be no more than 10 minutes; The same activity over a prolonged time period will generally cause a gradual disengagement regardless of the pedagogical format; Most videos display a gradual decline over time but many present a significant drop-off towards the end; Typically the slides with predmoninant text and accompanying narrative description are initially well engaged with, however they soon suffer a sharp decline in engagement resulting from skip. This is especially evidenced when the slide forward activity. This teaching remains on screen for longer than 30 seconds. format performs worst in terms of holding the students’ attention on video and is likely to have the same affect during the live lecture. On review with the the lecturers it was discovered the peaks students and corresponded to complex areas and so required a replay; Videos with slides (code) performs better than theory limited slides. Peaks in slides with code were related to detailed explanations that associated theory to code; In regard code walkthrough and code development, the students report that it is the code related sections that they were most likely to rewind and replay, again due to the active nature they are using it for i.e. following along. The walkthroughs tend to have a higher engagement than the active coding sections; Summaries are important but need to be addressed as conclusions.
- **Notes:** The study is limited to the two Java programming modules over a one year period and as such is representative only of that sample. It is also limited to a study of the lecture capture of programming code development lectures.

#### **Exploring the relation between self-regulation, online activities, and academic performance: a case study** - [10.1145/2883851.2883883](http://dx.doi.org/10.1145/2883851.2883883)
- **Source:** Proceedings of the Sixth International Conference on Learning Analytics & Knowledge - LAK '16
- **Problem:** Several research areas related to education have tried to avoid the one size fits all problem proposing multiple techniques to adapt a learning experience to the needs of each learner.
- **Objective:** This paper attempts to explore further this middle space through a study to explore the relationship between self-regulated learning, digital traces of student interaction with online activities, and academic performance.
- **Methodology:** Students were asked to answer the questions included in the Self-Regulation Section of the Motivated Strategies for Learning Questionnaire, consisting on a 7-point Likert scale from nine items. At the end of the semester the accumulated number of events was obtained for each student. The academic performance was obtained from the final course mark. This mark was calculated by taking into account the following assessments: online lecture preparation activities (10%), online tutorial preparation and participation (10%), written report about laboratory session (5%), collaborative project (15%), mid-term examination (20%), and final examination (40%), on a scale from 1 to 100. The data analysis has been carried out in five stages. The first stage included Exploratory Factor Analysis (EFA) of the data obtained from the Self-regulation strategy use questionnaire (SRSUQ). In the second stage we examined the reliability of the retained scales. The third stage included the use of correlation analysis to see the relationship between self-regulated strategy use, the interaction with the online activities, and academic performance at the variable level. In the fourth stage we first conducted a hierarchical cluster analysis to identify subgroups of participants where the similarities and the differences in their self-regulated strategy use and academic performance could be maximized. On the basis of the identified clusters, one-way ANOVA was performed to see whether learners in different clusters exhibit different patterns for interactions with online activities.
- **Dataset:**
- **Metrics:** - Access to any HTML page of the course material(Resource); - Expand/Collapse of a section within a page. Some pages had headings with the content collapsed that was exposed when clicking in the title (Col-Exp); - Events while using the embedded videos: play, pause, begin, and end a video (Video); - Events while answering the multiple choice questions next to the videos (VMCQ); - Events while answering the multiple choice questions in the course notes (MCQ); - Access to a page containing a dashboard illustrating the level of interaction with the course activities (Dboard);
- **Case study:** The study was conducted in a semester-long course on computer system for first year engineering students. By the end of the course, students should be able to design, build and configure an electronic system, demonstrate their understanding of how computers work, and write reports about the design process and its results. The course was designed as a blended learning experience. The study was conducted with 145 first year students who were studying a four year Bachelor of Engineering Degree in a research-intensive university in Australia.
- **Results:** The final two factors included 4 items in the Positive Self-regulated Strategy (PSRS) scale, and 3 items in Negative Self-regulated Strategy (NSRS) scale. While PSRS did not significantly relate to student academic performance the NSRS significantly and negatively associated with academic performance. However, the PSRS showed significant and positive association with three of the events registered in the online environment, namely Dboard, Expand/Collapse-of sections in the course notes, and Col-exp. This association means that the more students adopted positive self-regulated strategies in the course. NSRS and the access to online activities turned to be non-significant. Additionally, academic performance was found to be positively related to most of the indicators derived from online interactions, including Dboard, Col-exp, Resource and MCQ. This positive correlation suggests that the more frequently an individual engaged with these learning activities, the more likely they were to obtain a higher course score in the course. The 145 students were classified into a group of 86 High Self-regulated and High-performing students (cluster 1), and a group of 59 Low Self-regulated and Low-performing students (cluster 2). The students in cluster 1 differed from those in cluster 2 in terms of the PSRS use, NSRS use, and the academic performance. To be more specific, the High Self-regulated and High-performing students reported adopting more of the positive self-regulated strategies less of the negative self-regulated strategies and achieved a relatively higher final score than the Low Self-regulated and Low-performing students. The two groups of students had significant differences in the frequencies of interactions with five of the seven online activities. The factors Dboard, Resource, and Exercise had statistically significant differences between the clusters. The two remaining factors Video and VMCQ did not have a statistically significant difference among clusters. The High Self-regulated and High-Performing students interacted significantly more frequently with those five activities than the students with Low Self-regulated and Low performance. The results showed that while PSRS did not show a significant correlation with academic performance, NSRS did have a negative relation with academic performance. Additionally, the academic performance was found to be positively associated with frequencies of interactions with a number of online learning activities. The cluster analysis further identified two groups of students based on their self-reported use of self-regulated strategy and academic performance. The students in the High Self-regulated and High performing group adopted more PSRS, less NSRS, obtained higher final marks in the course, and tended to interact more frequently with online learning activities. The results are the opposite for the Low Self-regulated and Low-performing cluster.
- **Notes:**

#### **Analysing engagement in an online management programme and implications for course design** - [10.1145/2883851.2883894](http://dx.doi.org/10.1145/2883851.2883894)
- **Source:** Proceedings of the Sixth International Conference on Learning Analytics & Knowledge - LAK '16
- **Problem:** Q1: Effect of staff engagement on student engagement; Q2 : Effect of student engagement on student performance;
- **Objective:** We report on the design and application of a range of analyses of the interaction data.
- **Methodology:** For students, engagement events were categorized according to their initiation mechanism and included material-initiated, tutor-initiated and student-initiated events: Material-initiated events were events that were the result of prompts within the course materials; Tutor-initiated events were replies by students to forum and newsfeed topics initiated by course staff; Student-initiated engagement events, defined as the number of forum topics initiated by individual students. To complement the engagement data, performance and demographic data was collected for all students. Performance data included the marks obtained for the coursework and exam components of the two courses as well as the overall mark. We wished to determine whether the level of engagement exhibited by students was reflected in the marks obtained in assessment activities. This required a measure of individual student engagement, and a natural choice was to use the overall engagement as defined for Question 1, aggregated at the student level.
- **Dataset:**
- **Metrics:** For staff engagement, the principal metrics were the number of forum discussion topics initiated by staff members and, more implicitly, the overall course design, principally the distribution of activities.
- **Case study:** For the 2014-15 session of the new postgraduate degree in Management that is our focus here, 67 students from 29 countries on five continents were admitted. The median student age was 35. All students had significant work experience, with a median of 10 years. Our study here focusses on the first two courses. Both courses followed the same delivery structure: 2 weeks pre-study, 10 weeks term time study, and a 4 week exam period.
- **Results:** In regard Q1: Student-initiated engagement was relatively stable over time. For the Accounting course, there was a spike in engagement in the weeks following the end of the term and immediately preceding the examination period, suggesting that students were engaging more frequently in discussions about the course materials ahead of the assessment and exam. Although this may be in part due to the chosen metric, as course staff do not tend to initiate forum discussions. In contrast, material-initiated engagement was high for both courses, representing the majority of the overall engagement. In regard Q2: The Pearson correlation coefficient (ρ) between student engagement and course mark was 0.389 for Accounting and 0.228 for Marketing, suggesting that there is a weak relationship. For Accounting, the simple linear regression coefficient was positive and significantly different from zero, although the magnitude was small (coef = 0.5041, p = 0.00116, R2 = 0.151). For Marketing the coefficient was not significantly different from zero (coef = 0.07680, p = 0.0635). These correlation and regression results suggest that there may have been a small, positive relationship between engagement and performance at the level of individual students. The most striking observation, seen for both the Accounting and Marketing courses, is the drop off in engagement in the second half of the term as compared with the first half.
- **Notes:**

#### **Effects of In-Video Quizzes on MOOC Lecture Viewing** - [10.1145/2876034.2876041](http://dx.doi.org/10.1145/2876034.2876041)
- **Source:** Proceedings of the Third (2016) ACM Conference on Learning @ Scale - L@S '16
- **Problem:** To understanding how users interact with in-video quizzes, and how in-video quizzes influence users’ lecture viewing behavior.
- **Objective:** We aim to learn more about how learners engage with in-video quizzes, and how video interaction patterns differ in the portions surrounding in-video quizzes.
- **Methodology:** The Coursera logs we used for our analysis are labelled with an action (such as play, pause, or seek) associated with a point in the video, and a timestamp. We were able to reconstruct the seek source positions based on the previous event. Because we are primarily interested in where the user ends up seeking to, rather than the individual seek operations that got them to that point, if there are seek events that occur within 5 seconds of each other, we group them together into a single unit which we will call a seek chain. Using this approach, we reduced the 6.442.590 total seek events in our dataset into 2.103.336 seek chains.
- **Dataset:** Our dataset contains data from 96,195 users, of whom 61,453 viewed at least one video. There are 113 lecture videos in the course, totaling 19.5 hours of video content, with an average length of 10 minutes per video. With 109 in-video quizzes over 19.5 hours of video, this averages out to one in-video quiz for every 11 minutes of video.
- **Metrics:**
- **Case study:** The course we are analyzing in this work is the fourth iteration of the Machine Learning course on Coursera (ML4), which operated from October 2013 to January 2014. The number of users who answer the in-video quiz is higher than the number who finish watching the video – if we look at the 92 videos that have one in-video quiz, 74% of users who begin watching a video submit an answer to its in-video quiz, whereas only 66% watch it to completion. Certificate-earners also engage heavily with in-video quizzes. If we look at the 92 videos that have one in-video quiz, 79% of certificate- earning learners who begin watching a video submit an answer to its in-video quiz, whereas only 72% watch it to completion. That said, even certificate earners skip a few videos and in-video quizzes;
- **Results:** Of the 113 videos, 92 videos (81%) have 1 in-video quiz, 14 videos (12%) have no in-video quizzes, 6 videos (5%) have 2 in-video quizzes, and 1 video (1%) has 3 in-video quizzes; Certificate-earners also engage heavily with in-video quizzes. If we look at the 92 videos that have one in-video quiz, 79% of certificate-earning learners who begin watching a video submit an answer to its in-video quiz, whereas only 72% watch it to completion. That said, even certificate earners skip a few videos and in-video quizzes; Users are 4 times more likely to seek to the second of video where an in-video quiz is located, than to an arbitrary second of video; There are a large number of backward seeks originating from in-video quizzes. These reflect users who, having seen the in-video quiz, are reviewing the preceding section to find information to help them answer the question; The 10 seconds immediately preceding the in-video quiz are a popular destination of seeks, both in the forward and backward directions; The portion of the video surrounding the in-video quiz tends to receive more views. We also observe a trend of fewer views for portions of the video that occur later, which can be explained by in-video dropout. We believe the increase in the number of views surrounding the in-video quiz is due to users rewatching the portion preceding the in-ideo quiz, perhaps searching for material that will help them answer the quiz. Users rewatch the regions surrounding in-video quizzes, tend to backseek to review the materials preceding the quiz, and do not forward-seek over quizzes; We compare engagement metrics in videos with in-video quizzes to videos without in-video quizzes. On average, videos with in-video quizzes have more seek chains per viewing session. However, the increased seeking may also be attributable to video length – lectures with in-video quizzes tend to be slightly longer; 76.0% of users will answer the in-video quiz correctly on their first try; There are several possibilities for when backwards seek chains could occur in the context of in-video quizzes. We sought to see which of these reviewing strategies was the most popular. The former strategy – of seeing the in-video quiz, and seeking back to review before attempting to answer the quiz – is the most widely used. Of all backwards-seeking that occurs starting from an in-video quiz, a total of 60.5% actually occurs before the user attempts to answer the in-video quiz. 14.4% of backwards seeking from the in-video quiz occurs after the user submitted an incorrect response. 3.0% of backwards seeking from the in-video quiz occurs after the user submitted a correct response.
- **Notes:** Our results suggest futurework investigating the effects of in-video quizzes on in-video engagement rates.

#### **Data2U: scalable real time student feedback in active learning environments** - [10.1145/2883851.2883911](http://dx.doi.org/10.1145/2883851.2883911)
- **Source:** Proceedings of the Sixth International Conference on Learning Analytics & Knowledge - LAK '16
- **Problem:** One of the most common problems tackled with LMS data is the detection of students at risk of abandoning a course, the institution, or underperforming.
- **Objective:** This paper presents a naturalistic case study to explore how students use a dashboard displaying information about their engagement with preparatory activities in a first year engineering course used a flipped learning strategy. This paper is positioned at the intersection of these three spaces: the increasing presence of active learning methodologies, the increasing requirements for feedback in these contexts, an the use of learning analytics to report pedagogically meaningful indicators to students in real time.
- **Methodology:** The events were captured and used to produce a set of low-level indicators, their values were aggregated and made available through the dashboard provided as an additional resource in the online platform. The following data sources were used in the study: - Interaction with video clips embedded in the course notes; - Answers to formative assessment questions next to the video clips and embedded in the course notes; - Answers to formative assessment questions next to the video clips and embedded in the course notes; - Answers to a summative assessment sequence of exercises; - Access to the any course resource; - Access to the dashboard; For the purpose of the study the events were divided into two groups: Dashboard Events and Regular Events. Dashboard events are recorded when a student accesses the dashboard. Regular events include the rest of events occurring in the environment. If the separation between two events from the same user was larger than 30 minutes, the first event was marked as the end of the previous session, and the second event marked as the start of the next session. Four data analysis were carried out. The first one used a K-means clustering algorithm to explore if the students could be clustered based on the number dashboard accesses during the course. The second calculation also used K-means but based on when students accessed the dashboard during a study session. The third analysis explored how the pattern of engagement with the dashboard evolved throughout the semester. The clustering results obtained in the previous analysis were then used to label student interactions and obtain the changes in the dashboard use. The last study explored the relationship between dashboard use and the scores obtained in the midterm examination.
- **Dataset:**
- **Metrics:**
- **Case study:** The case study collected data about student interactions with the online resources available in a first-year engineering course deployed in a flipped and blended learning context at an Australian higher education institution. The course had an enrollment of 290 students. A total of 224 students used the dashboard at some point during the course (N = 224).
- **Results:** The first study aims at characterizing students based on the number of times they access the dashboard. The elbow method showed an optimum of 4 clusters with a quotient between the Sum of Square in Between (SSB) and the Total Sum of Squares (SST) of SSB/SST = 92.5% denoting a clear differentiation among the clusters. The elbow method showed an optimum of 4 clusters with a quotient between the Sum of Square in Between (SSB) and the Total Sum of Squares (SST) of SSB/SST = 92.5% denoting a clear differentiation among the clusters. The clusters have been labeled Low Engagement (LE), Medium Engagement (ME), High Engagement (HE) and Extreme Engagement (EE). The LE cluster contains most of the cohort and they viewed the dashboard once every 10 study sessions. The ME cluster contains students who viewed the dashboard every 2 sessions. Almost 10% of the students access the dashboard once per study session and there is a small percentage of students checking the dashboard more than once per session. The second study aimed at gaining a deeper understanding of how students interacted with the dashboard. For each student we selected the sessions that contained at least one dashboard view and then calculated the number of regular events before and after the first dashboard access. The K-Means algorithm was also used for this analysis and, as in the previous case, an optimum number of clusters of 4 was found using the elbow method. In this case we obtained the quality metric SSB/SST = 84.7%, lower than in the previous case, but still denoting a robust differentiation among clusters. Based on the number of events before and after a dashboard view, the clusters were labeled as: (1) Dashboard View in the middle of a study session (DBM); (2) Dashboard View in the beginning of a study session (DBB); (3) Dashboard View in the middle of a long study session (DBML); (4) Dashboard View near the end of a study session (DBE); The DBM cluster is the predominant type of interaction in almost every week of the course. This means that most of the students systematically used the dashboard as a reference in the middle of their work sessions. The evolution of the DBB cluster shows an interesting pattern. After a large number of students being in this category in the first and second weeks, the size of the cluster has a very significant reduction. The evolution of the number of students in the DBE cluster is almost the opposite. The first two weeks the cluster has a reduced number of students, but then it grows until it becomes the second most populated cluster. If we consider both the DBM and DBML clusters together we can conclude that as the semester advances more students use the dashboard in the middle of their study sessions. The final study explored the relation between the use of the dashboard and the scores obtained in the midterm examination. A model was derived using linear regression between the scores and the average dashboard views (ADVS). The linear coefficients had no statistical significance and R2 = 0.002 suggesting that the number of dashboard views are not associated with the midterm score.
- **Notes:**

#### **Predicting Students' Performance: Incremental Interaction Classifiers** - [10.1145/2876034.2893418](http://dx.doi.org/10.1145/2876034.2893418)
- **Source:** Proceedings of the Third (2016) ACM Conference on Learning @ Scale - L@S '16
- **Problem:** Inasmuch as the course year moves forward, the students’ participation in the LMSs tends towards steep drop-offs and highly unequal patterns of participation.
- **Objective:** In this work we study if it is possible to obtain more accurate classification models through the analysis of the students’ interaction in an incremental way.
- **Methodology:** We use WEKA in order to analyze the datasheets using three different algorithms; JRIP, J48 and Bayesian Network. We chose these algorithms because the classification models that they provide are built in a different way; classification rules, decision trees and neural networks, respectively.
- **Dataset:**
- **Metrics:**
- **Case study:** We have used Moodle to create a training program which is divided in 11 blocks. Each week a new block is unlocked for 15 days and students must carry out three basic tasks to complete each part: read a lesson, complete a quiz and add a post in the forum. We gathered the students’ interaction in two different years; 2012 (N= 111 students) and 2013 (N= 84 students). We discretized students’ final grade as “FAIL” and “PASS”.
- **Results:** It is possible to obtain better classification models using an incremental interaction.
- **Notes:** This case study is based on a course already structured in different blocks, and so it will be needed to test the accuracy of incremental models in different cases. For instance in those courses in which the interaction is not only limited by time intervals, being this way the students able to access, more or less freely, to the different resources.

#### **Profiling MOOC Course Returners: How Does Student Behavior Change Between Two Course Enrollments?** - [10.1145/2876034.2893431](http://dx.doi.org/10.1145/2876034.2893431)
- **Source:** Proceedings of the Third (2016) ACM Conference on Learning @ Scale - L@S '16
- **Problem:** What are common behavioral profiles of students who enroll MOOCs multiple times? How do students change their behavior between subsequent offerings of the same course?
- **Objective:** In this study, we examined how student behavior changes between subsequent course offerings.
- **Methodology:** In our analysis, we examined only data about students’ first and second enrollment. We did not analyze students who enrolled only once, and we also excluded any subsequent enrollments. We conducted a cluster analysis using the variables listed on metrics. We performed K-means clustering using Lloyd’s algorithm for values of K between 2 and 10 and the evaluated the percentage of variance explained by the different clustering solutions. We selected the K-means algorithm as the size of our dataset was too large for analysis using some of the more sophisticated classification techniques that involve pairwise distance matrix.
- **Dataset:** In total, we had 26,025 double course enrollment records (52,050 course enrollment records).
- **Metrics:** - Days: No. of days active; - Sub: No. of submitted assignments; - Wiki: No. of wiki page views; - Disc.: No. of discussion views; - Posts: No. of discussion messages written; - Quiz.: No. of quizzes attempted; - Quiz. Uni.: No. of different quizzes attempted; - Vid. Uni.: No. of different videos watched; - Vid.: No. of videos watched;
- **Case study:** The data for this study comes from the 28 offerings of the 11 different MOOCs offered by the University of Edinburgh on the Coursera platform.
- **Results:** We identified five clusters of the behavior of returning students. The  identified clusters reveal that the largest part (85% of all enrolled students) have no or have very little course activities. Around 10% of the students focused primarily on viewing video lectures, while 4.1% of students were highly engaged and, besides watching videos, also utilized quizzes and engaged in homework assignments. Finally, less than 1% of student put an emphasis on online discussions, while being less engaged with video lectures. This cluster of students also stayed longest active in courses. The majority of students from all the clusters except the “Social” cluster either just enrolled in a course or had very low level of engagement. A certain number of students who utilized both video lectures and quizzes during their first enrollment either retained the same level of engagement or focused primarily on video lectures in the second course enrollment. The most interesting finding is related to the students from the “Social” cluster who had the highest level of participation in online discussions and also most days spent in the course. While a certain number of students became disengaged in the next offer of the course, a large chunk of them (28%) kept their level of participation, signaling the goal of engaging with other learners rather than the prescribed course content.
- **Notes:**

#### **Modeling and Predicting Learning Behavior in MOOCs** - [10.1145/2835776.2835842](http://dx.doi.org/10.1145/2835776.2835842)
- **Source:** Proceedings of the Ninth ACM International Conference on Web Search and Data Mining - WSDM '16
- **Problem:** The low completion rates of MOOC participants preliminary statistics show that less than 5% of the participants have completed a course—has been a central criticism. Students have very different learning behavioral patterns, and these online learning behavior may also deviate from those in traditional learning environments.
- **Objective:** We focus on studying how students engage in MOOCs and to what extent we can predict their learning behavior.
- **Methodology:** We first conduct a regression analysis to examine correlation between student demographics and course selection. Regarding demographics, we consider gender, education (graduate degree including master and PhD, bachelor, and those with degree below bachelor), and age. We first study users’ participation pattern of forum activities including posting new threads and replying to questions. Second, we study their learning behavior forwatching videos and doing assignments. We design an algorithm based on deterministic finite automaton to approximate effective learning time. Specifically, we define three states: idle, video, and assignment, in the state automaton. We propose a latent dynamic factor graph (LadFG) model to address the problem. The model incorporates observed learning activities, including forum activities, watching videos, and doing assignments, into a unified framework. We use a latent learning state to model students’ learning state. We compare our model with several alternative predictive models: Logistic Regression (LRC); SVM; Factorization Machines.
- **Dataset:** In total, there are 56.800.000 time-stamped activity logs.
- **Metrics:** We particularly consider two prediction tasks: assignment grade prediction and certificate earner prediction.
- **Case study:** Employing xuetangX, one of the largest MOOC platform from China. The dataset used in this paper consists of 11 completed courses in the Fall 2013 and Spring 2014 semesters. We categorize the courses into two types: science (Computer Science and Electronic Engineering) vs. non-science (Economics, History, and Sports) courses.
- **Results:** We observe that compared to male students, females are significantly more likely to take non-science courses and less likely to choose science courses. Moreover, compared to users with low education (<bachelor), bachelors are significantly more likely to take non-science courses and significantly less likely to choose science courses. In contrast, graduate students are significantly more likely to take science courses. 94% users in our sample never participate in posting or replying to questions. Additionally, among active users, their forum activities decrease with time for all but new post in science courses suggesting that users’ participation enthusiasm in courses decays over time. Within non-science courses women post significantly more questions than men though they  reply marginally significantly fewer questions. In contrast, their amount of new post in science courses is significantly less than male students. We find that bachelors post significantly more questions in non-science courses. In contrast, graduate students do not ask many questions while their amount of replies in science courses is marginally significantly higher than those with degrees below bachelor. Within non-science courses, users post more questions when a course requires more working hours. However, the amount of replies is significantly negatively correlated with course effort requirements. The median time for watching videos is 4.53 minutes per course and 0 for working on assignments. In fact, 36% users never watch videos, and 52% of them never do assignments, suggesting that doing assignments requires more effort from users. Compared to male students, female students spend significantly more time on both videos and assignments in non-science courses, while they spend significantly less time on both activities in science courses. For both science and non-science courses, bachelors work hardest among all education groups, and the effect size is significantly stronger for science courses than non-science course. Graduate students spend least time on study in non-science courses. When the course requires higher effort, students spend significantly more time on assignments and surprisingly, they spend significantly less time on videos. Overall, among 11 courses in our data, the certification rate lies between 0.84% and 14.95%. The average certification rate for science courses is lower than that in non-science courses, though the difference is not statistically significant. We find that compared to male students, females are significantly more likely to get the certificate in non-science courses, however,  the size of the gender difference decreases significantly after we control for forum activities and effective learning time. Compared to students with degrees below bachelors, bachelors are significantly more likely to get the certificate in non-science courses, while their high effort on science courses does not transform to significantly higher certificate rate. Graduate students are also significantly more likely to get the certificate in non-science courses. Both forum activities are good predictors for getting certificates. In non-science courses, the size of the effect between posting and replying questions is about the same. In science courses, posting questions matters more than answering them suggesting that students who ask questions are more likely to get certificates than those who answer questions. In both two types of courses, the amount of time spent on videos and assignments are significantly positively correlated with certificate rates, 6 although courses, the amount of time spent on videos and assignments are significantly positively correlated with certificate rates. In terms of F1-score, the proposed LadFG model clearly outperforms all alternative methods.
- **Notes:** Understanding the complex and subtle forces underlying the learning process can significantly help design better courses and improve the learning effectiveness.

#### **Explaining Student Behavior at Scale: The Influence of Video Complexity on Student Dwelling Time** - [10.1145/2876034.2876051](http://dx.doi.org/10.1145/2876034.2876051)
- **Source:** Proceedings of the Third (2016) ACM Conference on Learning @ Scale - L@S '16
- **Problem:** The analysis of student behavior through clicks tends to be too granular for sense-making; that is, to be able to unambiguously assign meaning to clicks and subsequently explain student behavior
- **Objective:** We aim to make two contributions. First, we formalize both information complexity and student dwelling time within the context of videos by proposing a mathematical definition for both. Then, we explore whether, and if so how dwelling time is explained by information complexity.
- **Methodology:** To define a plausible and robust measure of complexity, we adopt a set of features based on well-known psycholinguistic findings on the causes of reading difficulty and are designed to be robust against overfitting. Lexical familiarity indicates how familiar a reader is with a word. It influences a reader’s fixations, such that more frequent words take less initial processing time and high-frequency words are more likely to be skipped than less frequent words. This can generally be approximated with two metrics. Based on the occurrence of a word on either the Dale list of 3000 common words or on a large representative collection of writing. For this study, the Google Books N-Gram corpus will be used for the term count function cnt. Two ‘languages’ from Wikipedia were used to train a model of textual complexity: Simple English and regular English. These two languages are intended to be distinctive in their level of complexity as authors are instructed to use easy words and shorter sentences, but not to include less information. The following pre-processing steps were performed on the Wikipedia data set. The data consisted of two dumps from August 3, 2011, containing all articles encoded as wiki-text for both languages. The original transcripts, originally uploaded by the administrators of the respective courses, were extracted for each video. We define a paragraph within a video based on the locations of the in-video questions. The complexity analysis was performed on the paragraphs, in order to make any local difficulties more apparent in the analyses and reduce the influence of confounding effects that can occur over a longer time period. Click-stream data from students interacting with the selected courses was examined as proxy of student behavior. Click-stream actions were attributed to video paragraphs. The total amount of watching episodes before further filtering was 1.556.256. Users not registered as students were excluded from the data. We filtered for only those students who finished watching a video paragraph. A Logistic Regression Model (LRM) was trained on the paragraphs from each of the pre-selected 10,000 articles per Wikipedia language.
- **Dataset:** Of the five MOOCs, the data of a total of 104 videos were analyzed. Click-stream actions were attributed to video paragraphs. The total amount of watching episodes before further filtering was 1.556.256. After filtering the resulting data set contained 471,179 unique user sessions with any of the videos.
- **Metrics:** Dwelling rate: is the time that users spend on a piece of content. We define dwelling time as the total time that a student spends on watching a video relative to the nominal length of that video.
- **Case study:** The data used in our study comes from five MOOCs. These courses were organized by Leiden University between 2014 and 2015. The MOOC videos differ substantially in the topics being discussed, such as environmental issues, political affairs, linguistics, and tax law. The most common production style used throughout the videos is the ’talking head’ setup. In certain videos this was supplemented or mixed with additional graphics or text.
- **Results:** The features give similar results on both data sets, indicating the possibility of applying the Wikipedia data set features to analyze Coursera transcripts. All predictors are significant at a p<.001 probability. The problem of distinguishing between paragraphs seems substantially more challenging with an accuracy of 68.03%. The histograms show how the predicted levels of complexity are distributed for each data set, differentiating between Simple English and regular English for Wikipedia and showing a normal distribution for the Coursera data set. The normal distributions suggest that the model can be applied to Coursera transcripts in the same way as it can be for the Wikipedia paragraphs. We compare information rate with how long and how much students watch a video paragraph for those students who finished watching a paragraph. An increased student dwelling at the lower as well as at the higher ranges of information rate. The relationship between information rate and dwelling rate follows a polynomial pattern, such that high dwelling times are typical for videos having either low and high rates of information. A similar trend was found for the relation between information rate and dwelling time. The value of both contributions is confirmed by two regression models. For information rate, a LRM gave 68.03% accuracy on distinguishing Simple English from regular English in our ground truth. For dwelling rate, the polynomial regression model gave an explained variance of 22.44% by information rate. The computational model of information complexity for videos was successful in explaining meaningful student behavior. This shows the validity of applying the textual model to spoken words, in line with earlier findings showing that language is (partly) processed in similar ways irrespective of the presentation modality. Student dwelling increases with a high information rate. This is also as expected: a higher information rate can make it difficult for students to understand the content, making it necessary to re-watch parts of the video.
- **Notes:** The information rate of a MOOC video is a substantial predictor of dwelling time and rate.

#### **Towards triggering higher-order thinking behaviors in MOOCs** - [10.1145/2883851.2883964](http://dx.doi.org/10.1145/2883851.2883964)
- **Source:** Proceedings of the Sixth International Conference on Learning Analytics & Knowledge - LAK '16
- **Problem:** Discussions in MOOCs (Massive Open Online Courses) are large-scaled and asynchronous in nature, and therefore more difficult to control. We begin by investigating the following two research questions. 1) What kinds of discussion behaviors are associated with more learning? 2) What kinds of learning materials appear to trigger more of these discussion behaviors?
- **Objective:** In order to design interventions to increase the quality of discussion in MOOCs.
- **Methodology:** We adopted two approaches. First, we used propensity score matching to pair students who have a similar level of involvement in other activities in the course, and compared learning between the matched control and treatment groups. Second, we performed a week-level within-subject analysis. We investigated whether individual learners had higher course performance during weeks in which they displayed more higher-order thinking behaviors in the discussion forum. We then investigated what is different in the course materials across weeks that lead to different levels of cognitive engagement, and what topics may trigger cognitively richer discussion among students. To answer this question, we used LDA (Latent Dirichlet Allocation) in order to identify which topics are associated with the occurrence of higher-order thinking behaviors. One contribution of our work is a coding manual to capture students’ discussion behaviors. We categorize a post as interactive if the post is constructive and the student is referring to someone else’s idea expressed earlier in the conversation. A decision tree was used in the coding manual. In the first stage of the coding process, the coder is asked to decide whether a post is on-task or off-task. In the second stage, for on-task discourse, the coder is then asked to decide to which specific categories of cognitive engagement each post belongs. There are six mutually exclusive categories, O, A2, A1, C2, C1, I in our tree decision. We grouped A2, A1, and C2, C1 into A and C respectively. The purpose of our first analysis is to measure an effect of higher-order thinking behaviors over paying general or focused attention to course materials. To this end, we created three mutually exclusive groups of students based on the highest level of cognitively relevant discussion behaviors they displayed in the forum. - Group2[higher-order]: A binary variable, which equals 1 if the student has contributed at least one constructive or interactive post during the course, otherwise it equals 0; - Group1[paying-attention]: A binary variable, which equals 1 if the student has contributed at least one active post during the course but has not displayed any constructive or interactive posts, and otherwise equals 0; - Group0[off-topic]: The group associated with this binary variable contains the rest of the students, i.e., students who have not contributed any on-topic discussion during the course; By including only the students who have both pretest and posttest on record, and have participated in the discussion forum, we arrive at a final sample size of 491. In metrics, we explain our dependent and control variables.
- **Dataset:** Altogether, the dataset contains data from 27,750 registered users, and a total of 7,990 posts and comments. Of these, 1079 learners have both pretest and posttest on record, 491 of whom participated in the discussion forum, generating 3864 posts and comments in total. The course record contains 1487665 student clicks.
- **Metrics:** Engagement behaviors can be differentiated into one of four modes: (1) Passive: Passive mode of engagement is defined as learners being oriented toward and receiving information from the instructional materials without overtly doing anything else related to learning; (2) Active: Learners’ engagement with instructional materials can be operationalized as active if some form of overt motoric action or physical manipulation is undertaken; (3) Constructive: Constructive behaviors are defined as those in which learners generate or produce additional externalized outputs or products beyond what was provided in the learning materials; (4) Interactive: Interactive behaviors should meet two criteria (a) both partners’ utterances must be primarily constructive, and (b) a sufficient degree of turn taking must occur; Dependent variable: - Post-test: The student’s standardized post-test score; Control variables: - Pre-test: The student’s standardized pre-test score; - Numpost: The total number of posts the student has contributed throughout the course; - OLI-registration: A binary variable indicating whether the student has registered for OLI (Open Learning Initiative), which offered them supplementary learning-by-doing activities in each unit; - Video: The student’s number of videos clicked on at least once; - Quiz: The student’s number of quizzes clicked on at least once;
- OLIsite: The student’s number of clicks on the OLI website; - Forum: The student’s number of clicks on the forum;
- **Case study:** The dataset we used in this analysis is from the course “Introduction to Psychology as a Science” offered through Coursera collaboratively by Georgia Institute of Technology and Carnegie Mellon University. The course incorporated elements from the OLI (Open Learning Initiative) “Introduction to Psychology” learning environment. One special characteristic of the course was that it administered a pre/post test with the intention to support research. Course materials included video lectures, assigned MOOC activities, learning-by- doing activities in the OLI environment, and weekly high-stakes quizzes.
- **Results:** We did a regression analysis and observed that students who display higher-order thinking behaviors have more learning gains than those who did not display any higher-order thinking behaviors but show that they are paying active attention to course materials from their posts in the forums; And the students whose discussion behaviors indicate they are paying active attention to course materials also have higher learning gains than students who are constantly being off-topic in the forums. The result shows that controlling for the number of other activities the student engaged in during the course, including watching videos, doing quizzes, visiting the forum, and vising the OLI website, for students who contributed the same number of posts, those who displayed higher-order thinking behaviors had higher learning gains than students who did not display higher-order thinking behaviors (p-value= 0.001). Similarly, students whose discussion behaviors indicated paying active attention to course materials had more learning gains than student who did not show active attention to course materials (p-value=0.031). We further contrasted between the higher-order thinking group and the paying attention group by computing the relative effect size associated with these binary variables. We found that the higher-order thinking group had an effect size of 0.36 in comparison with the off-topic group, while the paying-attention group had an effect size of 0.26. In both cases, we computed effect size using the Cohen’s d method. Due to this difference in their relative effect sizes, we consider that displaying higher-order thinking behaviors in discussion forums is associated with more learning gains than displaying discussion behaviors that show general or focused attention to course materials, which in turn is associated with more learning gains than posting in the forum but being off-topic all the time. The result in the treatment group, which indicates the student has displayed higher-order thinking behaviors in the course, has a significant effect on learning. The average posttest score for the treatment group is 28.5, and the average posttest score for the control group is 27.3. One thing to notice is that although in this matching, there were no significant differences between groups on any of the matching variables, the trend was always in favor of the control group, giving them an advantage. Nevertheless we still find the treatment group learned more. The result here shows that for a pair of students who are categorized as similar leaners by their prior knowledge and engagement level in the course, the one who displayed higher-order thinking behaviors had higher learning gains than the one who did not. We see that content related to daily life and social experience is associated with higher-order thinking behaviors; We see that the contents that have more technical terms are associated with cognitively poorer discussion; We also found that a lot of discussions in the forums are off-topic;
- **Notes:**

#### **Longitudinal engagement, performance, and social connectivity: a MOOC case study using exponential random graph models** - [10.1145/2883851.2883934](http://dx.doi.org/10.1145/2883851.2883934)
- **Source:** Proceedings of the Sixth International Conference on Learning Analytics & Knowledge - LAK '16
- **Problem:** First, do engagement with learning content and performance on assessments provide additional information—beyond the endogenous mechanisms of a random network structure—about a learner’s connectedness to others in the social network? Second, do time-lagged effects, such as joining the discussion in response to struggling with the previous assignment, relate to performance and engagement counts from the previous week?
- **Objective:** This paper explores a longitudinal approach to combining engagement, performance and social connectivity data from a MOOC using the framework of exponential random graph models (ERGMs).
- **Methodology:** Sometimes a reply was really a comment to a specific prior reply, and sometimes a comment was really an open reply to the Original Poster (OP). We chose not to distinguish these two actions after all. The choice arises as to whether a reply should connect its poster to everyone who posted previously or only to the OP. To construct our social networks, we connected only the OP and the subsequent repliers. As a result, each thread is represented by a star in the network, with the OP as the node in the middle and all others in the periphery. All networks show a relatively big connected component and several smaller components in the periphery. The visualizations show the network shrinking over time, which is confirmed by basic summary statistics. The basic idea of ERGMs is to build a stochastic model that captures the generative features of an observed social network. By analogy with regression models, dependent variables are the nodes and links in the observed network, and the independent variables are summary statistics for various network structural features.
- **Dataset:** BDE had over 48,000 enrollees during the duration of the course, with a small portion actively participating. A total of 1,380 students completed at least one assignment, and 638 students received a certificate. We restrict our attention to the users who posted or commented at least once in the forum. This yielded a sample of 770 individuals, including students, the instructor, and teaching assistants. Among this sample, 440 students (57%) completed at least one assignment, and 155 (20%) earned a certificate.
- **Metrics:** For this study, we are interested in the relationship between MOOC forum social links and course participants’ behavior and performance in current and adjacent weeks. Each individual has four attributes extracted from each week’s data, including assignment score (score, the only performance attribute), the number of posts initiated/commented/replied (posts), the number of posts viewed (post views), and the number of video lectures downloaded or played (lectures). Counts of posts, post views, and lectures constitute our simplified engagement measures.
- **Case study:** The dataset used in this study was collected from a Coursera course “Big Data in Education” (BDE) offered by Teachers College, Columbia University in Fall 2013. The course spanned eight weekly sessions, each consisting of five to seven lecture videos, readings, and a quiz, which requires the students to do data analysis and submit the results. All weekly assignments were automatically graded and accounted for 100% of the final course grade (70% was the threshold for earning a certificate). Students were encouraged to use the online forums throughout the duration of the course, and this was the principal way in which instructors and TAs interacted with learners.
- **Results:** On Week 2: Model 1 serves as the baseline model, which contains two structural features of the forum network, edge and alternating k-stars. The negative value for edge (-4.09) indicates that the density of the network is lower than would occur by chance. A negative value for alternating k-stars (-0.99) indicates that hub frequencies are also lower than would occur by chance. The phenomenon in which higher degree nodes attract more links is known as preferential attachment; For Model 2, we add four nodal attributes, two related to performance and two related to engagement. The results are a significant positive effect for assignment scores on connectedness in the forum network and a significant negative effect for lecture views/downloads. The score difference and post view effects were not significant; In Model 3, we added attributes from the previous week. It turned out that individuals with higher assignment scores from the previous week tended to have more links in the social network in the current week.  Individuals who viewed more posts from the previous week also tended to have more links in the current week, while individuals who posted more in the previous week tended to have fewer links in the current week’s social network (controlling for other effects). The lecture views/ downloads from the previous week did not have a significant effect on the current week’s relations; In Model 4 (the full model with all variables), we explore whether forum social relations correlate with future learning behavior. The effects for both assignment scores and the lecture views/downloads were positive and significant, indicating that individuals with more social links in the current week tend to have higher assignment scores and more lecture views/downloads in the next week; Beyonde Week 2: We ran the full set of models on data from week 2 through week 7. The structural feature effects from the baseline Model 1 are relatively stable and consistent over all weeks. However, in more complex models, the effects were not consistent with the findings in the Week 2 network. Similar observations concerning fading or inconsistent effects held true in Models 3 and 4 for network statistics with individual attributes from the previous week and subsequent week. The effect of previous week score, significantly positive in Week 2, was not significant in most other weeks except for Weeks 6 and 7, and in the latter case, the sign had changed. Subsequent week score was consistently a positive effect for Weeks 2 and 3, though nonsignificant afterwards.
- **Notes:** This study has several limitations, which suggest directions for future work. As discussed in the introduction, the meaning of social connectedness here remains fairly simple. Links are unweighted and reflect only the process of replying (or commenting) to an original poster. Content analysis, preferably automated for scalability, could significantly improve the definition of the network itself. Steps toward including natural language processing of posts in defining the network are part of ongoing work.

#### **On the Impact of Personality in Massive Open Online Learning** - [10.1145/2930238.2930240](http://dx.doi.org/10.1145/2930238.2930240)
- **Source:** Proceedings of the 2016 Conference on User Modeling Adaptation and Personalization - UMAP '16
- **Problem:** Keeping MOOC learners engaged with the course and platform are of major concerns to instructional designers and MOOC instructors alike. Does personality impact learner engagement, learner behaviour and learner success in the context of MOOCs? Can learners’ personalities be predicted based on their behaviour exhibited on a MOOC platform?
- **Objective:** To investigate the impact of personality on learning in MOOCs.
- **Methodology:** We included a fifty item Big Five personality questionnaire in the first week of the course as an optional component. A total of 2.195 (9.3%) registered learners began the process of filling in the personality questionnaire. 1.356 learners eventually completed this process (5.7% of registered learners). The fifty items are short descriptive statements and are answered on a Likert scale. Based on the provided answers, for each of the five personality traits (openness, extraversion, conscientiousness, agreeableness, neuroticism) a score between 0 and 40 is computed which indicates to what extent the learner possesses that trait. Supported by another papers, learners who score high on extraversion tend to have a strong need for gratification. In the MOOC setting, such gratification can be fulfilled through interactions with other learners. We also expect forum-based features to be useful to predict high levels of agreeableness (people who tend to help others). We hypothesize that learners who are very conscientious will be more disciplined in terms of video watching and quiz question answering than learners who score low in this trait, inspiring us to explore video & quiz related features. The openness trait embodies academic curiosity and we hypothesize it to correlate positively with the amount of time spent on the platform and the material. We extracted the following features: Time watching video material; Time solving quizzes; Questions learners attempted to solve; Questions learners attempted to solve (the total number of quiz questions a learner answered); New forum questions; Forum replies; Total forum postings; Forum browsing time; Forum accesses; Forum interactions; Total time on-site; Average video response time; Average quiz response time; Videos skipped; Videos sped up; Maximum session time; Average/standard deviation session time; Final score; Then a correlation analysis between these features and the personality traits derived from the learners’ personality questionnaires was performed in order to answer the first research question. To work in the second research question, we experiment with two state-of-the-art regression models based on Gaussian Processes (GP) and Random Forests (RF), respectively, which have been shown to perform well in previous personality prediction works. Due to the limited number of learners, we resort to 10-fold cross-validation. In order to evaluate the accuracy of our personality trait predictions, we resort to Spearman’s rank correlation coefficient with the two variables being the learners’ ground truth personality trait score and the predicted trait score.
- **Dataset:** Overall, 23,622 users registered for the course. Less than half of the registered learners (40%) engaged with the course, watching at least one lecture video. A total of 9.523.840 log traces were recorded of timestamped, including clicks, views, quiz attempts, and forum interactions.
- **Metrics:** The five traits can be summarized as follows: (1) The openness trait is displayed by a strong intellectual curiosity and a preference for variety and novelty. (2) The extraversion trait refers to a high degree of sociability and assertiveness. (3) Conscientiousness is exhibited through being organized, disciplined and achievement-oriented. (4) People who score high on agreeableness are helpful to others, cooperative and sympathetic. (5) The neuroticism trait indicates emotional stability, the level of anxiety and impulse control.
- **Case study:** We investigate on the data traces of 763 learners who participated in the EX101x Data Analysis MOOC running on the edX platform from August 31, 2015 to November 9, 2015. EX101x Data Analysis teaches various introductory data analysis skills in Excel and Python. The course was set up as an xMOOC. Lecture videos were published throughout the ten teaching weeks. To pass the course, ≥ 60% of the questions had to be answered correctly.
- **Results:** Overall, 12% of the engaged learners earned a certificate by answering 60% or more of the quiz questions correctly. Notably, on average, less than one hour of lecture material (of approximately 300 minutes of video lecture material) was consumed by the engaged learners. Less than 15% of engaged learners were active in the course forum; by the end of the course, a total of 4,419 posts (questions, replies and comments) had been created. While there are thousands of active learners, most learners are active only sporadically; only a small percentage of learners remain active throughout the entire MOOC. Due to the length of the personality questionnaire, we suspect some learners to more or less randomly provide answers instead of truly answering to the personality statements. This questionnaire should take between three and eight minutes. We consider the personality data of all those learners as valid that spent at least three minutes and at most twelve minutes on the questionnaire. After this filtering step, we are left with 1,082 valid personality questionnaire responses that we continue to analyse in the remainder of this section. Our learners score lowest on extraversion and highest on openness and agreeableness. The majority are male (64%) and between the ages of 20 and 40 (62%). More than 40% of our learners have completed a first university degree already. In order to conduct a meaningful correlation analysis, we partition our 1,082 learners into two sets: those learners with high and those with low prior knowledge based on their self-reported expertise in the pre-course survey. As all questionaires and surveys in this MOOC, the pre-course survey was voluntary and not all learners completed it. We are thus left with 763 learners who completed the personality questionnaire and stated their prior knowledge level. Across the two sets of learners we do not observe any statistically significant correlations between behavioural features and the traits of agreeableness and neuroticism. Only two personality traits are significantly correlated with a number of features: openness to experience and conscientiousness. Among the learners with low prior knowledge the amount of time spent watching video lectures and number of quiz  questions learners attempted are positively correlated with conscientiousness to a significant degree while a significant negative correlation is found for the number of videos skipped — i.e., learners with a high-self discipline and striving for achievement are likely to be more thoroughly engaged with more learning materials than learners who are not. The same features are inversely correlated with the openness to experience trait to a significant degree—i.e. learners that are more intellectually curious & prefer variety are less likely to spend time focused on the learning material than learners with lower openness scores. As a consequence they earn a lower grade. In the case of learners with high levels of prior knowledge we observe only four significant correlations between features and personality traits: three forum features (number of replies, number of forum posts and number of forum interactions) are positively correlated with conscientiousness. In contrast to our expectations, learners with high levels of extraversion are not positively correlated with forum behaviour, in contrast, the only other significant correlation (between the amount of time spent on the forum and the extraversion) trait is a negative one – learners with higher levels of extraversion spend less time on the forum than learners with lower levels of extraversion. We have to conclude that behavioural features extracted from MOOC log traces are correlated to a lesser degree with personality than lexical or behavioural features extracted from social networks such as Facebook and Twitter. We plot for each of the personality traits the effectiveness our two regression approaches achieve as measured by Spearman’s rank correlation coefficient. Gaussian Processes perform better in this setting than Random Forests yielding higher correlations in three of the four traits that result in significant results; The correlation coefficients tend to increase with increasing courseweeks as more activity data about each learner is gathered; Extraversion (ρ = 0.31) and neuroticism (ρ = 0.22) achieve the highest prediction accuracy by the end of the course.
- **Notes:** Not investigated so far has been the impact of personality on learning in MOOCs — is personality predictive of success and behaviour in the current massive open online learning environments? If we were to find this to be the case, it would open avenues for personalization and adaptation of learning in MOOCs based on learners’ personalities. In contrast to the classroom setting where learners form a relatively homogeneous group (in terms of age group, cultural exposure, prior knowledge, etc.), MOOC learners have very diverse backgrounds.

#### **Student differences in regulation strategies and their use of learning resources: implications for educational design** - [10.1145/2883851.2883890](http://dx.doi.org/10.1145/2883851.2883890)
- **Source:** Proceedings of the Sixth International Conference on Learning Analytics & Knowledge - LAK '16
- **Problem:** Understanding student behavior. Can we identify different clusters of students based on differences in their regulation strategies? Do these differences in regulation strategies reflect in differences in the use of (digital) learning resources? What combinations of (digital) learning resources contribute most to course performance for each cluster? Do differences in regulation strategies reflect in differences in course performance?
- **Objective:** To provide insight into student behavior by focusing on individual differences in approaches to learning and subsequently how these differences affect the use of (digital) learning resources.
- **Methodology:** We used multiple log indicators to capture ways students used the learning resources. In most cases the frequency of use and the duration of use were logged. Student attendance was registered on an individual level by scanning student cards upon entry of the lecture hall registered for all 17 lectures of the course. The viewing of the lecture recordings was monitored on an individual level and could be traced back to date, time, amount and part of the lecture viewed. Two different types of LMS data were gathered. Except the previously mentioned digital resources, the recorded lectures and the formative assessments, the LMS also offered Powerpoint slides and additional reading materials (PDF) for download as well as some illustrative videos about certain topics. First the total amount of hits within the LMS was registered. These are hits as clicking on links to the recordings or formative assessments, clicking on announcements, checking grades, clicking on links to PDF files or links to certain video files. Second the total time spent in the LMS during the course was registered. This is the total time in minutes a student was logged on to the course in the LMS during the entire timeframe of the course. This measure was calculated by accumulating the time differences between logging on the course and subsequently logging of or logging on to another course. The Inventory Learning Style (ILS) is a self-report diagnostic instrument intended to measure aspects of study method, study motives and mental models about studying in higher education. The ILS consists of 120 items and contains four domains. For the purpose of the current study only the sub-scales of the domain regulation strategies were scored. These sub-scales are: self-regulation external regulation and lack of regulation. We performed a two-step cluster analysis on ILS regulation strategy data. A two-step cluster analysis determines the natural and meaningful differences, formed in clusters, which appear within the current population. The two-step method is preferred over other forms of cluster analysis when both continuous and categorical variables are used and when the amount of clusters is not pre-determined. Next a MANOVA between the different clusters was conducted to determine significant differences in the use of (digital) learning recourses. The MANOVA was used to determine if certain clusters, based on regulation of the learning process, made a significant amount more use of certain learning resources than others. Third a stepwise multivariate analysis was conducted for each cluster to determine the relative contribution of each of the different learning resources on course performance. This was done to determine which (combination of) learning resources contribute to the final grade for each separate cluster and differences between clusters. Finally an ANOVA determined if there were any significant differences between the different clusters and course performance.
- **Dataset:**
- **Metrics:** In most cases the frequency of use and the duration of use were logged.
- **Case study:** The participants were 333 first year university Psychology students (243 female, 90 male, Mage = 20.17, SDage = 1.66) attending an obligatory course on Biological Psychology. Students who took the course as an elective or had taken the course before were removed from the dataset. The course consisted of 17 face to face lectures, with a 120-minute duration over a period of 8 weeks.
- **Results:** The auto-clustering algorithm indicated that three clusters was the best model, , because it minimized the Schwarz's Bayesian Criterion (BIC) value and the change in them between adjacent numbers of clusters. Students in cluster 1 show no dominant regulation pattern, indicating that these students have no clear pattern to regulate their learning. Students in cluster 2 use a combination of two regulation strategies: lack of regulation and external regulation. They seek guidance in the learning process from external sources but when this external regulation fails, for example by absence of the instructor or unclear learning objects, they tend to show a lack of regulation. Also cluster 3 shows a combination of two regulation strategies. They try to self-regulate their learning but when they fail they use an external source to in order to compensate for this. They are mainly able to self regulate the learning process, but when they fail to do, they use an external regulation strategy to compensate for this deficiency. Cluster 2 students, mainly characterized by external regulation, show a greater use of the different learning resources. However, the differences in the use of the learning resources between the three clusters are not significant for lecture attendance, for recorded lectures, for hits in Blackboard, for Blackboard duration and for the average score on the formative assessments. Although the cluster analysis revealed that there are distinct differences in the students their regulation strategies, these differences in regulation have no significant impact on the use of the different learning resources throughout the course. The overall models differ in their explained variance: 45,2% for cluster 1, 23,2% for cluster 2 and 50,4% for students in cluster 3 students. Students in cluster 1 and cluster 3 benefit the most from formative assessments. Students in cluster 2 benefits mostly from face-to-face lectures. Cluster 1 students also benefit from recorded lectures, while cluster 3 students benefit more from attending lectures. There were no significant differences between groups for course performance as determined by the ANOVA (F(2,330) = 1.018, p = .363). This finding indicates that the structure of the course is beneficiary for students who report low self-regulated learning, but is a disadvantage for students who report high self-regulated learning. The students who report high self-regulated learning benefit more from the offered learning resources. This finding implies that not only duration of use has an impact on course performance but also the reported regulation strategy has an impact on the effectiveness of the learning resources.
- **Notes:** This finding has two implications for learning analytics. First the contextualization of learning data with a broader set of indicators is crucial in establishing the impact of the learning data analysis since these conditions affect the learning process. The use of the same learning resources to the same extent have different impacts on different groups of students. Current learning analytics visualization trends use dashboards to mirror a student their activity with the class average. However, this class average is not as straightforward as previously assumed. Regulation strategies thereby do not account for the previously reported differences in the use of digital learning resources by students but does account for differences in effect of that use. This research confirms, once again, the low predictive value LMS use has on course performance. For the three clusters, frequency and duration of LMS use were not significant in contributing to course performance. We found no significant relation between the amount of logons to the LMS and external regulated students. Surprisingly no significant differences were found between the three different clusters and course performance.

#### **Generating actionable predictive models of academic performance** - [10.1145/2883851.2883870](http://dx.doi.org/10.1145/2883851.2883870)
- **Source:** Proceedings of the Sixth International Conference on Learning Analytics & Knowledge - LAK '16
- **Problem:** There has been minimal attention investigating how such information can be best deployed to promote reflection and action among teaching staff and students.
- **Objective:** This paper proposes how a recursive partitioning technique can offer instructors a predictive model to help them derive data-informed pedagogical interventions and personalized feedback to different student subpopulations.
- **Methodology:** The first type (VID) consisted of an interactive HTML page with a video clip introducing new course concepts. All play and pause events in the video clips were recorded. The second type of activities (VEQ) were included immediately next to the video clip and consisted of a formative assessment in the form of multiple-choice questions related to the concepts covered in the adjoining video clip. Students could answer each question, review whether the answer was correct, or request to see all the answers. This last option was offered only if an answer was provided. The events correct, incorrect and show were recorded denoting the three possible actions. The third type of activity (EQT) required students to read text in an HTML document with additional formative assessment in the form of multiple-choice questions (identical to the ones previously described). The fourth set of activities (EXC) required students to solve a sequence of exercises and provide their answer through a multiple-choice question. The three types of assessment (VEQ, EQT, and EXC) were treated as separated categories in the analysis. For each week of the course, a set of variables was extracted and used to calculate the predictive models. For example, the model for week 3 was calculated with the set of variables reflecting the number of events recorded during that week (ignoring the rest). The study aimed to show the feasibility of obtaining a predictive model of the performance of the students in the midterm and final examination using the data about their interaction with the online activities. The performance of the model was measured using the Mean Absolute Error (MAE) defined as the mean of the differences between the predicted and real scores, and the Root of the Mean Squared Error (RMSE) defined as the square root of the mean of the square differences between the predicted and real scores (MSE). The model validation was performed following a leave one out cross-validation strategy. Predictive models were calculated for all eleven weeks of the semester using recursive partitioning. For each model, a collection of rules and a tree structure was produced. The performance of the resulting model is assessed based on the accuracy of the score predictions in the leave nodes.
- **Dataset:**
- **Metrics:** - VID.PL/VID.PA: Number of play/pause events in the videos for the week; - VEQ.CO/VEQ.IN/VEQ.SH: Number of times a question next to a video clip was answered correctly, incorrectly or the answer was shown; - EQT.CO/EQT.IN/EQT.SH: Number of times a question in the course notes was answered correctly, incorrectly or the answer was shown; - EXC.CO/EXC.IN: Number of exercises answered
correctly/incorrectly.
- **Case study:** The case study described in this section collected data from a 13 week first year course at a large research intensive university in Australia. All course components were offered in electronic form and made available following a blended learning strategy mediated by the corporate Learning Management System. The data was collected in the 2014 offering of a large first year engineering course (n = 272). The weekly schedule included one 2-hour lecture, one 2-hour tutorial and a 3-hour laboratory session. The course design included the same pattern of activities for the core weeks 2-5 and 7-13.
- **Results:** The mean and standard deviation of the midterm and final scores were 13.3 (4.1) and 19.1 (8.8), respectively. A predictive tree was obtained through recursive partitioning, containing 13 nodes and 6 rules. Students in the subpopulation with the lowest predicted score have a high number of incorrect exercise submissions and a low number of play video events. An instructor may refer to this information when customizing the feedback inviting them to increase their engagement with the videos, which may help reduce the number of their incorrect exercise submissions. The MAE for all four models calculated is stable at approximately 15% of the midterm score (3 out of 20). The RMSE, on the other hand, offers a higher value (approximately 4) but still below 20% (4 out of 20). The score predicted for a subgroup may have an average of 15 or 20% error. The performance of the models improves when predicting the score for the final exam. In this case, since the score is in the range [0-40] the MAE is below 11.8% (4.726 over 40) whereas the RMSE is below 14.22% (5.688 over 40).
- **Notes:**

### Elsevier
#### **Temporal predication of dropouts in MOOCs: Reaching the low hanging fruit through stacking generalization** - [10.1016/j.chb.2015.12.007](http://dx.doi.org/10.1016/j.chb.2015.12.007)
- **Source:** Computers in Human Behavior
- **Problem:** The large number of students who enroll in (and drop out of) each course raise methodological difficulties for instructors as they work to identify academically at-risk students and provide in-time interventions. The overall research question: “How and to what extent can we build a prediction model that can accurately and reliably identify struggling students in MOOC forums in advance so that teachers can provide timely and quality pedagogical support to them?” The specific research questions: - How can we synthesize the features for temporal model construction to improve the prediction performance? - How can we employ stacking generalization to improve the temporal prediction performance?
- **Objective:** We propose a method for predicting the gradual falling away from participation in a course which focuses in large part on forum participants. The major research goals of this study are 1) to experiment and demonstrate a temporal modeling approach for students' dropout behavior; 2) to show the advantage of appended feature modeling space based on principle component analysis (PCA) over a summed features modeling space; and 3) to explore the power of the ensemble learning method (stacking generalization) in enhancing the prediction ability.
- **Methodology:** Data was obtained from two sources: First, we obtained click-stream data for the entire duration of the course directly from Canvas. Second, quiz scores and discussion forum data was obtained in Json format through the Canvas API. The social network degree is also calculated for each student as a reflection of social interaction. To construct this, each student is considered as a node and a comment from one student to another in the forum is regarded as an edge between these two students. The degree value is calculated as the number of edges the student has. For each week of the course, the dropout label for each student, active in the current week, is calculated based on examining whether there is any activity from the student in the immediate next week and beyond. This generates the label vectors, where 0 indicates dropout and 1 indicates active. In this paper, we experiment with two approaches. (1) In the first, we follow an intuitive approach, adding the values of the current features with the historical features together to generate new features for the prediction modeling. In other words, a new feature is constructed by sum the value of current week feature with previous weeks. Thus, the number of features (6) is constantly the same for all weeks as they are used to construct the model. (2) In the second, all the historical features in the previous weeks are directly appended as additional features to the features of the current week. That is, while the first week only has 6 features available to forecast the dropout label for the student in the second week, the second week will have 12 features to rely on to predict student dropout for the third week, with the third week having 18 features and so on. In this study, two algorithms General Bayesian Network (GBN) and decision tree (C4.5) are implemented. AUC, as a single measure, is invariant to the decision criterion selected. Besides AUC, precision is also presented in this work to show the robustness of the proposed stacking algorithm.
- **Dataset:**
- **Metrics:** In order to construct the features for model building, the following fea-tures are computed from the click stream data for each student: Number of discussion post, number of forum views, number of quiz views, number of module views, and number of active days.In order to understand which approaches performed works best at various points in the duration of a course, principle component analysis (PCA) for every week was performed using both feature spaces.
- **Case study:** We focused on a project management course launched in August, 2014, and hosted by Canvas. The course lasted eight weeks with 11 modules and 3617 registered students. Except for the first 4 modules which all took place in the first week, each module lasted roughly one week. The end of each module was in most cases accompanied by spaces for online discussion and quizzes. In total, there were 14 discussion forums and 12 multiple-choice quizzes.
- **Results:** The data become more non-isotropic in later weeks than the early stages and particularly from Week 6. Therefore, appended historical features were used in the first 5 weeks while for Week 6 and Week 7 only the current week of features was applied. The prediction performances for the first week are all the same because there are no historical features to append and they all use the same six features. The range of the AUC for GBN is [54.7%, 80.2%] and [80.2%, 94.4%] for modeling using sum features and appended features respectively. The average over the weeks for GBN is 64.7% and 89.0% for modeling with sum feature and appended features. The range of precision for GBN is [74.7%, 89.1%] and [79.3%, 94.9%] for modeling using sum features and appended features respectively. The average over the weeks for GBN is 83.4% and 89.7% for modeling with summed features and appended features. The same rule is also applicable to C4.5. The range of AUC for C4.5 is [49.1%, 84.9%] and [80.3%, 94.4%] for modeling using sum features and appended features respectively. The average over the weeks for C4.5 is 60.1% and 86.4% for modeling with sum feature and appended features. The range of precision in C4.5 is [77.2%, 89.9%] and [77.2%, 94.7%] for modeling using sum features and appended features respectively. The average of specificity over the weeks for C4.5 is 85.0% and 89.6% for modeling with summed features and appended features. Therefore, the proposed temporal modeling approach with appended features over a particular number of weeks outperforms the baseline summer measures in predicting students' possibility for dropping out. The range of AUC for GBN and C4.5 are [80.2%, 94.4%] and [80.3%, 94.4%] respectively. The average for GBN and C4.5 are 89.0% and 86.3%. By contrast, when stacking these two algorithms, the range of AUC is [80.7%, 96.1%] and the average is 90.7%. Similarly, the range of precision for GBN and C4.5 are [79.3%, 94.9%] and [77.2%, 94.7%] respectively. The average for GBN and C4.5 are 89.7% and 89.6%. The precision range for stacking method is [80.7%, 95.8%] and the average precision is 91.7%. Therefore, the stacking method, which takes advantage of the two algorithms, outperforms the base algorithm alone.
- **Notes:**

#### **Student behavior in a web-based educational system: Exit intent prediction** - [10.1016/j.engappai.2016.01.018](http://dx.doi.org/10.1016/j.engappai.2016.01.018)
- **Source:** Engineering Applications of Artificial Intelligence
- **Problem:** User behavior prediction.
- **Objective:** We focus on the task of the learning session end prediction for specific student in the e-learning web-based system (prediction of student exit intent in the session).
- **Methodology:** In order to better illustrate various sources of students' feedback which can be collected within the e-learning system we provide a brief description of ALEF, the adaptive learning system based on concepts of Web 2.0. It offers multiple courses focused on programming (procedural, functional, logical) and software engineering education. The user model is the next core component, which is based on the overlay user modeling. Collection of students' feedback is in the ALEF system realized by the feedback framework and their requests to system are captured by the logging framework. In order to be able to predict if a user will leave the application in the next action or he/she will continue the session, we have to deal with several limitations. At first, the data come in a continuous stream, which eliminates the usage of batch approaches. At second, the data are unbalanced, due to the fact that users in a session typically visit multiple LOs, while every session has logically only one leave action. The third limitation refers to the fact that data characteristics dynamically evolve over the time, due to the users' behavior changes (e.g., students behave differently at the beginning of the course or during the night before an exam). Based on these limitations, we proposed a polynomial binary classifier, using the stochastic gradient descent algorithm, which is able to process stream of data in real time and dynamically determine the importance of context attributes describing observations. To improve the classifier results we designed it to calculate weights individually for every user. The attributes come from multiple sources. At first, some attributes are acquired directly from educational system logs (the user, LO, timestamp). At second, some attributes are derived or calculated (student behavior in the session, his/her typical behavior, LO characteristics). (1) We compared precision and accuracy of the classifier trained by various ways – globally and personalized per user and per course. The global variant means, that the one model for classification was trained for all users. In the opposite, in the personalized “per user” variant, one model for each user was trained respectively. Moreover, we also trained a “per course” classifier,which analogically means, that one model for every coursewas trained. (2) We focused to optimization of the learning rate λ. This parameter is important in a phase of modifying attributes weights. (3) We compared solutions to classes' imbalances problem. At first we used the technique of over sampling the rarer class (last LO in session).
- **Dataset:** 452.000 observations.
- **Metrics:** To evaluate proposed classifier and to tune its features, we used several variants-global classifier (classifier using only the attributes weights trained for all observations) and also personalized “per user” (classifier using attributes weights trained for each user individually) and “per course” (classifier using attributes weights trained for each course separately).
- **Case study:** For our task we used the information about students' visits of learning objects (LOs). We had available activity logs about learning object (LO) visits from 882 users within 3 years, which means almost 452.000 observations. These logs contain attributes describing information about the student and LO and also the interaction details sucha as: – Student information: unique identifier, role; – Learning object information: unique identifier, course, type, difficulty, rating, parent, title; – Interaction details: user, LO, component, from which user came to LO-menu, some widget or from outside, begin of interaction timestamp and interaction duration.
- **Results:** (1) The classifier results increase in all cases logarithmically. After first iteration (452,000 observations considered) reaches the best results the global classifier variant (precision: global=0.262, per course=0.256, per user=0.229; accuracy: global=0.812, per course=0.810, per user=0.802). The reason is that this variant was trained on the highest number of observations; (2) All tested variants achieve a logarithmical increase of precision. There is also a dominance of variants considering number of rows trained before. The classification precision increase variously, based on used learning rate; (3) Results show that there is no clear winner, because accuracy and precision create an inverse proportion. When predicting user's future behavior, it is not always important only to identify the user's last action in the session. For this task it is sufficient to find out that the user will probably leave the application in short time or after the next few page views. Such an information can be even more useful than identification of the last action, because it provides more time to make an offer to the student what will make him/her retain in the application; The top attributes importance for individual users slightly vary from user to user, but there are some attributes, which weights belong consistently to the top important. These attributes are (in descending order): - Global average probability of leaving the application after visiting the current LO; - LO visit order in the current session; - User's average session length (LO visits) normalized by a static constant; - The difference of the session length (LO visits) between the current course and average of all user's courses normalized by a static constant; - The difference of user's average session length (LO visits) and the average of all users’ sessions normalized by a static constant; We found out there is a correlation between count of user's activity observations (882 users) and the precision (0.10419, p=0.00097), respectively the  accuracy (-0.19284, p=0.00001), according to the one tailed t-test on Pearson correlation measure. The most correlating attributes are classification prediction and final course results as the final course grade, the score reached on final exam and the final course score. We found the statistically significant correlation between the precision and the final exam score (0.1445, p=0.03415) respectively between the precision and the final course grade (0.1283, p=0.05434). In both cases, there was a positive correlation, which means that we were able to classify better the actions of users who reached the better final results in their courses.
- **Notes:**

### IEEE
#### **Big Data Application in Education: Dropout Prediction in Edx MOOCs** - [10.1109/BigMM.2016.70](http://dx.doi.org/10.1109/BigMM.2016.70)
- **Source:** 2016 IEEE Second International Conference on Multimedia Big Data (BigMM)
- **Problem:** One of the major concerns of MOOC is its high dropout rate, which mostly means vast majority of students who signed up the course in the beginning can not pass in the end.
- **Objective:** In this paper, we focus on relating student on-line learning behavior (such as watching video, posting a thread etc.) to drop out.
- **Methodology:** We extract the meta information about the course, student’s course enrollment records and most importantly, the users behavior log from Edx. We extract 39 courses data in total, which all have nearly 40 days (one month plus 10 days) of users behavior log. The first 30 days would be the input data source to the dropout prediction model, and the log of ten days afterwards can be used to test whether student is drop-out. We define student to be drop-out if he does not have any learning activities in ten continuous days. We design two relational tables to store the results after finish data preprocessing, User table and Action table. Each record in User table is a event generated by a specific user, and contains username, actionId, time, iP, agent, videoid, videoCurrentTime, videoOldSpeed, textitvideoNewSpeed. ActionId is the foreign key which points to the Action table. We divide all samples into two sets, one is training set, containing 120542 samples, the other is test set, containing 80362 samples. We train and tune four classification models, SVM, Logistics Regression, Random Forest and Gradient Boosting Decision Tree (GBDT). We adopt 5-fold cross-validation[7] for all classification models. The parameters of decision tree, namely n_estimators[8], ranges from 100 to 500, the step is 50, and the depth is 3.
- **Dataset:**
- **Metrics:** We define student to be drop-out if he do not have any learning activities in ten continuous days. We define three kinds of classification features: - EnrollmentFeature: Characterizes user’s behavior in a particular course. A period means a continuous session by a user. Inside a period the time interval between each two consequent actions is shorter than a particular threshold. This is innovated by our observation of log; - UserFeature: Characterizes users behavior in platform. There are totally 14 categories of course modules, and they are about, chapter, combinedopenended, course info, dictation, discussion, html, outlink, peergrading, problem, sequential, static tab, vertical and video; - CourseFeature: characterizes course profile in platform. For example, total visits times and total students engaged time can reflect liveness of the course.
- **Case study:** We collect 39 courses data from XuetangX platform, one of the largest on-line learning platforms in China, to build and test our dropout prediction model. XuetangX platform is based on Open Edx, which make our work extensiable to any MOOC platform which is also based on Open Edx.
- **Results:** We have found that some users have a preference of completing a course at night or even at the time nearly dawn. Moreover, some students study at both morning and night. The best result we achieve is 88% accuracy with GBDT model.
- **Notes:**

#### **Causal Association Analysis Algorithm for MOOC Learning Behavior and Learning Effect** - [10.1109/DASC-PICom-DataCom-CyberSciTec.2016.53](http://dx.doi.org/10.1109/DASC-PICom-DataCom-CyberSciTec.2016.53)
- **Source:** 2016 IEEE 14th Intl Conf on Dependable, Autonomic and Secure Computing, 14th Intl Conf on Pervasive Intelligence and Computing, 2nd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)
- **Problem:** Few people study the association between behavior and learning effects. Learning effect is still showed by considering unit testing, final-exam score and participation in discussion forums, which is very similar to traditional evaluation method.
- **Objective:** We proposed a casual association analysis (CAA) algorithm, which regarded different learning behaviors as reason and learning effect as result, to analyze the causal association between them.
- **Methodology:** In order to reduce the computation time required, we use the Apriori principle to reduce the term set. Apriori Principle: If an item set is frequent, all its subsets are also frequent, which can be described as follows: if a frequent item set is not frequent, all its superset are not frequent. We analyze a lot of learning behaviors which may affect the learning effect, then make a further study on the casual association between learning behaviors and effect by experiment.
- **Dataset:**
- **Metrics:** We describe the components of MOOC learners’ behaviors, as the followings: (1) Learning actors (WHO). Actors refer to all learners in MOOC environment; (2) Learning motivation (WHY). Motivation is the original purpose of learning, and learning behavior motivation can be divided into the following categories: active learning, for self-improvement, self-learning certified drivers and others; (3) Patterns of learning behavior (WHAT). In MOOC, patterns is the way to learn, including watching teaching videos, downloading materials, participating in discussion forums, completing unit test and so on; (4) Learning time (WHEN). Time of learning behavior includes login time, start and end time of watching a video, the time of browsing webpages and so on; (5) Learning space (WHERE). MOOC learners' learning space is MOOC platform; (6) Learning behavior media (MEDIA). In MOOC learning process, the media including videos, pictures, PowerPoint and other rich-media types, which are more intuitive and specific; In MOOC learning process, effective learning behavior of learners is broadly divided into the following five categories: (1) Selecting learning courses; (2) Watching videos; (3) Downloading materials; (4) Forum; (5) Homework and test Evaluation;
- **Case study:** The experimental data used for CAA algorithm are from the Chinese University of MOOC platforms course “College Computational on Computer Fundamental a perspective Thinking”.
- **Results:** We can obtain the following relationship between learning behavior and effects: (1) If the enrollment time of the courses is in the last half of the course, it is difficult to achieve good grades; (2) There is a positive correlation, within a certain range, between the lengths of online time and watching video with learning results, that is the longer the time, the better the effects, but beyond a certain range, the time of studying does not enhance the effect obviously; (3) The times of posting does not influence learning effect greatly; (4) Since the scores of tests and homework is added to the learning effect, the students with good effect basically completed the tests and homework on time; We find learning behaviors not conducive to improve some other learning effect: (1) Page refresh. When watching the video, refreshing the page frequently illustrates an unstable learning environment of the network; (3) Video pause. Time length of each MOOC video is generally within 5 to 20 minutes, and the learner pause frequently in a relatively short period of time, shows both the learner is lack of concentration, which not guarantee better learning results;
- **Notes:** MOOC provides learners a very large autonomy, but also require higher self-discipline.

## 2017

### ACM
#### **Stereotype Modeling for Problem-Solving Performance Predictions in MOOCs and Traditional Courses** - [10.1145/3079628.3079672](http://dx.doi.org/10.1145/3079628.3079672)
- **Source:** Proceedings of the 25th Conference on User Modeling, Adaptation and Personalization - UMAP '17
- **Problem:** It is not evident that stereotypes could be useful for predicting finer-grained problem-solving behavior, given that each course can feature many dozens of problems or exercises to solve.
- **Objective:** In this paper, we have atempted to explore the prospects of stereotypes in MOOCs “beyond dropouts” – for predicting student performance at the problem level. Our goal was to find stereotypes that could be useful (or actionable) for predicting a user’s success at solving problems.
- **Methodology:** The collected data included key-presses with time, assignment information, and student id, and was aggregated to describe meaningful events in the students’ programming process. Students were given a demographic questionnaire that solicited their age, gender, programming background, and the highest level of education attained. As a criterion to judge whether we were able to obtain the desired split between groups when looking at multiple ways to group students, we used differences between models of student learning in each group. Our rule of thumb is that if groups are truly different in how their members learn, the group models would demonstrate different performance in a cross-prediction task. We demonstrate our evaluation approach by assessing simpler ways of grouping. These simpler groupings would include those known a priori (e.g., demographics, prior achievements) and those known a posteriori (e.g., overall course performance). To model student learning, we used an approach called performance factors analysis that is based on logistic regression. We first examined simpler demographic and course performance-based approaches to student grouping. For the demographic data, we used gender and education level reported in the background information. Performance data were extracted from the course statistics available at the end. The key idea behind our behavior mining approach is to characterize student problem-solving behavior on the level of micro-paterns that define how the student progresses to the correct solution through several incorrect solutions, and how his or her knowledge grows from assignment to assignment. To build the micro-paterns, we started by processing student intermediate programming steps that classified the programming behavior at each step. Ten, we applied sequential patern mining to extract sequential micro-paterns. We tried five different setings for clustering students behavior, changing the clustering method (hierarchical, spectral), and the number of clusters.
- **Dataset:** We labeled 137,504 sequences of snapshots thatwere contributed by 1788 students solving 241 problems. Te length of sequences ranged from 1 to 475, with an average of 5.3; 92,549 sequences had more than one step, 64,328 had more than two steps, 48,195 had more than three steps, and 38,768 had more than four steps.
- **Metrics:** - Gender. Students were split by gender. The majority of students were males (about 71%). - Education level. Students were split into three groups. Tere were 524 students that had primary and secondary education, 154 students who atended college, and 120 students in graduate school. - Number of transactions. Students were split into three equal percentile groups – low, medium, and high – by the total number of problem atempts. - Problems Solved. This grouping was produced by an agglomerative clustering of four course-level counts: problems solved (at least one submission 100% correct), problems partially solved (at least one submission > 0% correct), problems atempted but not solved (at least one submission of 0% correct), and problems not atempted. The clustering yielded three groups: low (mostly not atempting problems), high (mostly solving problems), and medium (everyone else). - Percent Correct. This grouping was a split with three percentile groups with low, medium, and high values of overall percentage correct of the times students purposefully tested, ran, or submited their code.
- **Case study:** The data for the study comes from four introductory programming courses and MOOCs offered at a research-oriented University in Europe in 2014 and 2015. A single iteration of the programming course lasted for seven weeks and used Java as the programming language. Each week, students worked on tens of programming assignments with varying complexity. Out of 2739 students that started the courses, 1788 students were included in the initial sample. Out of those, 798 students answered the questionnaire and were included in the final analysis sample.
- **Results:** Students in Cluster 1 have a higher frequency of “tinkering” patterns, while the right side shows that the students in Cluster 2 demonstrate a much higher frequency of careful building patterns. More specifically, students in Cluster 1 frequently increased the conceptual content of their programs in consecutive steps with a long amount of time spent on at least one of those steps, spent a long time for increasing concepts in one steps and then took another long step decreasing concepts of their program or spent a long time at least on one step to increase conceptual content of their programs and not only failed in increasing the level of correctness, but also jumped back to the point where no test was passed. On the other hand, students in Cluster 2 did considerably less “tinkering” while focusing on large incremental building steps, in which they often spent a long time building their program. They often had long steps in which they added more concepts to the code and successfully increased its correctness. They had these building steps more frequently when they started developing their program, while they were on mid-stages of code development and also at the time they ended development. The results of cross-prediction using behavior-based cluster models demonstrated that the discovered clusters (tinkerers, movers) were not performance-based stereotypes. In other words, the two clusters that we found did not differ sufficiently to form stereotypes that could beter predict student performance and serve as a basis for personalization. However, we observed that they separated students into distinctive groups with stable but different behaviors. Tinkerers are less efficient and have lower grades. On average, students in Cluster 2 took fewer steps to solve the problem, were faster at solving the problems and as a result, were also more efficient in solving the problems and as a result, were also more efficient in solving the problems. Furthermore, the average student grade was also higher in Cluster 2 than in Cluster 1. On the other hand, there was no significant difference between clusters in respect of the number of solved problems, although students in Cluster 2 atempted more problems and solved more problems, on average, as compared to those in Cluster 1. Students in Cluster 2 often thought for a long time, added concepts, and increased code correctness. On the other hand, students in Cluster 1 seem to be weaker because they had more unsuccessful steps, they added concepts with no test being passed, or they changed (added/removed) concepts that did not influence the code’s correctness.
- **Notes:**

#### **Widget, widget as you lead, I am performing well indeed!: using results from an exploratory offline study to inform an empirical online study about a learning analytics widget in a collaborative learning environment** - [10.1145/3027385.3027428](http://dx.doi.org/10.1145/3027385.3027428)
- **Source:** Proceedings of the Seventh International Learning Analytics & Knowledge Conference on - LAK '17
- **Problem:** Studies clearly emphasise that for learning  analytics visualisations to have a positive effect, they need to be embedded into the instructional design and that the students’ personal preferences. - With the activity widget in use, do widget indicator scores again correlate significantly and positively with the tutors’ gradings of individual students? - With the activity widget in use, are the scores of the responsiveness indicator again better predictors for the students’ individual grades than those of the others? - With the activity widget in use, are the widget indicator scores produced in the first half of the course again better predictors than those produced in the second half?
- **Objective:** The aim is to make students aware of the platform activity of their team in relation to their own activity level.
- **Methodology:** We implemented the activity widget (called Elgg) into the learning environment of European Virtual Seminar (EVS) and made it available to students and tutors in the 2015-2016 run of the course. In this current study (referred to as ‘online study’ throughout this paper) we investigated whether using the activity widget live in a run of the course yielded similar or different correlations between the widget indicator scores and the grades and whether the regression analyses performed in structural equation modelling (SEM) showed approximately the same path-coefficients when compared to the exploratory study. There are two different views available in the activity widget: one showing the widget indicator scores for the whole run of EVS and one showing them per month. The widget indicator scores are automatically calculated from the data recorded in the EVS platform and are scaled from 0 to 10. We conducted a t-test to see whether the difference between the widget indicator scores from the online study and those from the exploratory study were significant or not. Then, the scores of the three widget indicators (W1, W2, W3) were correlated with the students’ four individual grades given by the tutors (T1, T2, T3, T4) using Spearman’s rank correlation. We also applied structural equation modelling in order to determine predictive relations between the widget indicators and the grades. We evaluated the activity widget using the Evaluation Framework for Learning Analytics (EFLA) questionnaire twice: the first evaluation was conducted in the middle of the course and the second one at the end.
- **Dataset:**
- **Metrics:** There are five indicators representing different types of activities on the platform: ‘W1 initiative’, ‘W2 responsiveness’, ‘W3 presence, ‘W4 connectedness’ and ‘W5 productivity’. The students’ final grade for the course can range from 0 to 10 and is comprised of several components: 50% are based on the grade for a team’s research report which is given by the expert; 20% are based on the grade for a team’s collaboration process which is given by the tutor; 30% are based on the grade for the individual student’s contribution which is also given by the tutor. This last grade is called the ‘individual-overall’ grade (T4) and is divided into three subgrades: ‘T1 planning & progress’, ‘T2 contribution to team’ and ‘T3 support’. These four grades evaluating an individual student’s contribution are the ones used in our analyses.
- **Case study:** (2015-2016) of the European Virtual Seminar on Sustainable Development (EVS). We were interested in the users’ experience with the widget during the 2015-2016 run. EVS runs for five months (November 1 till April 1) every year.
- **Results:** In the exploratory study, all grade-indicator combinations except the one between ‘T1 planning & progress’/‘W3 presence’ yielded significant and positive correlations when measuring the students’ activity over the entire length of the run. In the online study, however, ‘W2 responsiveness’ is the only widget indicator that positively and significantly correlates with the four grades. All grade-W2 correlations are significant at the 0.01 level and higher than .500. That is, there are less significant correlations in the online study than in the exploratory study but those that are significant are stronger. In the exploratory study the scores of the indicators ‘W1 initiative’ and ‘W2 responsiveness’ correlated significantly with all four grades in months 1, 2, 3 and 4 with W2 also significantly correlating with the grades T2, T3 and T4 in month5. The indicator ‘W3 presence’ had the smallest number of significant correlations with the different grades that were rather low. The strongest correlations were obtained between W2 and all grades in month2. Three widget indicator scores could be used as predictors for all four grades except the ‘T1 planning & progress’ / ‘W3 presence’ combination. The ‘W2 responsiveness’ was the strongest and most significant predictor. In our current online study, there are only three predictive relations i.e. the ‘W2 responsiveness’ indicator is a predictor for  the grades ‘T2 contribution to the team’, ‘T3 support’ and ‘T4 individual-overall’. None of the other indicators can be used as predictors. Most students liked the idea behind the dashboard and appreciated to see their platform activities being set in relation to those of their team members. Many students, however, mentioned several issues they were concerned about: The activity widget was not able to reflect activities outside the platform nor did it take the quality of the posts into account. Some students complained that they noticed people posting irrelevant things in order to achieve higher scores. The tutors also expressed their appreciation of the activity widget in the open ended comments and generally liked having the widget as a reference. For most of them, the activity widget confirmed their own impression about their students throughout the course.
- **Notes:** This paper is a sequence of a previous paper (10.1109/TLT.2016.2622268).

#### **A data-driven analysis of student efforts and improvements on a SPOC experiment** - [10.1145/3063955.3063956](http://dx.doi.org/10.1145/3063955.3063956)
- **Source:** Proceedings of the ACM Turing 50th Celebration Conference - China on - ACM TUR-C '17
- **Problem:** Can online learning activities explain the performance of students in SPOC classroom? What is the difference of online activities between students who get higher scores and those who get lower ones?
- **Objective:** In this paper, we leverage more data analysis to explore the potential reason why SPOC classroom students did better in algorithm problems.
- **Methodology:** To observe the learning trajectory of students in SPOC classroom, we collected student online learning behaviours in the form of clickstream. In this paper, we extract five main kinds of features, which are the common factors impact on students' scores significantly in general online courses. We also use statistical method, such as correlation analysis, regression model, to understand what factors may improve the algorithm performance of students in SPOC classroom.
- **Dataset:** The clickstream of this course is supported by Coursera, which contains around 400,000 records.
- **Metrics:**
- **Case study:** In 2014 spring, we conducted an experiment on the course of Data Structures and Algorithms in Peking University to evaluate the effectiveness of flipping with SPOCs. Among 828 undergraduate students majoring in science or engineering took this course, 397 of them attended five SPOC classes and the others attended the traditional classes. Among them, we collect 347 and 292 valid questionnaires at the beginning and the end of the course respectively. Most students in this course had a basic programming skills, but only a little of them had learnt object-oriented programming language, most students learnt C rather than C++ in the prerequisite course Introduction to Computing.
- **Results:** We compare the students with higher score and those with lower score, and analyze the reasons that cause the difference. These features include the page viewing times, the video watching times, the quiz submitting times, the programming lab submitting times and the number of the posts on the forum. The results of regression model demonstrate that page viewing times has a significant positive impact on students' final score, which means the higher students' efforts were, the higher their final scores would be. Nonetheless, the impact of other four factors are not significant, which is different from our common intuition. We find that the distribution of times that students with higher scores visited the website is almost like a normal distribution, while there is an abnormally high number of students with lower scores who visited the course website less than 400 times.
- **Notes:** Firstly, the students in the same class are also from the same school, so they could discuss in the classroom or dormitory, instead of using the forum. Secondly, this is a required course in their college life, so that every student has to submit the quizzes and the programming labs to get as more points as possible. Thirdly, students prefer to download the videos, instead of watching the videos online, because of the low network bandwidth in China.

#### **Blending Measures of Programming and Social Behavior into Predictive Models of Student Achievement in Early Computing Courses** - [10.1145/3120259](http://dx.doi.org/10.1145/3120259)
- **Source:** ACM Transactions on Computing Education
- **Problem:** How might predictive measures derived from programming behavior be augmented with additional, non-programming-based data? Can predictive measures that incorporate multiple data sources outperform measures derived solely from programming behaviors?
- **Objective:** To predict students’ success. This article is more aligned with research interested in identifying patterns of behavior.
- **Methodology:** (Experiment 1): We performed a linear regression with variables within the Normalized Programming State Model (NPSM) acting as predictor variables and individual assignment grades as the outcome variable.  We next aggregated an entire semester’s worth of IDE data and correlated these data with  students’ overall assignment averages; (Experiment 2): Based on social learning theory, which emphasizes the importance of regular participation within a learning community, we decided to construct a measure that describes students’ general social participation within a given window of time. We decided to make the time span equal to 2 weeks. In order to determine whether or not regular participation was a reliable predictor of academic success, we considered each student’s semester-long Participation Level average. In the case of Course 1, in which participation was mandatory, we were concerned that Participation Level might covary with prior computing grades. (Experiment 3): To evaluate the ability of the two measures to explain the variance in individual assignment grades, we replicate the basic statistical procedures reported in Experiment 1. However, this time, we add the Participation Level measure to each linear regression.
- **Dataset:**
- **Metrics:** (Experiment 1): Three performance indicators were used for the analysis: (1) students’ grades on individual assignments; (2) students’ overall assignment average, and (3) students’ final grades, which were based on the grades received on programming assignments (35%), labs (10%), participation (5%), in-class quizzes (10%), midterm exams (20%), and a final exam (20%). (Experiment 2): Based on posts: Level 1 - No posts or replies, Level 2 - One post or reply, Level 3 - Two posts and fewer than two replies, or two replies and fewer than posts, Level 4 - Two or more posts and two or more replies. (Experiment 3): Three performance indicators were used for this analysis: (1) students’ grades on individual assignments; (2) students’ overall assignment average, and (3) students’ final grades, which were based on the grades received on programming assignments (35%), labs (10%), participation (5%), in-class quizzes (10%), midterm exams (20%), and a final exam (20%).
- **Case study:** (Experiment 1): Programming and grade data for this analysis were collected from a 15-week CS 2 course offered during the Spring 2014 semester at Washington State University. In total, 140 students were enrolled in the course, 129 of whom completed the course. Of these, 95 students (87 male, 8 female) consented to release their programming log data and course grades for analysis in this study. The course used C++ as its instructional language, and required students to use the Microsoft Visual Studio programming environment; (Experiment 2): The same course considered in prior experiment. Course 1 enrolled 140 students, 129 of whom finished the course and received a grade. 108 of these students (100 men, 8 women) consented to releasing their data. Course 2 took place during the 15-week fall semester of 2013 and enrolled 123 students, 110 of whom completed the course and consented to release their grades (99 male, 11 female). (Experiment 3): The same course considered in prior experiment. The course enrolled 140 students, 129 of whom finished the course and received a grade. 108 of these students (100 men, 8 women) consented to releasing their data.
- **Results:** (Experiment 1): Regression analysis detected significant relationships between Normalized Programming State Model (NPSM) and final course grades, and between Watwin and final course grades. However, statistical analysis failed to detect a significant relationship between the Error Quotient and final course grades. On this particular dataset, the NPSM is a better predictor than the Error Quotient or Watwin Score. Interestingly, performing a linear regression with NU as the sole predictor variable explains more variance than either the Error Quotient orWatwin Score for both assignment  average, F(1,93 = 15.06), p < 0.01, Adj. R2 = 0.13, and final grade, F(1, 93 = 23.676), p < 0.01, Adj. R2 = 0.19. This strongly suggests that any measurement based on programming behaviors would do well to look beyond compilation behavior. (Experiment 2): Indeed, in Course 1, a correlational analysis between a student’s level of participation and prior grade in CS 1 was found to be significant (r = 0.468, p < 0.001). However, in Course 2, this was not the case (r = 0.12, p = 0.26). Hence, we decided to retain prior CS 1 grade as a covariate in further analyses with Course 1 (by using a MANCOVA), whereas we used a MANOVA for Course 2, adding prior CS 1 grade as a separate independent variable for comparison purposes. CS 1 grade was not a significant predictor of student performance in Course 1. In contrast, in Course 2, students’ prior CS 1 grade was a significant predictor of students’ exam scores and final grade. In both groups, a between subjects ANOVA detected a statistically significant difference between the quartiles (Course 1: df = 3, F = 11.62, p < 0.001; Course 2: df = 3, F = 8.15, p < 0.001). These results suggest that regular online social participation does, in fact, predict student grades in both treatments: Students who take a more active role in the activity stream tend to do better in the course. (Experiment 3): With the exception of Error Quotient’s prediction of final grade, combining a programming-based measure with a socially based measure produces a statistical model that accounts for more variance than any measure does in isolation. Indeed, in most cases, we find that a simple measure of social participation is able to account for more variance than either Error Quotient or Watwin.
- **Notes:**

#### **Deep Knowledge Tracing On Programming Exercises** - [10.1145/3051457.3053985](http://dx.doi.org/10.1145/3051457.3053985)
- **Source:** Proceedings of the Fourth (2017) ACM Conference on Learning @ Scale - L@S '17
- **Problem:** In these online environments, teachers’ ability to observe students is lost. Understanding a student’s incremental progress is invaluable.
- **Objective:** We performed representation learning with recurrent neural networks to understand a student’s learning trajectory.
- **Methodology:** Our method focuses on a student’s sequence of submissions within a single programming exercise to predict future achievement. We model student learning and progress by capturing representations of the current state of a student’s program as they work through the exercise. For our experiments, we focus on the sequences of intermediate submissions on Exercise 18 in order to predict their success on the next coding challenge. We believe success on the next exercise is a good indicator of the student’s learning progress since succeeding challenges add new concepts incrementally. For our model, we used a Long Short Term Memory (LSTM) RNN architecture. For our task, xt is the program embedding vector of a student’s trajectory at time step t. The output indicate whether the student successfully solved the next exercise. In order to expand Deep Knowledge Tracing (DKT) to  understand students as they produce rich responses over time within an exercise, we trained a recursive neural network which allowed us to vectorize the abstract syntax tree (AST) representation of student programs.
- **Dataset:** This Exercise 18 data set contains 1,263,360 code submissions, of which 79,553 are unique, made by 263,569 students. 81.0% of these students arrived at the correct solution in their last submission.
- **Metrics:**
- **Case study:** The Hour of Code course consists of twenty introductory programming exercises aimed at teaching beginners fundamental concepts in programming. Students build their programs in a drag-and-drop interface that pieces together blocks of code.
- **Results:** The LSTM model consistently outperforms the state of the art baseline by around 5%. Our program embedding model was able to correctly predict the output for 96% of the programs in a hold out set, compared to a 54% accuracy from always predicting the most common output.
- **Notes:**

#### **MOOC Dropout Prediction: How to Measure Accuracy?** - [10.1145/3051457.3053974](http://dx.doi.org/10.1145/3051457.3053974)
- **Source:** Proceedings of the Fourth (2017) ACM Conference on Learning @ Scale - L@S '17
- **Problem:** Producing a predictor can be difficult because the target values which indicate whether each student dropped out or completed the MOOC, and which are usually required by supervised learning algorithms, become available only at the end of a MOOC – at which point any intervention is moot.
- **Objective:** In this paper, we seek to fill a methodological gap in MOOC dropout prediction research and to investigate how the training paradigm affects the accuracy of the trained predictors.
- **Methodology:** We use a standard architecture – clickstream features classified by L2-regularized logistic regression – and compare performance across 4 different training paradigms as well as 2 simple baseline prediction approaches. In order to gauge how much “added value” is brought by machine learning approaches to MOOC dropout prediction that utilize detailed clickstream information, we also assessed the accuracy of two simple baseline heuristics. Baseline 1 uses only demographic information to make predictions. We also compared against an even simpler Baseline 2 which requires no machine learning (and hence no training data) at all; rather, the predictor makes predictions based on the number of days since the student last interacted with the courseware.
- **Dataset:**
- **Metrics:** The key independent variable that we manipulated in our experiments was the training paradigm. Specifically, for each week w of each of the 40 MOOCs, we trained one classifier for each of the following paradigms: (1) Train on same course (post-hoc): When predicting which students from course c will drop out, train using features and target labels from the exact same course c; (2) Train on other course from same field: When predicting which students from course c will drop out, train using features and target labels from a different course c? that has already completed, and for which the target labels are thus already available; (3) Train on many other courses: When predicting which students will drop out, train using features and target la-bels from many different courses (not necessarily within the same discipline); (4) Train using proxy labels: When predicting at week w which students from course c will drop out, train using proxy labels corresponding to whether each student persisted – i.e., interacted with the MOOC courseware at least once – within the previous week w−1.
- **Case study:** We conduct machine learning experiments on 40 HarvardX MOOCs. The grade threshold for certification differed across the MOOCs but is typically around 70%.
- **Results:** The most accurate training paradigm was Train on same course (post-hoc), which is the predominant training paradigm used in the dropout prediction literature. It achieved an accuracy (averaged over all 8 weeks, and all MOOCs within each week) of 90.20%. The second most accurate approach was Train using proxy labels. This approach does not require any MOOC – similar or dissimilar – to have been offered previously. This approach attained an accuracy (averaged over 40 MOOCs and 8 weeks) of 87.33%. The third most accurate approach was Train on many other courses, with an accuracy of 85.56%. This training paradigm attained a higher accuracy than did Train on other course from same field (76.85%), suggesting that, if it is not possible to exploit course-specific structure via either Train on same course (post-hoc) or Train using proxy labels, then it is better to harness prior data from a large variety of courses than from just a single course. Baseline 1 achieved an average prediction accuracy of 58.85%, suggesting that only a small amount of information about dropout is contained in the demographics. Baseline 2 performed remarkably well: It attained an average dropout prediction accuracy of 82.45%.
- **Notes:**

#### **Modeling MOOC Student Behavior With Two-Layer Hidden Markov Models** - [10.1145/3051457.3053986](http://dx.doi.org/10.1145/3051457.3053986)
- **Source:** Proceedings of the Fourth (2017) ACM Conference on Learning @ Scale - L@S '17
- **Problem:** While we can easily observe the changes in behavior of students in real classrooms, MOOCs present a challenge due to their hands-off nature and sometimes irregular schedule due to being a full-time worker.
- **Objective:** We propose unsupervised learning methods for automatically discovering and characterizing student learning behavior patterns or profiles from large collections of click logs associated with MOOCs.
- **Methodology:** We propose a novel two-layer hidden Markov model (TL-HMM) to discover latent student behavior patterns via unsupervised learning on large collections of student behavior observation sequences. The TL-HMM model provides with us the following two patterns to characterize student behavior: (1) the latent state representations, and (2) the latent state transitions.
- **Dataset:** We extracted a dataset consisting of 18,941 students who produced 85,240 sequences with an average length of 7.31.
- **Metrics:** We used the following ten actions as our action set A: (1) quiz start, (2) quiz submit, (3) wiki (course material), (4) forum list (view the list of all forums), (5) forum thread list (view the list of all threads in a specific forum), (6) forum thread view (view the list of posts within a specific thread), (7) forum search (a search query issued against the forum), (8) forum post thread (a new thread was posted), (9) forum post reply (a new post was created within an existing thread), and (10) view lecture (defined as either streaming or downloading a lecture video).
- **Case study:** We conduct experiments to qualitatively analyze both types of patterns discovered from empirical MOOC log data. We looked at the MOOC logs associated with the textretrieval-001 Coursera MOOC offered by UIUC.
- **Results:** Results show the latent state representations learned by fitting a 4-state TL-HMM to the textretrieval-001 sequence dataset. Our interpretations of the states is as follows: (a) state 0 likely captures all sequences where a student logged in to the platform and did nothing else (likely just checking for updates); (b) state 1 seems to capture a more engaged browsing session, where there is non-negligible probability associated with different activities such as quiz taking and forum browsing and, importantly, these activities have high probability symmetric edges (so students are taking quizzes one after the other, or viewing forum threads in succession); (c) state 2 captures a “forum browsing” state, with most weight being placed on consecutive thread views; and (d) state 3 seems to capture a more passive student, with negligible probability mass associated with forum activity (with low symmetry in the edges). Students that perform well in the course preferring states 1 and 2 over states 0 and 3. We plot the latent state transition diagram for a second group of “low” students. These students were selected so that they attempted all required quizzes in the course, but such that their average quiz score was ≤ 70%. Here, we see that state 1 has a large increase in size, where we might such that their average quiz score was ≤ 70%. Here, we see that state 1 has a large increase in size.
- **Notes:**

#### **Getting to Know English Language Learners in MOOCs: Their Motivations, Behaviors, and Outcomes** - [10.1145/3051457.3053987](http://dx.doi.org/10.1145/3051457.3053987)
- **Source:** Proceedings of the Fourth (2017) ACM Conference on Learning @ Scale - L@S '17
- **Problem:** Because English is the primary language for MOOCs, for learners with beginning and intermediate English language ability, authentic materials are often beyond their language proficiency and may become incomprehensible without help.
- **Objective:** In this paper, we aim to learn more about not fluent in English (ELLs), their motivations, behaviors, and outcomes.
- **Methodology:** HarvardX pre-course survey included four questions on learners’ English fluency in reading, writing, speaking, and listening (e.g., How fluent are you in English, the language of this course - reading?), and users rated their English fluency on a 5-point Likert scale (0=Weak; 1=Basic; 2=Intermediate; 3=Proficient; 4=Fluent). Here, we define ELLs as learners who answer in the fluency questions not fluent. We fit logistic regression models estimating students’ probability of certification by ELL status, controlling for age, gender, geography, developing nation status, and online behaviors.
- **Dataset:**
- **Metrics:**
- **Case study:** HarvardX MOOCs by analyzing data from 100 courses which is a subset of 150 courses that includes only those learners who filled out pre-course surveys self-reporting their English proficiency and motivations for enrolling in the MOOC (11.35% of all use cases, representing 521221 unique users).
- **Results:** The top five countries with most ELLs in HarvardX MOOCs are India, USA, Brazil, China, and Spain. We restricted our sample of MOOCs to only those that had 100 or more ELLs enrolled and 100 or more students who received certification (n=64). In these 64 courses, using the operationalization for ELLs that we presented above, we found that the proportion of ELLs by course ranged from 6.0-29.6%. At the course level, certification rates for ELLs ranged from 0.33-35.6% for ELLs and 0.32-56.3% for non-ELLs. Preliminary modeling in the full sample suggests the odds of ELLs certifying are roughly 0.6 times the  odds for non-ELLs (p<0.001), accounting for the nesting of students in courses and in different configurations of covariates. In regard a transcript variable which describes the number of video transcript events from tracking logs, we found that ELL users, on average, create significantly more transcript events than non-ELL users. Comparison of ELL and non-ELL users’ behavior of transcript download show that ELLs downloaded transcripts more than non-ELLs. In regard video and forum interaction events for ELL and non-ELL users based on the cumulative data on each type of interaction (i.e., video play, pause, forum threads, forum comments). On average, we found that ELLs take significantly more video play, video pause, and seek actions compared to non-ELL users. Forum participation metrics revealed that compared to ELLs, non-ELL users start more forum threads make more comments, and votes more on other learners’ posts.
- **Notes:**

#### **Characterizing ELL Students' Behavior During MOOC Videos Using Content Type** - [10.1145/3051457.3053981](http://dx.doi.org/10.1145/3051457.3053981)
- **Source:** Proceedings of the Fourth (2017) ACM Conference on Learning @ Scale - L@S '17
- **Problem:** English Language Learners (ELLs). Face the dual challenges of learning course content and learning in a non-native language, which can potentially increase their cognitive load and decrease their overall learning.
- **Objective:** To infer student behavior through clickstream logs.
- **Methodology:** We analyzed clickstream data focusing on interactions with video. The Stat. Therm. course ran from April 1 to June 30, 2015. We analyzed data from all 2971 participants, of whom 32% were categorized as ELL based on browser language preference. To understand how student behavior varied across different content with text and narrations, and those that contained narrations only, we coded the videos for content type (narration and text). We computed a number of video interaction behavioral (e.g., play, pause, seek and speed).
- **Dataset:**
- **Metrics:**
- **Case study:** Two MOOCs deployed on Coursera: an Introduction to Psychology (“Psych”) course of 47 videos, taken by 13,887 students, and a Statistical Thermodynamics (“Stat. Therm.”) course of 26 videos, taken by 2971 students. Approximately 33% of the 13,887 students enrolled since March 25, 2013 were categorized as ELL based on their browser language preferences. 3372 students (35% ELL) volunteered to participate in Coursera’s demographic survey. The top five English-speaking countries represented in the data were United States (31%), India (14%), Canada (5%), United Kingdom (5%), and Singapore (3%), while the top five countries categorized as ELL were Brazil (9%), Russia (7%), Greece (7%), China (6%), and Spain (6%). The Stat. Therm. course ran from April 1 to June 30, 2015. We analyzed data from all 2971 participants, of whom 32% were categorized as ELL based on browser language preference. Coursera’s demographic survey was removed in March 2015, therefore students who created a new account on Coursera after that date (such as the students in Stat. Therm.) were not presented with the survey. As a result, we do not have demographic information for the students enrolled in this course.
- **Results:** Our exploratory data analysis revealed that students’ behavior appeared to depend on the specific content of the video being watched. Our results confirm previous findings that ELLs exhibit distinct behavioral patterns compared to native English speakers in MOOCs. These behavioral patterns vary depending on whether there is text displayed on the screen. In portions of the video that had text as well as narration, ELLs paused significantly more, slowed down the play rate significantly more, and sped up the play rate significantly less than native English speakers. ELLs seek from these video sections significantly more, and seek to these sections significantly less than English speakers.
- **Notes:** Course video designers should consider the language needs of their students to decide what video design principles are appropriate to apply. Further study of specific video content types is needed.

#### **MOOC student dropout: pattern and prevention** - [10.1145/3063955.3063959](http://dx.doi.org/10.1145/3063955.3063959)
- **Source:** Proceedings of the ACM Turing 50th Celebration Conference - China on - ACM TUR-C '17
- **Problem:** Students' drop out.
- **Objective:** To develop a general system for predicting students’ dropout.
- **Methodology:** We conduct statistical study on students’ behavioral data and the tendency of the data. In this paper, we say a student was dropped out at a specific time if the student remain inactive from that time until the very end week counted towards grade. We conduct correlation analysis on the behavior data to see whether these are correlated with dropout or not. Besides the behavior data, we also take the changing between weeks of the data into account. In this paper, we use the Random Forest to classify the data. We conduct the statistical study on data of Peking Univeristy Data Structures and Algorithms course on Coursera and select another course – Introduction to Computing by the same institution on Coursera to test whether our system could fit other courses as well. The course lasts for 12 weeks and also have quizzes, programming assignments, midterm exam, final exam and forum discussion for grading.
- **Dataset:**
- **Metrics:**
- **Case study:** We conduct the statistical study on data of Peking Univeristy Data Structures and Algorithms course on Coursera. The course lasts 14 weeks and consists of lecture videos, quizzes, programming assignments and a discussion forum. The final two weeks are optional and not counted toward certification or final grade. In total, there are 13, 683 students who registered and visited the course web-page. The course itself is in Chinese, alone with English version of course material and video subtitles. 9, 088 out of the 13, 683 students are from developing countries. 72% of students visited the course fromAsia, and 21% of students are from North America. Temost of students (59%) are from Mainland China followed by 19% of the students from United States and 5% of Students from India. For the course, only 1, 037 students still visited the course till the last three weeks and merely nearly a hundred students passed the course.
- **Results:** Since if a student inactive for more than 3 weeks, the student will relatively highly possible dropout from the course, we label the data with whether there is any activity in most recent three weeks. For the first few weeks, our system performance is relatively bad. This is caused by users who registered for the course but did not intend to finish the course. Those users will visit the course material frequently in the first few weeks and cause a lot of noises. Such a noise may be eliminate if we classify user motivation in advance. The system on Data Structures and Algorithms course performs beter than the system on the other course. This may be caused by the fact that the Introduction to Computing course was opened for registration during the whole course period but the Data Structures and Algorithms course did not accept new students aſter the first two weeks. F-1 drop of our system is caused by the midterm exam. For both Data Structures and Algorithms course and Introduction to Computing course, the midterm exams are conducted at week 8. This fact infers that students may change their behavior patern when facing to an exam.
- **Notes:**

#### **Using Learning Analytics to Investigate Patterns of Performance and Engagement in Large Classes** - [10.1145/3017680.3017711](http://dx.doi.org/10.1145/3017680.3017711)
- **Source:** Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education - SIGCSE '17
- **Problem:** As class sizes grow instructors face heightened challenges in motivating and engaging diverse populations monitoring students’ achievements, and providing helpful feedback and guidance.
- **Objective:** The goal of the investigation is to reveal smaller groups of students with similar patterns of performance and engagement.
- **Methodology:** The LA approach used for investigating patterns of performance and engagement on the course has three main steps: (1) Select the population for the study; (2) Identify the clusters: For the data set used in this work, the method recommended the use of a minimal (2) or a maximal (14) number of clusters, neither of which are satisfying. Values in this range are explored using an alternative approach - the “elbow” method. The goal is to manually choose a K at which the marginal gain drops significantly, producing an angle (elbow) in the graph. For each value in the range, 100 executions of the k-means algorithm are run and the solution with the highest likelihood is selected. Final examination grades are used to provide context for the analysis and appraisal results; (3) Analyze and create appraisals for the clusters: The normalized, average values of the summative, formative, and behavioural features are abstracted onto a scale - very low (VL), Low (L), medium (M), high (H), and very high (VH) - to help reveal patterns within and among the clusters, considering multiple dimensions; A simple criterion for selecting students with extreme patterns is to consider the top X% of students with the highest or lowest measurements in each of the summative, formative, and behavioural dimensions. Experiments on two of these subpopulations are carried out: (1) overly engaged participants, which are those with the highest num-ber of interactions with on-line course material.
- **Dataset:**
- **Metrics:**
- **Case study:** Historical data from a required, introductory course in C programming for engineering students, APSC 160, at The University of British Columbia are used in this research. This course focuses on program design and problem solving, and has over 1000 students and 70 Teaching Assistants (TAs) each year. The course has two midterm examinations.
- **Results:** The analysis was performed running k-means with five clusters, identified as C1, C2, .., and C5. The result obtained was the normalized average values for the nine features spanning the S, F, and B dimensions. The overall class median on the final exam is 77. C1 consists of 39% of the class population. These students mostly perform extremely well on summative assessments. They frequently attend the lectures and are highly engaged with the in-class activities. In addition, they have high levels of interaction with the on-line course content; C2 consists of 13% of the students. These students mostly perform well on summative assessments. Their class participation and engagement with the in-class activities are both slightly higher than the class average. Students in this cluster spend very little or no time interacting with the course content available on-line; C3 consists of 12% of the students. These students mostly perform poorly on summative assessments. Their participation and engagement with the in-class activities and engagement in on-line activities are significantly higher than the class average, making them by far the most engaged with the course content available on-line; C4 consists of 26% of the students. The students mostly perform very poorly on summative assessments. Their class participation and engagement with the in-class activities are both slightly higher than the class average. Students in this cluster spent less time than average interacting with the on-line course content; C5 consists of 10% of the students. These students perform extremely poorly on summative assessments. They rarely attend the lectures and are disengaged during the in-class activities. Students in this cluster spent very little or no time interacting with the on-line course content; Students with the highest 20% of the average, behavioural values in the class are selected as the subpopulation. After identifying the clusters, the elbow method is used. The results obtained from running k-means with three clusters, identified using E1, E2, and E3, are reported. E1 consists of 12% of the students in the class. These students mostly perform relatively well on summative assessments. Their class participation and engagement with the in-class activities are higher than the class average. Their engagement with the on-line activities are very strong; E2 consists of 4% of the students in the class. These students mostly perform poorly on summative assessments. Their class participation and engagement is lower than the class average. Their engagement with the on-line activities are very strong; E3 consists of 4% of the students in the class. These students perform extremely poorly on summative assessments. Their class participation and engagement is significantly lower than all of the other clusters. Despite heavy interactions with the on-line course content, they exhibit extremely poor performance;
- **Notes:** Good overview about related work.

#### **Detecting changes in student behavior from clickstream data** - [10.1145/3027385.3027430](http://dx.doi.org/10.1145/3027385.3027430)
- **Source:** Proceedings of the Seventh International Learning Analytics & Knowledge Conference on - LAK '17
- **Problem:** Devising statistical techniques that help us identify students who are changing behavior in the middle of a term.
- **Objective:** To analyze clickstream data within a course in order to provide invaluable information to course instructors and to education researchers. We focus instead on detecting changes in individual student activity over time, relative to the activity of the class of a whole.
- **Methodology:** Our approach to change detection relies on modeling each student’s activity relative to that of the overall student population in the class. This population (or background) will typically vary significantly as a function of time since student behavior is strongly affected by temporal effects such as days of lectures, weekday versus weekend effects, assignment deadlines, exams, and so on. To detect changes in activity we allow for the possibility that each student’s activity rate changes at some unknown time point during the course. The proposed approach that we performed  works in the same manner for both the Bernoulli binary model and the Poisson count model, the only difference being in how the likelihood is defined and the parameters are estimated for each. Once the parameters of both the no-change and the changepoint models have been estimated, we select the best model from the two candidate models. In this paper we use the Bayesian Information Criterion (BIC) to fairly compare models (in the case where models have different numbers of parameters). We simulated daily binary time-series of student click activity. In the first dataset the clickstream data spanned 85 days, which included 10 weeks of instruction as well as activity before and after the 10 weeks.  We restricted changepoints to be in the range from day 10 to day 75. The students that were considered to have changed by the BIC scores were categorized into two groups: students who increased their click activity and students who decreased their click activity. We will refer to these groups as “Increased” and “Decreased” respectively. We calculated the probability of a student getting a passing grade given that the student is in the Increased group that the student is in the Increased group or in the Decreased group.
- **Dataset:** The first dataset has 380,000 records and the second 50 records.
- **Metrics:** Two types of student behavior are considered for the students described in this paper: - The first is a student’s pre-viewing behavior. It is defined as a Preview Event when a student views or downloads a file prior to the event start date; - The second type of behavior is related to the students’ reviewing activities. A Review Event is defined as an event when a student views or downloads a file after the event end date, e.g., a student downloading a lecture file after the class in which the material was covered.
- **Case study:** The clickstream data that we used in our study was recorded via the Canvas learning management system (LMS). Students use Canvas to download course content, take online quizzes, watch videos, and submit assignments. We used data sets from two courses at UC Irvine in our study, both offered during the 2015-2016 academic year. The first is a face-to-face 10-week course with 377 enrolled students. Lectures were held three times a week, and there were 3 midterms and one final exam. These students generated approximately 380,000 click events; The second data set was an online course offered for 5 weeks. There were 176 enrolled students in this course with 50 clickstream activity. There were 25 video lectures in total and students were supposed to watch one lecture per day from Monday through Friday. This course had a single final exam on campus after the 5 lecture weeks; The clickstream data spanned 85 days, which included 10 weeks of instruction as well as activity before and after the 10 weeks.
- **Results:** The variability in the simulation roughly corresponds to what we observed in the real student data. Half of the students were simulated no change in behavior over time. The BIC method was able to successfully detect that a model with a changepoint is preferred over a model with no changepoint for this data. The Poisson count model detects significantly more student changes than the Bernoulli binary model, for both preview and review event types; In the first dataset: More than a quarter of the students increased their previewing activities in the third week, which is the week before the first midterm. The most of the changes in reviewing activity happened towards the end of the quarter, particularly during week 10 before the final exam. The number of detected changes per week, for increases and for decreases, are strongly correlated. Students in the Increased group have a higher probability of passing the course, while the students in the Decreased group have a higher probability of failing; In the second dataset: A similar overall pattern to that for the 10-week class, namely that the Poisson model using counts detects considerably more changes than the Bernoulli method using binary data. 50% of students having increased or decreased count activity relative to the population, for each of the two types of  events. One difference we found between the two courses was the proportion of students who exhibited no change at all, for either preview or review events: 13% of students in the 10-week course and 25% in the 5-week courses. This difference might be due to the number of exam. We observe very high activities at the end of the course session for students in the Increased group, for both Preview and Review event types.
- **Notes:** The second dataset is very poor.

#### **Studying engagement and performance with learning technology in an African classroom** - [10.1145/3027385.3027395](http://dx.doi.org/10.1145/3027385.3027395)
- **Source:** Proceedings of the Seventh International Learning Analytics & Knowledge Conference on - LAK '17
- **Problem:** Teachers are still expected to perform from affective recognition and counseling, maintaining engagement of all pupils in the classroom, and intervening to aid poorly performing students. In developing nations, this problem is especially acute: poor facilities, a high student to teacher ratio, and a lack of technology solutions to provide insights and functionality to assist their students in their learning.
- **Objective:** In this paper, we focus on studying the engagement and performance of students in a classroom using the Cognitive Learning Companion (CLC) system.
- **Methodology:** CLC is a suite of cognitive capabilities supporting multiple modes of learning enabled on the mobile, and delivered through the cloud. CLC capture fine-grained user interactions with content and device. Student Activity Information (SAI) which sit on the cloud and is used to develop analytical and cognitive models. The insights that CLC will generate make use of analysis of SAI and will generate actionable insights with factors such as performance, engagement, context, atendance/dropout, teacher’s effectiveness that characterize learning outcomes, as well as teacher’s and schools’ effectiveness. The insights are the basis for identifying at-risk students, create individual or group profiles for each at-risk cluster, and develop preliminary intervention plans. CLC will recommend interventions that improve learning outcomes for underperforming learners (e.g., those with poor reading skills), introduces advanced learning/reading plans for well-performing learners, that provide resources and support for schools, as well as that allow to develop system guided feedback mechanisms for beter transparency and accountability measurement in schools. The CLC (Cognitive Learning Companion) has been developed through extensive interactions with primary school teachers from a leading K-12 group  of schools in Nairobi, Kenya, that are interested in adopting effective blended teaching/learning practices with their students. CLC supports delivery of content and assessments to students, and reports performance metrics to teachers. Using data collected, we analyzed and measured the classroom alignment based on students and teacher engagements in the classroom. We analyzed the performance data of the students based on the 16 quizzes for science and mathematics.
- **Dataset:**
- **Metrics:**
- **Case study:** We deployed the system in a local school in Nairobi Kenya for a 4 week pilot program (January – February 2015) in a classroom composed of 27 students atending Grade 5 mathematics and science classes, and 4 teachers. The teachers comprised one science teacher, one mathematics teacher and two head teachers who were assigned to physically observe the classroom dynamics and engagement. The official time allocated for the class lecture was 35 minutes. During this period the teacher has to properly breakdown the lecture into two or more sessions (e.g., lecturing, revision, Q&A, and/or classwork sessions) based on prescribed lesson plan, and then begins to distribute the 35 minutes to the individual sessions accordingly. Twelve lessons were conducted for each subject, and 16 quizzes (composed of a total of 169 questions) were given to students. The students were instructed to use our instrumented platform to carry out their learning while separate teachers taught their re-spective subjects of mathematics and science through the CLC.
- **Results:** The teacher spent most of their time on page 5. Most of the students were concentrated on Page 3. Further analysis of the content revealed that Page 3 was heavily loaded with examples. The total time taken to complete 7 quizzes in mathematics ploted against the score. Wherein on average most students spend about 10 minutes (∼500-600 seconds) to complete a quiz.  Students who spent longer amounts of time when doing the quiz, had higher performance scores. The local regression for each quiz shows a positive increase in performance over time and then a sudden drop in performance for the classroom. Using the performance data, we classified students into high performers (score above 75%), average performers (score between 50 and 74%), and low performers (score below 50%) students. We found that on average that students with high-levels of engagements tend to perform well in the given quizzes.
- **Notes:** It is not clear what engagement represents in this paper.

#### **Modeling Student Learning Styles in MOOCs** - [10.1145/3132847.3132965](http://dx.doi.org/10.1145/3132847.3132965)
- **Source:** Proceedings of the 2017 ACM on Conference on Information and Knowledge Management - CIKM '17
- **Problem:** MOOCs have been criticized for its early dropout, low completion rate, lack of personalized support and feedback, and strong dependency on participants’ self-regulation. Existing research on analyzing and mining students’ behavior patterns has varied across multiple aspects including identifying and classifying engagement styles, predicting dropout, grade of assignments or certificate, and recognizing students who need help. A less explored but perhaps a more significant aspect is the ways in which the students prefer to learn.
- **Objective:** Current research effort can be categorized into two major bodies: one focuses on qualitative analysis of students’ learning activities, and one focuses on building predictive models about students’ learning outcomes. Our goal in this work is to characterize and model students’ learning behaviors by capturing the homogeneity among students, which we define as latent student groups.
- **Methodology:** We introduce the concept of latent student groups. Each latent group possesses a unique distribution over students’ learning behavior paterns observed in the MOOCs activity log data, such as transitional paterns, intensity and temporal distribution of their learning activities. To understand the intensity of students’ activities peaked and if this is general in all students, we further examine the activity distribution in the students who kept engaging until the end of the course and those who finally got their certificates. We characterize a latent student group with a Hidden Markov Model (HMM), where hidden states depict students’ latent learning intents via their association with the observed learning activities. The latent student groups are identified by imposing a Dirichlet Process prior over the clustering property of students’ learning behaviors. We evaluate the proposed model first qualitatively demonstrate the latent student groups discovered from students’ behavior sequences, i.e., the distribution of latent states, and transition and emission probabilities in the identified groups. Ten we perform a series of quantitative evaluations to validate the effectiveness of our model against several state-of-the-art models in dropout prediction, certificate prediction, course satisfaction and demographics-based clustering. We include several popularly used clustering and classification algorithms in student behavior analysis as our baselines for comparison: Support Vector Machine (SVM), Vanilla RNN, K-Means, K-Means+HMM. As SVM and vanilla RNN baselines are supervised methods, we split the whole dataset into two parts (for training and the rest for testing), and perform 5-fold cross-validation on them. To ensure the consistence of the comparison results, the other three unsupervised algorithms, i.e., our model, K-Means and K-Means+HMM, are applied on the same train/test separation.
- **Dataset:** - Computer Science 101 (CS101): 7 weeks, 3.078 students and 3.332.743  logs; - Statistical Learning (Stat): 10 weeks, 8.090 students, 10.328.000 logs.
- **Metrics:** We define the five categories by aggregating similar click events in the log data, i.e., “video”, “problem”, “forum”, “navigate”, and “access”. For instance, the “video” category corresponds to the aggregation of events like “play video”, “stop video”, “pause video”, “seek video”, “speed change”, “load video”, “show transcript”, and “hide transcript”. As our model works in an unsupervised manner to cluster students into latent student groups, we compare all the algorithms based on metrics for clustering evaluations. Specificially, we choose the following three metrics that are commonly used to measure clustering results: - NMI. Normalized mutual information (NMI) is an information theoretic measure about how well the computed clusters and the ground-truth clusters predict one another, normalized by the amount of information inherent in the two clustering systems. A higher NMI means a beter clustering quality; - Entropy. It uses external information such as class labels to evaluate clustering quality. A lower entropy indicates a beter clustering quality; - F1 score. It measures howclose a clustering result is to the ground-truth cluster labels. F1 score is evaluated by computing the pairwise precision and recall in all clusters.
- **Case study:** We collected student learning activities from two edX courses, i.e., “Computer Science 101 (CS101, Summer 2014)” and “Statistical Learning (Stat, Winter 2015),” via a data-sharing agreement with the Stanford University. Records each course include viewing video lectures, atempting graded quizzes and homework assignments, and participating in course forum discussions. Each course there are more than ten thousand students, a small portion (around 30%) of them actually accessed the course content. The final selected datasets consist of 3078 and 8090 students for CS101 and Stat, respectively. The dropout rate increased from 40% to 92% in both courses.
- **Results:** We can notice a spike of activities in the last week of the course, when the students are required to finish certain actions in order to get the certificate. This indicates that students who are more motivated, e.g., to obtain the completion certificate, behave more consistently. The return time interval between two consecutive actions also carries important information about their learning experience. A shorter return time indicates a student is more engaged with the course content, as he/she continues the action flow; but a longer return time might indicate the student is losing his/her interest in the course. In regard a dropout prediction, our model achieved the best performance against all baselines in terms of NMI and entropy measurements, except the F1 comparison in Stat course. When we looked into the detailed clustering results from the K-Means baseline, we found almost all of the students were clustered into one big cluster every week. Although these two methods can leverage direct supervision from historical data, they still suffer from insufficient representation of students’ learning behaviors, and therefore are inept to accurately predict their future behaviors. In regard a certificate acquisition prediction, the predictions from each method are made after the Midterm, one week and two weeks before the end of the courses, because the completion rate is very low in both courses (less than 10%). In this experiment, we only use NMI to compare different algorithms’ clustering results. Both courses the proposed model outperformed all the baselines except the prediction made in Stat course one week before the end of it. We observed that our model performed better than other baselines. In regard demographics-based clustering we select gender, age and education level to analyze. Our model achieved much better information gain than the second best method, K-Means, on all three demographic atributes, except the education level in the Stat course.
- **Notes:** Understanding, analyzing and modeling a student’s unique preferred learning style effectively would help students with learning and retaining knowledge, and also help instructors be more aware of the learning progress of a particular student, so as to offer individualized or group instructions to improve students’ learning efficiency, depth, retention and enjoyment.

#### **Using learning analytics to explore help-seeking learner profiles in MOOCs** - [10.1145/3027385.3027448](http://dx.doi.org/10.1145/3027385.3027448)
- **Source:** Proceedings of the Seventh International Learning Analytics & Knowledge Conference on - LAK '17
- **Problem:** Fewer studies have focused specifically on the activities learners undertake in relation to the self-regulatory learning action of help-seeking. (1) What learner profiles emerge from help-seeking behaviours displayed in MOOC environments? (2) What effect does the learning design have on different help-seeking behaviours in MOOCs in relation to learner performance?
- **Objective:** We present the initial results of an exploratory study that uses learning analytics techniques to explore the profiles of learners’ help-seeking behaviour in MOOCs.
- **Methodology:** The approach adopted in this study relies on the lens of self-regulated learning. A feature matrix was then created which included 25 features which could represent help-seeking and was scaled using min-max feature scaling prior to the performance of dimension reduction using t-distributed Stochastic Neighbor Embedding (t-SNE). Traditional clustering algorithms like k-means was performed with the affinity propagation algorithm to find the number of clusters and simultaneously group similar learners within a cluster.
- **Dataset:**
- **Metrics:** Data for each of the MOOCs was extracted from the Coursera course and clickstream databases. behaviours including active forum participation (i.e., the creation of forum posts, the creation of forum comments, forum thread subscription, forum post up voting, forum post down voting, forum tag creation and forum reputation points), passive forum participation (i.e., forum post views, forum comment views and thread views), assessment (i.e., assignment, quiz and survey atempts, and the number of times a quiz result screen was viewed), search (i.e., search queries in discussion forums and search queries for overall course), video interaction features (i.e., total videos accessed, number of times the play event was triggered, number of times the pause event was triggered, number of times the playback speed of a video was changed, number of times video seeking eventswere triggered and total video event interactions), total clickstream views, grade, whether the learner was enrolled in a signature track and notification subscription (i.e, email announcement subscription).
- **Case study:** Four MOOCs were used in this study. These MOOCs were all developed by the University of Melbourne and delivered through the Coursera platform. Two of the MOOCs, Discrete Optimization and Assessment and Teaching of 21st Century Skills, represent MOOCs with a strong emphasis on professional development. Alternatively, the two other courses, The French Revolution and Animal Behaviour, were aimed more at a general interest audience. The sample included learners who achieved a grade of more than 40% for the course offered in 2015.  The sample size of learners for each MOOC used in the study was: Discrete Optimization (301), Assessment and teaching of 21st century skills (655), The French Revolution (825), and Animal Behaviour (658).
- **Results:** Five learner profiles were identified using the clusters generated through the affinity propagation technique: (1) Low engagement: Low grades, low page views, low video interaction, low, multiple grade atempts, no forum threads/post reads; (2) Assessment-focused - low grades: Low grade, high assessment atempts, low page views, low video interaction, no forum thread/post reads; (3) Passive engagement: Medium grade, moderate page views, low forum thread subscription, low forum /course search, moderate forum thread/post reads; (4) Active engagement: High grades, moderate assessment atempts, high pageviews, high forum participation, low forum thread subscription, low forum/course, search; (5) Assessment-focused - high grades: High grade, high assessment atempts, low pageviews, low video interaction, no forum activity, low forum thread subscription, low forum/course search; It was found that engagement with the discussion forums, whether active or passive, generally resulted in higher performance in the MOOCs overall.
- **Notes:**

#### **Pass the Idea Please: The Relationship between Network Position, Direct Engagement, and Course Performance in MOOCs** - [10.1145/3051457.3054008](http://dx.doi.org/10.1145/3051457.3054008)
- **Source:** Proceedings of the Fourth (2017) ACM Conference on Learning @ Scale - L@S '17
- **Problem:** To predict learner outcomes.
- **Objective:** In this paper we investigate whether learners benefit more from interacting directly with peers or positioning themselves to indirectly receive knowledge and ideas through the forum’s social network.
- **Methodology:** After a learner made a post on a thread they were automatically added to a mailing list which would send out an email each time there was a subsequent new post on that thread. To represent this as a social network, we modeled every learner as a node.  For the forums explored in this study there was no reliable means of recording if one post was in reply to another. Therefore, we used a model with undirected edges. We investigate whether learners’ final grades are more closely correlated with direct or indirect connections to their peers. The direct measures were the number of threads on which a learner posted and the number of peers a learner interacted with on those threads. The indirect measures we explored were the frequency at which a learner serves as the link between peers (betweenness), a learner’s connectedness to influential peers (Bonacich Power), and the number of peers through which a message from a learner would need to travel to reach all other learners (closeness).
- **Dataset:** Innovation 1: 7363 posters, 891 threads, 47000 sign ups, 7200 assignments, 6100 unique posters; Innovation 2: 2739 posters, 684 threads, 33000 sign ups, 3300 assignments, 2000 unique posters; Matlab 1: 1129 posters, 588 threads, 63000 sign ups, 7700 assignments, 1500 unique posters; Total: 11231 posters and 2163 threads;
- **Metrics:** The specific direct metrics we examined were the number of threads to which a learner posted and the number of peers with whom they shared at least one thread. We contrasted these with three indirect metrics of social network positioning: Betweenness, Bonacich Power and Closeness.
- **Case study:** The data used in this research came from three MOOC sessions. Two of these sessions were different offerings of Innovation, a course on entrepreneurship which heavily encouraged forum participation. The other session was Matlab, a course on programming. All three sessions had active instructor and teaching assistant involvement on the forums.
- **Results:** The number of threads a learner posted to was significantly correlated with grade more often than any of the other statistics. The next most promising statistic was the number of neighboring peers a learner has followed by betweenness and closeness. Bonacich Power was only significant in one sub-forum. These results suggest that direct measures of learner interactions are more consistently correlated with course performance than indirect measures. In order to predict the final grade, we concluded that combining the features does not add value if the outcome of interest is final course performance. In the Matlab course, instructors were more engaged than the average learner, as measured by all the metrics we considered in this study. This was not true for the Innovation courses with the exception of closeness. In all sub-forums in which instructors participated, the instructor closeness scores were at least a standard deviation ‘closer’ than the mean closeness score.
- **Notes:**

#### **Predicting the decrease of engagement indicators in a MOOC** - [10.1145/3027385.3027387](http://dx.doi.org/10.1145/3027385.3027387)
- **Source:** Proceedings of the Seventh International Learning Analytics & Knowledge Conference on - LAK '17
- **Problem:** Identification of students with engagement indicators that are expected to decrease in the near future.
- **Objective:** This paper proposes an approach to predict the decrease of engagement indicators in MOOCs using the students’ data about their activity in the course that is available at the moment in which the prediction must be made.
- **Methodology:** MOOCs are typically structured as a sequence of chapters. To define an indicator that quantifies the engagement of a student with a given type of task from the beginning of the course until the moment in which the indicator is computed. For example, an assignment engagement indicator could be obtained by averaging the percentage of assignments submitted in each chapter that has already ended. A dataset was created for each engagement indicator and chapters. Each dataset contains the students’ feature vectors computed at the end of a given chapter along with the corresponding label stating whether the engagement indicator decreased or not at the end of the next chapter after filtering out two types of samples. The most relevant features of the datasets generated for chapters were selected using a best first forward search combined with the Correlation based Feature Selection (CFS) method to evaluate the quality of feature subsets. CFS aims not only at finding subsets with features that have high individual prediction ability, but also a low degree of redundancy among them. Four different classification algorithms were employed to build models to predict the three engagement indicators according to the approach proposed in this paper. The selected algorithms were logistic regression (LR), stochastic gradient descent (SGD), random forests (RF) and support vector machines (SVM). The prediction tests are reported using area under the curve (AUC) as performance metric.
- **Dataset:**
- **Metrics:** Three main tasks that students were expected to carry out in the course: watching lecture videos, answering finger exercises and submitting assignments. The video engagement indicator was obtained by averaging the percentages of lecture videos that were totally or partially watched by a student in every chapter before reaching its end. The exercise engagement indicator was computed by averaging the percentages of finger exercises answered by a student in every chapter before its end. The assignment engagement indicator was calculated by averaging the percentages of assignments submitted in each chapter that has already ended.
- **Case study:** The data employed in the experimental study was obtained from the MOOC “6.002x Circuits and Electronics” that was offered on edX in the spring of 2013. The course schedule comprised 15 calendar weeks. The course was structured in 14 chapters and included a midterm and a final exam. Chapters comprised two types of assignments: homework problems that included numerical and formula responses, and lab exercises based on an interactive circuit simulator. The main contents of every chapter are explained in two sequences of lecture videos interspersed with short and simple comprehension questions called finger exercises. Grades were based on homework sets, online laboratories, a midterm and a final exam. Students with a final grade of 50% or greater received a certificate of accomplishment. There were 26,947 students enrolled in the course, only 6,752 watched at least a lecture video or answered at least a finger exercise or submitted at least an assignment in one of the first 12 chapters before the corresponding deadline. A certificate of accomplishment was granted to 1,099 students.
- **Results:** The best AUC value obtained for each dataset are using SGD in nearly all cases, although LR exhibits a very similar performance. SGD was able to predict the video engagement indicator with AUC values ranging from 0.81 to 0.894, exercise engagement indicator with AUC values from 0.837 to 0.906, and assignment engagement indicator with values from 0.718 to 0.914. The accuracy obtained with SGD ranged from 81.56% to 89,09% for the video engagement indicator, from 80.04% to 88.79% for the exercise engagement indicator and from 59.56% to 85.39% for the assignment engagement indicator.
- **Notes:** One limitation of the proposed approach is that the first predictions cannot be made before the end of chapter 2.

#### **Analyzing Content Structure and Moodle Milestone to Classify Student Learning Behavior in a Basic Desktop Tools Course** - [10.1145/3144826.3145392](http://dx.doi.org/10.1145/3144826.3145392)
- **Source:** Proceedings of the 5th International Conference on Technological Ecosystems for Enhancing Multiculturality - TEEM 2017
- **Problem:** To classify the students' learning behavior in order to understand how they are learning, how to improve the content structure and student's grade.
- **Objective:** In this paper, the exploratory analysis of data (EDA) is performed by classifying the collected data and filtering the collected characteristics with Principal Component Analysis (PCA).
- **Methodology:** (1) To analyze the correlation matrix of each trait with the goal of extract from the trait the less correlated parameters; (2) As the content instructional design is really high and then difficult to interpret, PCA algorithm was applied to detect the principal components to reduce dimensionality of the problem and extraction of characteristics of our dataset; (3) Based on the results, applied classification algorithms to the data set to detect different states for any trait proposed. The result of this steps is a classification of a students for a concrete trait. For this purpose, a non-supervised learning algorithm has been chosen: K-means. To determine the number of possible group elbow method has been used;
- **Dataset:**
- **Metrics:** Classification of features is proposed by taking into account the traditional organization ofLMSs: (1) Pre-study: the student reads the general information for the course, and knows how to deal with it. The pre-study trait contains indicators related to number of accesses to surveys, guides about the general course, software, installations, and news; (2) Basic study: the student uses the specific course contents and contains indicators about the access to general contents, multimedia learning, theoretical contents, and specific guides; (3) Evaluation: the student makes assessments and practices. Indicators are thought about partial qualifications about each module, the whole module, and the total final grade; (4) Communication: the student uses the communication tools for the study by asking for questions and helping other students. Several general forums area available, as well as specific contents about a set of contents of modules; (5) Implication: the student spends time regularly in the course. They are measured through students’ actions, intervals, dedication, are recorded, among others;
- **Case study:** Moodle is the LMS employed in this study. The course selected for this study is a basic desktop tools course where the use ofword processor, spreadsheet, and so on (five modules in total), are introduced to students. All modules have the same structure: multimedia interactive content, basic documentation and assessment. A total of 70 anonymized parameters/indicators, which can be seen as characteristics, have been recorded and used to define five learning traits.
- **Results:** There are at least three parameters with a low correlation: Quality survey, Expectative survey and General information. The first component PCA1 it can be understood as the component that highlights the student that read all the previous information and follows the news. This kind of student it can be understand as a conscientious student. By other hand the second component describes the students are more interested in obtaining the more practical knowledge about the course: software guides and video tutorials, it may be considered as a practical student. Finally, the PCA3 describe a student that only is interested in what is hap-pening in the course, probably is a sophomore or a student that do not understand the distance methodology. Four group of students are built based on the PCA1 and PCA2 components. The four groups then can be described as: - Group 1: PCA1 high. Students that read everything and prepare themselves to begin the course; - Group 2: PCA1 low. Students that neither read some much information nor are prepared to study; - Group 3: PC1 medium and PC2 High. Students that are more interested in quick and visual information and do not take care of another information maybe because they are sophomores; - Group 4: PC1medium and PC2 Low. Students that are neither interested in quick information maybe because they are sophomores.
- **Notes:**

#### **A Visual Approach Towards Knowledge Engineering and Understanding How Students Learn in Complex Environments** - [10.1145/3051457.3051468](http://dx.doi.org/10.1145/3051457.3051468)
- **Source:** Proceedings of the Fourth (2017) ACM Conference on Learning @ Scale
- **Problem:** How can we interpret learners’ interactions in online learning environments? More specifically, how can we identify strategies, infer intentions, assess learning, or evaluate quality of engagement? How can one use log data to evaluate student’s attitudes and knowledge when the design space is unlimited and the solution space is underdefined? Furthermore, how can this be applicable to learners with diverse backgrounds and goals?
- **Objective:** In this paper we seek to combine the benefits of theory-driven (top down) knowledge engineering and data-driven (bottom up) knowledge discovery to make sense of data from complex systems. We propose a workflow, supported by a system, which allows researchers to hypothesize patterns based on data and quickly test these hypotheses.
- **Methodology:** We have developed Tempr, a visual analytic approach, to assist with knowledge engineering. The main goal of this tool is to inform the top-down knowledge engineering process with patterns that emerge bottom up. Tempr takes input in the form of two ".txt" files, one for each group of learners. Each file is comprised of the sequence of log events for each user in that group. Tempr has three main panels: - The Heatmap Panel. This panel supports hypothesis raising. It provides an overview of all actions in the tool over time and helps to identify which actions show similar patterns and could be further investigated; - The Merging Panel. This panel supports exploration of different groupings of actions. This is key for knowledge engineering, as it helps to see what combined actions may highlight the differences in how groups of students learn; - The Visualization Panel. This panel enables the comparison of an action or merged sets of actions by groups of students over time; We extracted the sequential list of actions logged as each student interacted with the virtual lab. Classifying learners to two groups was done by calculating their learning gains. We then applied a tertiary split, comparing the high learning gain group (HL) to the students with low learning gains (LL), ignoring the middle third.
- **Dataset:**
- **Metrics:**
- **Case study:** The PhET Circuit Construction Kit (CCK) is the most commonly used virtual lab in the PhET family. Students in CCK construct and test DC electric circuits by using a variety of components that include batteries, wires, light bulbs, resistors, and measurement instruments such as ammeters and voltmeters. Overall, there are 124 different types of actions that students can perform at each moment. These actions include adding, moving, joining, splitting, and removing components, as well as changing the attributes of components. The study included an activity on the topic of DC circuits, which took 30 minutes. The activity asked students to explain the effect of connecting multiple resistors on the voltage and current of a circuit. Students received this general learning goal and a general recommendation to explore several resistors within the same circuit loop, on different circuit loops, and a combination of the two. Pre- and post- tests were given to each student so that we could measure learning gains across the activity.
- **Results:** The heatmap sorted by the last column, to emphasize differences between common actions of HL vs. LL during the last 20% of the interaction. We see that the top seven events are ones that HL perform more frequently than LL. HL and LL are nearly indistinguishable. LL testing drops off at the end of interaction, but during the rest, the median lines are roughly equivalent. Both have quartiles that are roughly equivalent, with LL behaving less consistently in the middle of the interaction.
- **Notes:**

#### **Planning prompts increase and forecast course completion in massive open online courses** - [10.1145/3027385.3027416](http://dx.doi.org/10.1145/3027385.3027416)
- **Source:** Proceedings of the Seventh International Learning Analytics & Knowledge Conference on - LAK '17
- **Problem:** To what extent can these same approaches be deployed to help students achieve their stated goals?
- **Objective:** In the current research we address this question in a natural field experiment, by prompting some MOOC students to plan their course participation in advance.
- **Methodology:** The pre-course survey has two purposes in this research. First, the survey contained our planning prompt treatments. Second, the pre-course survey also collected information about demographic and behavioral covariates, as part of a standard battery of survey questions included in all HarvardX classes. These answers were used to define our exclusion criteria, and allowed us to test for treatment effect heterogeneity. We constructed 13 separate logistic regressions, each of which tested a single interaction between the treatment effect and one of the covariates listed: % Female; Lives in USA, Country HDI; Full-Time Employed; Part-Time Employed; Concurrent Student; Bachelor's Degree; Advanced Degree; MOOCs Enrolled; MOOCs Completed; Pre-Course Enrollment; Pre-Course Verification.
- **Dataset:**
- **Metrics:** Our pre-registered analysis plan focused on only one primary outcome: whether or not students completed enough of the coursework to “earn a certificate” in their class. This requires earning a grade above a certain threshold (between 70-80%, depending on the class), and if they achieve the threshold before the final date, they are deemed to have “certified” in the class.
- **Case study:** This research was conducted in three online courses created by HarvardX, teaching Business, Chemistry and Political Science. Each course was taught by a Harvard professor, and paralleled an existing course at the University. The course material consisted of video lectures, assigned readings, and discussion boards, and chapters of the course were doled out in sequence over 2-3 months. Grades were determined by a combination of quizzes, peer-assessed written assignments, and self-assessed participation. Every class run by HarvardX has a pre-course survey embedded as the first chapter of the course. Intentions were self-declared in the pre-course survey, as an option in a non-binding multiple-choice question. 60,778 students enrolled in these three courses.
- **Results:** The majority of students (57%) intended to certify, indicating high interest. And most students who earned a certificate intended to do so (83%). But intentions alone were not enough, as among those students who intended to certify, only a minority completed enough work to achieve that goal (16%). We find a consistent and robust effect of planning - students prompted to write out their plans at the beginning of the course had a higher follow-through rate than those who were not prompted write out their plans. The results imply that planning prompts increased course completion by 29%. The analysis finds that only one of the covariates significantly moderated the effect on course completion. That is, MOOC students who were also at a traditional school were more likely to benefit from the planning prompt. The next strongest moderator, age, is highly correlated with school enrollment, and not significant after this correction. The regression coefficients imply that planning increased completion rates from 13.6% to 19.4% among those not enrolled in school, and from 12.5% to 25.5% among students who were concurrently enrolled in school.
- **Notes:**

#### **Certificate Achievement Unlocked: How Does MOOC Learners' Behaviour Change?** - [10.1145/3099023.3099063](http://dx.doi.org/10.1145/3099023.3099063)
- **Source:** Adjunct Publication of the 25th Conference on User Modeling, Adaptation and Personalization - UMAP '17
- **Problem:** Do MOOC learners behave differently after clinching a passing grade?
- **Objective:** In this work, we answer this question by empirically exploring to what extent MOOC learners’ behaviours are impacted by one particular assessment event: the course passing event (i.e. the moment the learner accumulate sufficient scores to receive a certificate).
- **Methodology:** We analyze the log traces (our observable events from which to infer learning behaviour). In order to determine whether behavioural changes can be observed on individual learners, we represent each passer by a vector of her normalized quiz scores. Then, we resort to k-means clustering of all generated vectors. We measure the distance between learner feature vectors by their Euclidean distance. We generate multiple k-means clusterings with k=7. For each of these seven clusterings, we assess the clustering quality using silhouete coefficients, an effective technique for assessing the quality of a clustering result. Our final clustering is the one with the highest silhouete score. Each unit in each cluster is represented by the average score learners in that cluster achieve in that unit with their respective confidence bands.
- **Dataset:**
- **Metrics:** - MOOCs & MOOC units: A MOOC M consists of a sequence of m units. Each unit contains videos and/or quizzes and is typically designed to be completed over the course of one calendar week; - Unit-n quizzes & videos: there are two core components of xMOOCs: (1) lecture videos, and (2) quizzes; - Learner’s Activities: We consider quiz scores and time spent on videos as the main measurements for learner activity on a MOOC platform; - Passers: Passers are learners who are eligible to receive a MOOC certificate at the end of the MOOC as their assessment scores reach the defined threshold;
- **Case study:** The log traces of 4, 000 learners who successfully completed one of four MOOCs offered on the edX platform: Introduction to Functional Programming (FP), Data Analysis (DA), Treatment of Urban Sewage (SEW), Solar Energy (SE). Each course is set up as an xMOOC with weekly releases of lecture videos and graded quizzes. None of the MOOCs have a final exam. The assessment is exclusively based on the scores learners reached in the graded quizzes. In each MOOC learners can continuously check their scores by accessing their course “Progress” page.
- **Results:** After the minimum passing threshold is reached, the variance of scores increases drastically, with a number of learners starting to score very low. In FP and SE, a larger number of  learners who maintain high scores after passing than learners who score low aſter passing. Concretely for FP, in the final unit, more than two thirds of the passers score 80% or higher on the remaining quizzes. In every MOOC a small to medium fraction of passers does not watch any of the unit’s videos (3.4% in FP, 3.0% in DA, 10.8% in SEW and 20.0% in SE). Across the four courses the trend over time is similar: the number of passers who do not watch lecture videos increases in the final units. With respect to the completeness of lecture video consumption we find a clear divide between DA and SEW & FP: in DA & SE learners’ normalized video consumption peaks around 1.0 (indicating that many learners watch the whole video lecture at normal speed), while in SEW & FP for most passers the normalized duration is below 1.0 indicating that they skip at least parts of the videos. We can conclude that learner behaviours on quizzes are distinctive before and after passing. In regarding the clusters:
- For passers who pass MOOCs early, the clusters share very similar activity levels before passing, but begin to differ immediately at the passing unit; - For nearly all passer groups and MOOCs, choosing k = 2 clusters yields the best clustering fit. This strongly indicates that for early passers, there are two dominant behaviour paterns: “reducing scores” (rapidly declining quiz scores for the units following the passing event) and “keeping scores” (the averaged scores of passers in one cluster stay more or less stable at a high level) after passing; - While FP and SE the dominant cluster is “keeping scores”, in DA across all groups the “reducing scores” passers dominate over those that keep participating in the assessment; - We also observe a behaviour unique to DA: a group of learners starting off slowly and finishing strong; We conducted a similar analysis for video consumption, but as expected based on the observation analysis, we did not find meaningful clusters or behavioural changes aſter passing.
- **Notes:**

### Elsevier
#### **Measuring motivation from the Virtual Learning Environment in secondary education.** - [10.1016/j.jocs.2017.03.007](http://dx.doi.org/10.1016/j.jocs.2017.03.007)
- **Source:** Journal of Computational Science
- **Problem:** European students abandon their studies with a lower secondary education or less. European secondary schools are routinely using VLEs in everyday learning activities.
- **Objective:** We envision an approach based on applying advanced Learning Analytics techniques to existing and extended educational data, in order to enhance the existing use of ICT in learning whilst providing key instruments to the educational system overall. The goal is not predicting the motivation of students, rather its measuring it.
- **Methodology:** Our purpose is to measure motivation individually (for every student), dynamically on a daily basis, and for every subject task the student confronts. There is not a consensus about the components of motivation; here we will adopt a point of view that is in accordance with our purpose of measuring motivation. In this sense, according to some authors motivation can be decomposed into three major components, regarding activity, persistence, and intensity. We validate our model checking its unidimensionality, its indicator reliability, and its construct reliability. Unidimensionality is measured by the Cronbach’s alpha. It measures the ratio of the intra correlation among indicators in respect to the total correlation of indicators. Values greater than 0.7 are accepted as corresponding to unidimensional constructs. In our case, as expected, we have obtained a value of 0.60, below the threshold. This is due to the behavior of FA rate and Agility rate indicators. Indicator reliability is measured by the communality of the indicators. It ranges between 0.61 and 0.81, indicating high reliability, except for FA rate and Agility rate with communalities of 0.26 and 0.29 respectively. Construct reliability is measured by the Average Variance Extracted by each construct respect of its indicators. Its value of 0.89 indicating a high internal reliability in the detected construct.
- **Dataset:**
- **Metrics:** We focus on the following types of behavioral indicators: (1) Speed indicators, measuring how fast an individual starts the task after its recognition, how fast an individual completes the task, and how fast the student moves from one task to the next one. - Agility Rate, measured as a sigmoid function of the difference between the date on which a task became available in respect of the first access of that task, by a student; - Time spent, measured as the total time spent on the VLE to complete a task; - Transition time, measured as the time spent between consecutive tasks on the VLE; (2) Persistence indicators, measuring the extent to which, an individual continues steadfastly to pursue a goal, in spite of the inherent difficulties. - Number of logs executed in a given task per day; - Resilience level, computed as the ratio of the maximum number of tasks performed in the interval of two hours in respect to the total number of tasks performed in a day, in a given subject; - Number of attempts used to submit a given task; - Persistence level, measured as the mean time used between consecutive attempts at a given task; (3) Intensity indicators, it is the effort done to achieve the goal. - Delivery rate, computed as the ratio of the fulfilled obligatory tasks in respect to the pending ones in a given subject; - Engagement level, computed as the ratio of all activities per-formed in one day for a given subject in respect to the maximum activity in that subject the past two weeks; - Competitive level, computed as the ratio of the total of the activities performed by a student in a given subject, in respect to the most active student on the same day; (4) Choice indicators, it is measure of the student priorities and hence motivation. - Curiosity rate, computed as the ratio of the fulfilled non-mandatory tasks in respect to the pending ones in a given subject; - Forum access. It measures the instances of accessing a Forum related to activities of a given subject on particular day; - Forum participation. It measures the participation in a Forum related to activities  of a given subject on particular day; - Priority rate, computed as the ratio of log access to any activity, on a given subject, in respect to all the logged access to that subject, in the particular day;
- **Case study:** To conduct this study we have developed a pilot in collaboration with six secondary schools in Catalonia. The schools agreed to participate by choosing two target subjects each and including the corresponding teachers as members of the pilot study. In total, the pilot survey has been composed by 6 schools, 12 different subjects, 15 secondary professors, and 487 students, which have been tracked during the 134 days of the first semester of 2014–2015 academic year.
- **Results:** The obtained results confirmed the correlation among the indicators, and the feasibility to measure motivation by using these indicators. It was proven that it is possible to index and measure it in a concise, systematic and structured way, through constructed indicators. The main output of the system, the motivation index, is easy to understand and very informative. It can be used by teachers to monitor and compare students and also by school managers to monitor and compare teachers performance, providing they operate on the same level of Moodle usage. The proposed approach of motivation modeling is general because both its components (activity, persistence and intensity) and their proposed indicators (speed, persistence, intensity and choice) are valid in all educational contexts. Nevertheless, the obtained index has its own limitations, among which, the first one is that it just addresses the activities conducted through the VLE and not all learning activities in which the students are engaged.
- **Notes:** The result are very poor to conclude something. However, as a pioneer of its own kind, this model needs to be tested in a more extensive context and it asks for further experiments and improvements in order to converge to a reliable index of motivation. Also, there is a data growth problem. The tables in the database grow at the rate of the number of students by the number of subjects per day. In its full potential, the system needs to rely in Big Data management capacities, in order to be applicable to large-scale problems.

#### **Using process mining to analyze students' quiz-taking behavior patterns in a learning management system** - [10.1016/j.chb.2017.12.015](http://dx.doi.org/10.1016/j.chb.2017.12.015)
- **Source:** Computers in Human Behavior
- **Problem:** We seek to answer the following research questions: Can process mining methods be used to detect specific student behavior patterns during various types of quizzes in the learning management system Moodle? If so, what types of student behavior patterns can be identified using these methods? And how frequently do the individual types of student behavior occur?
- **Objective:** The focus of this study is student behavior modeling, the core of our research addressing the possibilities of using process mining methods to identify different student behavior patterns in quiz-based learning activities.
- **Methodology:** We use innovative process mining methods to identify and map students’ quiz-taking behavior patterns in selected quizzes of various types. In this study, the process is the completion of a quiz by a student. The research framework based on process mining methods which we used in this study can be represented, in a simplified way, by a linear sequence consisting of the following three components: - Learning management system. LMS Moodle which, offers tools for designing and implementing various types of quiz-based learning activities; - Data extraction and pre-processing. In order to analyze data using process mining methods, it must be first extracted from the learning system and then transformed into the format required so that the selected process mining algorithm can be applied; - Behavior patterns discovery. In this component, the crucial part is an application of the selected algorithm to identifying the various ways students use to undertake the quiz-based learning activity. The first part of data analysis involved exploring so-called trace variants, i.e. cases with the same sequences of activities. The main goal here was to discover what trace variants the dataset analyzed contained and then to look for similarities between various trace variants and to group trace variants leading to similar patterns of students’ quiz-taking behavior. In the second step, we created and analyzed so-called process maps, which we then explored using the Disco tool based on an adjusted and extended implementation of the Fuzzy Miner algorithm called Disco Miner. The main reasons we selected Fuzzy Miner algorithm are, firstly, its capacity to identify even less frequent trace variants and secondly, the clarity of its outputs presented as process maps.
- **Dataset:** The dataset to be analyzed contained the following variables after pre-processing: Quiz ID, Student ID, Attempt Number, Activity Name, Activity Group, Activity Group Abbreviation and Timestamp. Among the total of 1298 attempts in all five analyzed quizzes.
- **Metrics:** An overview of elementary pre-processed quiz activity names in a quiz: - quiz_viewed (qv): Student displayed front page with instructions and information about the quiz; - quiz_attempt_started (qas): Student launched a new attempt in the quiz; - quiz_attempt_viewed (qav): Student displayed a question page (questions can be on one or several pages); - attempt_summary_viewed (asv): Student arrived at the final test overview page; - quiz_attempt_submitted (qasub): Student submitted the quiz attempt to be assessed; - quiz_attempt_reviewed (qar): Student displayed feedback on the previous attempt.
- **Case study:** We analyzed data from a total of five quizzes assigned in five different courses in the learning management system Moodle at the Faculty of Arts, Masaryk University, Brno (Czech Republic). All quizzes were administered in courses taught at the said faculty, whose thematic focus is philosophy, culture, and history. The quizzes for the analysis were selected based on two primary criteria: (1) we strove to include quizzes of various types, both in terms of their structure in the system and in terms of their pedagogical purpose; (2) we were interested in quizzes with the highest maximum number of attempts, and hence, with greater chances of various types of quiz-taking behavior occurring.
- **Results:** The followings quiz-taking behaviors were found: (1) Standard. It was found situation when all quiz tasks are on a single page (a), and when are from quizzes with tasks on several pages (b, c); (2) Feedback misuse. Can be characterized by the student displaying the page with an overview of the previous attempt (i.e. an overview of detailed feedback on the previous attempt) during quiz-taking. This type of student behavior therefore can occur only in courses with certain feedback display settings and for courses with multiple attempts; (3) Study materials misuse. The student looks into the study materials for the course while taking the quiz. The activity is usually repeated, with the student opening various kinds of materials available in the course; (4) Multitasking. We opt for this label to reflect that while pursuing the primary activity (the quiz attempt) the student engages in another (secondary) activity; 988 attempts (76.12%) involve standard quiz-taking behavior. Study materials misuse occurs in 260 attempts (20.03%), being the main type of non-standard quiz-taking behavior found in the LMS data. The remaining two types of quiz-taking behavior occur in significantly fewer cases. Feedback misuse occurs in 16 (1.23%) attempts only and multitasking behavior in just 3 attempts (0.23%). The above-listed types describe 97.38% of student quiz-taking behavior. We present all trace variants of the individual quiz-taking behaviors in the form of three process maps. Students, for instance, relatively frequently do not submit the quiz to be graded themselves but their attempt is submitted automatically when the set time limit is up. A non-negligible proportion of students leave the final quiz overview page to go back to completing questions. This behavior may occur because the students have noticed an unanswered task or it may suggest that specific Quiz/Test-Taking Strategies have been used. Finally, the process maps indicate that students taking quizzes relatively often use multiple viewer windows.
- **Notes:** The role of the teacher (or course creator/ administrator), who determines through quiz settings whether a particular type of non-standard quiz-taking behavior is possible at all. Further research can focus in particular on combining the approach we used with other approaches to detecting non-standard student quiz-taking behavior. Another possibility for extending the results of this study is exploring differences in quiz-taking behavior between successful and unsuccessful students. Another compelling line of research address the possibilities of so-called comparative process mining and using process mining to compare process variants or different groups of cases. As far as the nature of the data analyzed is concerned, it is important to note that event logs in fact do not record student behavior in LMS in its entirety but always necessarily capture only a fragment of real-life behavior. We analyze a limited number of quizzes and quiz types.

#### **Rational herd behavior in online learning: Insights from MOOC** - [10.1016/j.chb.2017.10.009](http://dx.doi.org/10.1016/j.chb.2017.10.009)
- **Source:** Computers in Human Behavior
- **Problem:** People generally follow their peers' choices and opinions, this widespread social phenomenon is often defined as herding. However, rational herding is distinct from irrational herding in terms of the dominant aspect in the decision-making process, which occurs because of observational learning among learners. While irrational herding occurs when learners passively mimic others' choice.
- **Objective:** In this study, we examine the herding phenomenon in online learning and evince that herding is common in online learning. In addition, we investigate the differences between rational and irrational herding in terms of instructors, courses, and learners.
- **Methodology:** The study starts from the data collection, and then we do the pre-process on the raw data including the data clean and extraction. We employ the econometric model and payoff externalities to test the herding phenomena. Then, sequential regression and cross-section models are employed to identify the rational versus irrational herding. Last, we test the robustness in terms of the difficulty level of the courses as well as the learner experience.
- **Dataset:** We collected accessible data on 619 courses and 2.071.147 learners as well as the 19.451.428 learning record for the learners, including learning time, learning process, notes, codes, and questions. The 619 courses are taught by 590 instructors.
- **Metrics:** For the herding estimation, we first determine whether the behavior of the online learning exhibits herding. Our model contains 2 variables: (1) Lag learn count, which estimates the influence of the previous period's learning; (2) Lag process, this variable is used to compensate for the deficiency of Lagged learn count in detecting the herding effect;
- **Case study:** iMOOC is one of the leading online learning websites in China, specializing in the training of coders. The main courses include the following categories: client-side development, server-side development, mobile development, database, cloud computing and big data, operation and maintenance, and visual design.
- **Results:** From the sequential correlation, the number of learners in the current period significantly positive correlated with the followers and fans. The experience of the learners and the instructors affects the learners' online learning. The notes and codes are negative correlated, whereas questions is positive correlated, indicating that online communication stimulates the users to continue learning (in iMOOC, questions are similar to discussion). This maybe because the notes and codes are for individual learning records, whereas the questions are for public discussions. Analyzing the Lag learn count and Lag process together, we identify the existence of herding effect in the online learning context. The details are: (1) Lag learn count has positive impact on learning (0.1627), which verifies the existence of herding behavior because if the learners observe many learners learning the same course, they might mimic others' behavior in the next period. Thus, the learning on the course will increase; (2) Lag progress, many learners who start a course in the last period continue studying the same course. Thus, the Lag learn count has positive influence. However, the Lag process has significantly negative impact (-0.0177), indicating that the learners' learning progress exhibits a decline trend in the next period; Rational herding evidence 1: (1) Lag followers learn has negatively impact (-0.2007); (2) Lag followers learn same course, which estimates the influence of followers learning the same course in the previous period, has positively influence (0.1239); Rational herding evidence 2: (1) The impact of Lag followers learn same course and Lag authentication X followers learn same course are opposite, indicating that the learners' interest in the course reduces significantly after observing the instructor's personal information. (2) The impact of Lag followers learn and Lag authentication X followers learn are opposite, indicating that the learners do not blindly follow other learners; Rational herding evidence 3: The impact of Lag followers and Lag authentication X followers are opposite, indicating that although the followers affect the behavior of the learners, the learners adjust their learning behavior according to the observed courses' attributes actively; Summarizing: We first evince the existence of herding in online learning by analyzing the learning data empirically; The learning in previous period (Lag learn count) is significantly positive correlated with the learning in current period in all models, that is, herding effect in online learning is affected by the number of learners positively; From the rational versus irrational herding test as well as the robustness test, rational herding dominates the learning process; The higher the difficult level of a course is, the more the rational herding of learners will be. In addition, the more experienced the learners are, the more rational their herding behavior will be; Herding behavior is affected by learners' experience positively. Approximately 25.4% of the learners cannot complete 10% of the course content that they have started.
- **Notes:**

### IEEE
#### **Zipf's law in MOOC learning behavior** - [10.1109/ICBDA.2017.8078713](http://dx.doi.org/10.1109/ICBDA.2017.8078713)
- **Source:** 2017 IEEE 2nd International Conference on Big Data Analysis (ICBDA)
- **Problem:** The learning coverage be law and can thus the Zipf’s in MOOC observes approximated with a Zipf distribution.
- **Objective:** We focus on the distribution of learning coverage. We define learning coverage as the amount of course materials accessed by the students.
- **Methodology:** This procedure, however, considers the intercept as a nuisance parameter, should be equal to log(C). Moreover, linear regression through ordinary least squares is inefficient in this case, given that r is an integer. A better method to fit the Zipf’s law for empirical data is to use the maximum likelihood estimation (MLE), which has been proven effective in practice for similar distributions, such as Zipf-Mandelbrot law. In our study, we also use MLE to estimate the exponent parameter in the Zipf distribution and check the goodness of fit by performing. The number of students generally decreases as the learning coverage increases, but not monotonically. We test the learning coverage for power law. However, the null hypothesis that the learning coverage fits power law is rejected in all 76 courses. We believe that the absence of monotonicity is a major cause for the rejection. We test the learning coverage for the Zipf’s law, which describes the relationship between frequency and rank. We sort the frequency of each learning coverage in descending order, and then conduct linear regression to the frequency versus the rank in log-log scale as a pre-experiment. The results show that the learning coverage fits well with a Zipf distribution consistently. For all 76 courses. And for all but 3 courses, the R-squared value is larger than 90%, indicating a high goodness of fit. The result is encouraging. We decide to use the maximum likelihood method for a more effective and accurate estimation of the Zipf’s law.
- **Dataset:** There are more than 40 million event log entries in the dataset. The minimum number of participants for a course is 101, and the maximum number of participants is 9,668. There are 27 courses with more than 1,000 participants. On MOOC, the content is usually organized as a multi-level tree: each course contains several chapters, a chapter contains several sections, and a section videos, texts, various materials, including contains assignments and quizzes.
- **Metrics:** We define learning coverage as the amount of content a learner has accessed. It is a measure of how far a learner has advanced into the course. Several different courses.
- **Case study:** In this study, we use a dataset provided by xuetangX which contains data of 76 (http: //www.xuetangx.com/) courses held by Tsinghua University in year 2014 and 2015.
- **Results:** The results show that the learning coverage of 47 courses is likely to fit with the Zipf’s law, which accounts for 61.84%. The results also show that over 25% of the courses have a p -value approximate to zero. We observe that the courses with more than 3,000 participants all reject the Zipf’s law. On the other hand, most courses with less than 1,000 participants are likely to fit with the Zipf’s law.
- **Notes:**

#### **Teaching Software Engineering Principles to K-12 Students: A MOOC on Scratch** - [10.1109/ICSE-SEET.2017.13](http://dx.doi.org/10.1109/ICSE-SEET.2017.13)
- **Source:** 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering Education and Training Track (ICSE-SEET)
- **Problem:** Most of the programming materials do not teach software engineering methods. (RQ1) Do students perform better on the questions related to the 7 programming concepts than on the 5 related to code quality? (RQ2) Are there age-related differences in the students’ performance on all 12 concepts? (RQ3) Can we predict course completion based on information from the student profiles and their activities in the first week?
- **Objective:** We wanted to investigate if the similar factors apply. Specifically, we are interested in examining which characteristics from the student profile and what type of participation and grading data from the first week of the course can be used for identifying the students that have potential for successfully finishing the course.
- **Methodology:** To explore the idea of teaching software engineering to children, we developed an online introductory programming course. To answer RQ1, we investigate the difference between the grades that the students received in programming and in software engineering questions. Related to RQ2, we compared groups older and younger than 12 respectively. We therefore created for each tested programming concept two sets, one with the test results of the 11 and the 12 year old students and one with the results of the 13 and 14 year olds. We then used the Wilcoxon rank sum test to compare the two sets and Cliff’s delta to calculate the effect size. We then used the Wilcoxon rank sum test to compare the two sets and Cliff’s delta to calculate the effect size. For RQ3, we use machine learning. We generate the dataset of the features. We model retention as a binary classification problem, where given those features, we try to predict whether the student will score over 60% on edX’s cumulative scoring mechanism. To evaluate the classification performance, we use Area Under the Receiver Operating Characteristic Curve (AUC) and F1 metrics. To select the appropriate classification algorithm, we run a 10-fold random selection cross-validation and aggregate the mean values for each classification metric. The results showed that both Logistic Regression and Random Forests achieve an AUC score of 0.84 and a comparable F1 score of 0.49 and 0.48 respectively.
- **Dataset:**
- **Metrics:**
- **Case study:** This course was marketed as an introductory programming course using Scratch. The course consists of six ‘weeks’. Each week consists of three components: videos, quizzes and forum interactions. The course included a total of 64 quizzes with a total of 117 multiple choice questions, while the 2 tests included 55 questions. The questions of the quizzes and the tests were designed to correspond to specific programming concepts. In total 3,179 students enrolled in the course, while 2,220 actively participated in it by watching at least one video or submitting at least one answer to a quiz or participating in the course discussion forum. 181 students successfully completed it, defined by edX as obtaining 60% of the possible points. Of the 2,220 active students in the course, 1,243 filled in this student profile questionnaire. Most students (73.71% of the 1,202 that filled in their age at the questionnaire) are 7 to 11 years old, with the most popular ages being 8 (for 19.97% of the 1,202 students) and 9 (for 17.30% of them). Out of the 1,093 students that reported their gender at the questionnaire, 346 (31.66%) are female. The first quiz on the course material was answered by 1,452 students (65.4% of the active ones).
- **Results:** (RQ1) Students generally performed better at the quiz questions, which is expected because they were designed to be formative rather than summative. The lowest percentage of correct answers was given for the Operators, Procedures and Variables categories. Comparing the performance of students on questions related to software engineering concepts with their performance on programming concepts, we found that the difference is not significant; (RQ2) Our findings indicate that the two sets differ significantly (p <0.05 and effect size >0.4) for the case of Operators and, especially, Procedures, with effect sizes of 0.468 and 0.615 respectively. The p-value is also small for the case of Conditionals, Coordination and Variables, but the effect size is too small. For the software engineering concepts that the students were tested on, the difference between the two age groups was not significant; (RQ3) The course “at home with help from parent” and being late in joining the course reduces the chance of successfully completing it. At the same time, the factors that have a statistically significant positive influence are the number of questionnaires that have been submitted, the mean grade in the quizzes and having a failed answer. The experience, the number of times that the discussion forum was accessed, the distinct videos that were watched and the number of skipped questionnaires also explain a small, yet significant percentage of the total deviance. The results also reveal that the effect of the gender is statistically insignificant, meaning that there are no differences between boys and girls in their chances of successfully completing the course.
- **Notes:**

#### **Using pre-course survey responses to predict sporadic learner behaviors in advanced STEM MOOCs work-in-progress.** - [10.1109/FIE.2017.8190623](http://dx.doi.org/10.1109/FIE.2017.8190623)
- **Source:** 2017 IEEE Frontiers in Education Conference (FIE)
- **Problem:** It has been found that substantial resources in terms of cost, time, and labor are required to produce MOOCs.
- **Objective:** The purpose of this paper is to understand the effect of academic preparedness and intention to fully utilize course materials on learner usage, specifically sporadic users, in an advanced engineering MOOC.
- **Methodology:** We developed a pre-survey for an advanced engineering MOOC. This was followed by data collection in the form of clickstream data for the complete 8-week duration of the course to categorize users into different learner groups based on their course behavior using a clustering method. Finally, the pre-survey results were analyzed alongside different learner groups to detect predictive behavior from pre-survey results. A survey was included in the course as part of the first week assignment. The survey gathered information on learners’ demographic, prerequisite background (especially mathematics), reasons for enrolling in the course, expected time commitment in the course, employment status, expectations for course participation, personal goals for the course, intrinsic and extrinsic motivation for learning, and English language skills. We used a K-means cluster analysis to analyze clickstream data. The cluster analysis was used to find similarity in learner usage patterns in accessing the course. We performed a Chi-square test of proportion for every pre-survey question related to our research questions to determine the likelihood that two variables are statistically different from each other (with a significant p-value of less than 0.05). The normal approximation assumption to use a Z-table was justified as np>5, where n is equal to the number of samples and p is equal to the proportion.
- **Dataset:** We collected a total of 196,836 clicking incidents or records, but only used a subset of 36,000 records for clustering.
- **Metrics:**
- **Case study:** NanoHUB-U offers courses on groundbreaking nanotechnology and nanoengineering topics. The courses are made available through the edX platform and have no prerequisite requirements for enrolment. The course focused in this study is titled Fundamentals of Nanoelectronics: live Basic Concepts and had 9,888 learners enrolled in its version. The course consisted of four units which were divided across 8 weeks.
- **Results:** A total of five clusters were identified: fully engaged learners, consistent learners, two-week engaged learners, one-week engaged learners, and sporadic learners. While 2,756 users were clustered, only 1,451 users initiated the pre-survey and only 969 users completed the survey. Out of 969 users, 218 users who completed the survey were not part of any clusters so they had to be removed from our dataset. So, we were left with 751 users, of which 172 were fully engaged, 102 were consistent, 121 were 2-week engaged, 196 were 1-week engaged and 160 were sporadic users. Learners who intended to watch all videos in the course were less likely to be sporadic users. This suggests that having course prerequisites and some background related to the course content could be a good filter for identifying sporadic users from the other cluster groups. In addition, we found that users who planned to dedicate more than 6 hours on average to the course and aimed for achieving a higher grade in the course were less likely to be sporadic users. Learners who intended to participate in all aspects of the course were also less likely to be sporadic in their use of course material. This result is particularly interesting as it can be used to filter out sporadic users at the start of the course by identifying the intent of the learners.
- **Notes:**

#### **Supporting quality teaching using educational data mining based on OpenEdX platform** - [10.1109/FIE.2017.8190730](http://dx.doi.org/10.1109/FIE.2017.8190730)
- **Source:** 2017 IEEE Frontiers in Education Conference (FIE)
- **Problem:** Identifying students who are at risk of failing a course.
- **Objective:** We elaborate how to build an external grader based on OpenEdX platform to integrate an auto-grading submission system.
- **Methodology:** There are different events initiated by learners, which are generated by their interactions with the OpenEdX platform. These events could be grouped into following types: resources interaction events (video, textbook, course navigation), problem interaction events, discussion forum events, survey events and so on. We used gradient boosting decision tree (GBDT) classifier to build the predicting mode based on an open source machine learning library scikit-learn. We employed 10-fold cross validation and use the average ROC-AUC as the evaluation metric.
- **Dataset:**
- **Metrics:** When users stream video, the following events will emit: ‘load_video’, ‘pause_video’, ‘play_video’, ‘show_transcript’, ‘speed_change_video’, ‘stop_video’ and so on. Another import type we focus on is the problem interaction events, which are emmitted if interact with core problem types: ‘problem_check’, ‘problem_show’, ‘problem_save’, ‘problem_reset’, and ‘problem_graded’. The last part events we cared is discussion forum related events: ‘comment.created’, ‘response.created’, ‘response.voted’, ‘searched’, ‘thread.created’ and ‘thread.voted’. We extracted some features that proposed by crowd, such as ‘number_forum_responses’, ‘number_submitions_correct’, and ‘average_pre_deadline submission_time’. We also extracted students’ learning behavior related features: indicate whether the learner would review resources if submitted a wrong answer; demonstrate the time that a learner spent on the resources before submit a problem; whether the student just resubmit another answer quickly when the former one is incorrect. We used some features to partition students into different types: reveal students who submit homework in timely fashion or at last minute fashion; show students who attempt to do the homework soon after the project has been released or not; indicate how long the learner spent time in discussion forum if the submission is incorrect.
- **Case study:** In the first 3 projects (P0, P1, and P2), learners need to build CPU components and learn how to code and debug assembly programs. In Fall 2016, there were 377 students registered our course. On the other side, the log record from OpenEdx showed that there were 15465 evaluation requirements in P0, 12068 evaluation requirements in P3, and 11993 evaluation requirements in P5 when the students prepared each project at home during Fall 2016 semester.
- **Results:** When the tutorials parts finished in week 06, we used the data from week 01 to week 06 with their freshman year’s GPA to predict the performance, and received the ROC-AUC metric is 0.936. We used data from week 06 to week 12 to predict whether the learner succeed in the final week 16, and the ROC-AUC metric is 0.975 using 10-fold cross validation. After quantifying importance for the feature of each experiment, we use the mean number as the week-invariant feature importance. The higher the score indicates that the feature is relatively more important in the predictive model. Feature x205 (Difference in grade between current pset grade and average of student’s past pset grade) imports temporal trends, and it became more important contributor to successful prediction; Feature x208 (Average time between a problem submission and problem due date over each submission) is a relative comparison, and it received higher importance score; Features that indicate study method are more important, such as feature x303 (Video access time before the problem submit) and feature x311 (Average of all problem the time between Problem_first_check and problem_first_get); Features that show the time spent on the course are play more important role in the predicting, such as feature x306 (Sum of all time intervals dedicated to the problem Time), feature x307 (Time between first problem_get to last problem_check), and feature x309 (Average for each problem learner finished correctly in the last 48h before the problem due).
- **Notes:**

#### **Students' behavior analysis under the Sakai LMS** - [10.1109/TALE.2017.8252342](http://dx.doi.org/10.1109/TALE.2017.8252342)
- **Source:** 2017 IEEE 6th International Conference on Teaching, Assessment, and Learning for Engineering (TALE)
- **Problem:** To solve cheating problem in e-learning system, some researches aimed of detecting cheaters by analyzing their learning behavior.
- **Objective:** Our work makes two major contributions. First, we developed a method to extract students' behavior under Sakai platform automatically. Finally, we evaluated students' performance using GBDT classifier.
- **Methodology:** We developed an automated extracting tool, written in Python language. This tool request HTTP pages of Sakai and parse the html file. Furthermore, this tool extract and preprocess the learning behavior related data. We used the Pandas library of Python to read the .xls files of reports. The reports data will be preprocessed together with the data extracted from html files. We used LCA (Life Cycle Assessment) methodology in evaluating, which means we considered students' behavior in whole course duration. To reduce the influence of nonsignificant data, the first step is designing features. In order to determine which kinds of students' learning behavior most represent their performance, we compared the learning behavior of the well-performed students and the inactive. For more dependable results, we also compared different machine learning classifiers.
- **Dataset:**
- **Metrics:**
- **Case study:** Sakai is an open source learning manage system and developed by four leading U.S. universities to meet the needs of teaching, learning and research. We use it in our course site. This course is the The Introduction of Computational Thinking (Fall 2014) in Beihang University.
- **Results:** In general, the students who perform excellent in learning spend more time in Lessons tool and perform actively in discussion forums. Besides, they submit assignments and quizzes earlier. The time between submitting and deadline and the scores represent their performance in assignments and quizzes.
- **Notes:**

#### **Implementation of learning analytics framework for MOOCs using state-of-the-art in-memory computing** - [10.1109/ELELTECH.2017.8074997](http://dx.doi.org/10.1109/ELELTECH.2017.8074997)
- **Source:** 2017 5th National Conference on E-Learning & E-Learning Technologies (ELELTECH)
- **Problem:** MOOCs are facing severe challenges such as high dropout rates, improper assessment, and lack of outreach to rural areas etc.
- **Objective:** Make a learning analytics (LA) system to measure, collect, analyze and report the data about learners and their contexts, for the purpose of understanding and optimizing learning and the environments in which it occurs.
- **Methodology:** Using this LA system, first, the general statistical analysis on the data has been carried out to dwell the student engagement patterns with the course content, forum and wiki. Later, we have developed machine learning models to predict the at-risk students. All the features which have been extracted and derived were analyzed statistically and the importance of each of the features is assessed using Random Forest. We consider the data of a student which is obtained continuously at all stages of his course and analyze the timestamps from the event log to predict his engagement in the course. Initially, four baseline algorithms have been used in our experiments. The algorithms are: Decision Tree – C5.0, Naïve Bayes, Logistic Regression and Support Vector Machines. In the second phase of the experiments, we have used two ensemble learning algorithms - Random Forest and Gradient Boosting. In the third phase of the experiments, we have implanted a new stacking ensemble using C5.0, Naïve Bayes, Logistic Regression, Gradient Boosting and Stacked Ensemble. We choose “AUC”, Area Under the ROC Receiver Operating Characteristic Curve (ROC), as the evaluation criteria through our experiments.
- **Dataset:** It has a total number of 13.5 Million logged events. The data set is unbalanced data set, which contains information of 79.29% dropouts and 20.71% of the students who completed the course. The data consist of four parts: (a) Course Data: It contains information about course and its modules; (b) Logged Events: It contains information about various online events of learners. For example, posting a question on the forum is an event; (c) Enrollment Data: It contains information about student’s enrollment to a course; (d) Status: It contains information about course completion. If a student completes a course then he will be labeled as “0” otherwise he will be labeled as “1”;
- **Metrics:** We have derived a total of 16 features after detailed analysis of the entire data. (a) Number of events in the last week; (b) Number of days between the end of the course and the last day of access of the course material; (c) Number of accesses in the last two weeks; (d) Total number of events; (e) The number of unique days accessed; (f) Total number of accesses till the prediction time; (g) Total number of accesses in the last week; (h) Total number of page navigations; (i) Total number of page closes; (j) Total number of problems solved; (k) Total number of videos watched; (l) From the start date of the course, after how many days a student accessed course content; (m) Median of access days; (n) Median of access days, after the first submission of a problem solution/quiz; (o) Total number of discussions on forum; (p) Total number of wiki views;
- **Case study:** In edX data of approximately 200.000 students from 39 courses.
- **Results:** If the number of events of a student is less than 50 then the probability of dropping out is more than 0.75. If the students have more than 500 logged events then the probability of dropping out of the course is close to zero. The number of events is an important feature to predict whether a particular student will discontinue from a course or not. The top five important features are: (1) Number of events in the last week; (2) Number of days between the end of the course and the last day of access of the course material; (3) Number of accesses in the last two weeks; (4) Total number of events; (5) The number of unique days accessed; Interestingly, except C5.0, the AUC of all the baseline models fall in the range of 0.85 to 0.87. It is observed that AUC is very low in case of C5.0 (0.75). Stacked ensemble has given the best performance among all of the prediction models. It has got an AUC score of 0.912, which is 3.6% better than the AUC of Gradient Boosting, which is the best model among all the baseline models. However, the stacked ensemble model took more turnaround time when compared to other models to make predictions for 24120 students.
- **Notes:** It is not possible to formulate a common performance tuning criteria to all the aforementioned models to obtain accurate predictions by controlling the related parameters. Each model has its own advantages and disadvantages.

#### **How Learners’ Interactions Sustain Engagement: A MOOC Case Study** - [10.1109/TLT.2016.2633268](http://dx.doi.org/10.1109/TLT.2016.2633268)
- **Source:** IEEE Transactions on Learning Technologies
- **Problem:** - How frequently do learners follow someone during a course? - Do learners who follow someone make a greater number of active contribution to discussion forum? - Do learners who follow someone in a course complete the course? - How can we characterise the differences between completion rates comparing follow and discussion contribution behaviours?
- **Objective:** Follow is one of the distinctive FutureLearn features that allows people to follow each other and directly access the contributions of those they follow on discussion boards. This study investigated: - behaviours of learners who follow someone from their course; - the pattern of follower learners’ participation in the discussion forum; - follower learners’ completion of the course;
- **Methodology:** We examined those data associated with the instance of the Developing Your Research Project (DYRP) course selected for this study. We examine the contributions to discussions in relation to whether the participants chose to follow others and examine course completion success amongst the followers.
- **Dataset:** Tracking around 1.2 million following relationships in the platform.
- **Metrics:** The source data which we analysed was a subset drawn from the standard datasets: (1) End of Course Stats: Overall participation rates in a MOOC, i.e., number of those enrolled in the course and those who left the course; (2) Enrolments: enrolment records of participant (learner_id, enrolled_at, unenrolled_at); (3) Step Activity: Number of steps completed by learners, i.e., those checked the “completed” mark (learner_id, step, week_number, step_number, first_visited_at, last_completed_at); (4) Comments: Records on the forum activities. This dataset identifies who posted: whether it was a reply, post timestamp, content and number of likes received. (id, author_id, parent_id,step_ text, timestamp, likes); (5) Followings: Records on follow relationships amongst participants (followed_user_id, follower_user_role, follower_user_id, follower_user_role, created_at); Learners are allocated across five categories, which are: (i) learners who follow (aka follower); (ii) followers who contribute to the discussions; (iii) followers who do not contribute to discussions (aka lurker); (iv) learners who contribute to discussions by posting (aka poster); (v) posters who do not follow; Learners are categorised by three distinct behaviours: (i) those followers who post to discussions; (ii) those followers who do not post to discussions; (iii) those posters who do not follow;
- **Case study:** FutureLearn has its own design prompting online discussions. An associated discussion board, designed as Twitter-like threads which enable the learners to scroll down and read sequentially comments. Learners are able to follow other participants in the platform by using the follow button. The datasets provided by FutureLearn are a snapshot of the participants’ activities observed from the 15th September-22nd November 2014. FutureLearn also provided a static followings dataset upon our request. This dataset contained follow interactions amongst participants between the first day of FutureLearn and 2015-09-16 09:45:43 UTC). The DYRP MOOC is composed of 80 steps spread across eight separate weeks. Learners who completed at least 50 percent of the steps in aweek are considered as a completer of theweek; otherwise, the learner is named as a non-completer of the week.
- **Results:** 9,855 learners enrolled, 5,086 (51.6 percent) participants actually visited the course pages after the course started. Of these, 3,852 (39 percent) completed at least one step and 2,631 (26.7 percent) revisited the course and completed further steps. In total 1,867 (18.9 percent) online discussions by writing at least one comment. 541 Learners who completed at least 50% of steps. The majority (70 percent) of those participants who followed at least one other person contributed to the discussions. At the same time there was a small number of participants who commented extensively but who did not follow any other participants. Every learner who posted to a discussion thread completed at least one step in the course. If a learner is socially passive, it is likely that they will complete none of the steps, i.e., over 40 percent of socially passive followers did not complete any of the steps. The proportion of course completers is high if learners are socially active. Moreover, a larger proportion of course completers (41 percent) is observed amongst the learners who follow and post. The learners who either post or follow make up a similar percentage, slightly over 30 percent. Learners in each of the categories, regardless of whether or not they are an overall completer, progressed through the individual weekly steps at different rates; Fairly high weekly completion rates (from 60 percent up to 95 percent) are observed for all learners in each category throughout the course; The only exception is in the last week where the average fell to slightly over 30 percent; Followers who contributed to discussion threads completed the highest number of steps and represent the largest proportion of overall completers; Posters who did not follow anyone completed more of steps than the followers who were socially passive in discussions; Followers who did not contribute to the discussion forum performed better than the course average in completed steps, implying that follow behaviours of learners could be used as an indicator for predicting their course completion. (i) Course Completers: The social activeness of the course completers were sustained until Week 6. After Week 6, they showed limited activity. They hardly posted or followed other participants or completed the week. Full participation based on three behaviours (post, follow, step completion)
was most prevalent in Week 1; (ii) Course Non-Completers. They have also been the most active in Week 1. Their level of activity and weekly completion declined sharply in Weeks 2 and 3, i.e., this is much earlier than course completers. Although no activity was observed in common especially after Week 3, it is still seen that a few of the non-completers kept on following someone or contributing to the discussions or very rarely completing the weeks. It appears that their behaviours are in accordance with the behaviours of lurkers in general discussion forums; According to our findings, only a small fraction of learners attempt to use the follow feature provided by the platform. Not all forum participants use the follow feature. 70 percent of forum participant do not follow anyone: (1) Lack of awareness: Course providers usually advertise the follow feature in the introduction page but do not promote it throughout the course. Therefore, some learners may possibly not be aware of the follow function; (2) Usability of follow: After a learner starts following someone and interacts with them, the learner must manually control updates; (3) Confusion of learners: Learners may not be able to decide who to follow. Learners may need help in finding like-minded study partners or conversations to initiate supportive interactions in MOOCs. It is important, therefore, to link learners to the right part-ners and to the right information in the discussion forums.
- **Notes:** In contrast of the objective, the current paper also show tables that summarise available state-of-the-art techniques considering their objectives and their prediction methods. Of these tables, one summarises the dropout definitions identified in the literature and their notable findings. There is no formal dropout definition, each study implements their own experiments using a variety of definitions. Two most widely-used definitions for dropout are: (1) Not completed the final week: If a learner does not engage in the final week’s activities, they are assumed that they dropped out of the course. Asimilar assumption, proposed by some researchers, is that learners are marked as dropped out if they did not submit the final assignments; (2) No activity during the most recent week or No further activities in the following weeks: This definition differs from the previous in terms of the timing of the drop-out. In order to observe learners’ behaviour. Typically four types of dataset are available: (i)pre- and post-course surveys; (ii) clickstream; (iii) the results of assignments; iv) activities in discussion forums. They have analysed the relationships between learners’ behaviours in MOOCs and their course completion rates to: identify possible reasons for low retention rates; provide necessary help to learners; predict learners’ future behaviours before they initiate.

#### **Towards Actionable Learning Analytics Using Dispositions** - [10.1109/TLT.2017.2662679](http://dx.doi.org/10.1109/TLT.2017.2662679)
- **Source:** IEEE Transactions on Learning Technologies
- **Problem:** Given the rigidity of student information systems (SIS) and LMS data, educators may encounter difficulties in designing pedagogically informed interventions. (Q1) To what extent can academic performance be predicted by online activity trace data? (Q2) To what extent can the use of the e-tutorials be predicted by SIS and disposition data obtained by self-reports? (Q3) Can we profile students into clusters based on their online activities?
- **Objective:** This study attempts to examine the underlying characteristics behind students’ activities on LMS by incorporating dispositional dimensions through self-reported surveys into conventional LA models. The ultimate aim of this research is to describe patterns of learning behavior (such as successful versus non-successful ones) not only in terms of trace variables (such as students who practice a lot, or not frequently, Q1), but also in terms of students’ dispositions (such as: students poor regulation of their learning, Q2, Q3).
- **Methodology:** We will investigate the relationships between four different course performance measures, seven LMS system trace variables, five SIS based variables, and 53 different learning disposition variables measured in eight self-report surveys. Our study applies self-report survey data and trace data through the logging of actual study behaviors and the specific choices students make in the e-tutorials. The data analysis steps of this study are all based on linear, multivariate models, making use of hierarchical regression analysis. We focused on the most stable predictor of the performance variables: the total number of clicks. We also cluster students based on K-Means cluster analysis to investigate characteristic differences between the several clusters in terms of performance and dispositional variables. The seven LMS trace data, three MML, three MSL trace variables, and the BlackBoard variable, are used as inputs for a K-means Cluster analysis. A 4-cluster solution is preferred based on both model fit, interpretability of cluster solutions, and avoiding clusters smaller than 40 students.
- **Dataset:**
- **Metrics:** Attitudes towards learning of mathematics were assessed with the SATS instrument: - Affect: students’ feelings concerning mathematics; - CognComp: students’ self-perceptions of their intellectual knowledge and skillswhen applied to mathematics; - Value: students attitudes about the usefulness, relevance, and worth of mathematics in personal and professional life; - NoDifficulty: students’ perceptions that mathematics as a subject is not difficult to learn; - Interest: students’ level of individual interest in mathematics; - Effort: the amount of work the student is willing to undertake to learn mathematics; - RiskTaking: how strong risk seeking and how less risk avoidant students are; - Procrastination: the tendency to avoid doing learning activities; Learning processing and regulation strategies, shaping self-regulated learning, are based on the Inventory of Learning Styles (ILS) instrument: - Deep processing strategy, in which students relate, structure and critically process new knowledge they learn; - Stepwise processing strategy, based on memorizing, rehearsing and analyzing; - Concrete processing strategy, focusing on making new knowledge concrete, applying it; - Self-Regulation of learning processes and learning content; - External Regulation of learning processes and learning results; - Lack of Regulation: the absence of regulation from student or from the environment;
- **Case study:** Three different digital systems have been used to organize the learning of students and to facilitate the creation of individual learning paths: BlackBoard and the two e-tutorials MML and MSL. The MML and MSL-environments take over the monitoring function: at any time, students can see their performance in the practice sessions, their progress in preparing for the next quiz, and detailed feedback on their completed quizzes, all in the absolute and relative (to their peers) sense. Participants in our study are a large and diversified class year of students in a blended introductory course in statistics and mathematics, completing a range of self-reports as a required course activity, using an educational system best described as ‘blended’ or ‘hybrid’, in a business and economics university program in the Netherlands. The main component is face-to-face: Problem-Based Learning (PBL), in small groups (14 students), coached by a content expert tutor. Quizzes are taken every two weeks and consist of items that are drawn from the same item pools applied in the practicing mode. We chose this particular constellation as it stimulates students with limited prior knowledge to make intensive use of the MyLab platforms. The bonus is maximized to 20 percent of what one can score in the exam. The subject of this study is the 2014/2015 cohort of first-year students, who in some way participated in learning activities (i.e., have been active in BlackBoard: in total, 1,069 students). In the investigated course, students work an average 30 hours inMMLand 22 hours in MSL, 25 percent to 40 percent of the available time of 80 hours for learning in both topics.
- **Results:** Quiz performance is slightly better predicted than exam performance. International students have a disadvantage over domestic students due to the strong focus on statistics in the Dutch high school system; no difference with regard to mathematics. Students from the advanced mathematics track in high school do better in mathematics, but not in statistics. Overall, students who try more attempts, who see more examples, perform better. The outcome that the intensity of using Black-Board predicts performance in mathematics, but not statistics, is explained by the circumstance that links to instructional videos for mathematical topics were provided in the BlackBoard environment, while links to videos related to statistics content were provided in the MSL environment, outside BlackBoard. International students, as well as female students, appear to be the most active students in the digital tools, resulting in higher mastery levels. One attitude, planned Effort in learning mathematics, has a consistent positive impact, whereas the Procrastination has a consistent negative impact. The very general tendency to RiskTaking behavior shows up in the learning process: students take the risk of being less well prepared. The attitudinal variables Affect, CognitiveCompetence, and experiencing NoDifficulty in learning are positively related to mastery level, and negatively related to using examples, but do not enter the multivariate prediction model. Deep learners, students with the most adaptive cognitive processing strategy, focus more on the PBL-mode (Problem-Based Learning) of the learning blend, and less on the digital mode using the e-tutorial. They use BlackBoard, primarily for the PBL-mode, more often, and MML less often; Stepwise processing, the most maladaptive strategy has all non-significant betas. Female students in our sample score higher than male students in Step-wise processing and International since international students in our sample score higher than Dutch students in Stepwise processing); the Concrete processing strategy: not correlated with MML trace variables in bivariate relationships, negatively impacting MML variables in a multivariate context, again because of gender and nationality effects in concrete processing; External regulation of learning correlates strongly with the intensity of using MML. The clusters being determined only by student activity in the learning environments, exhibit a clear structure: the higher the cluster number, the lower the LMS activity levels of the students shaping the cluster. With one exception: Cluster3 students use examples more intensively, relative to overall activity levels. The smallest cluster, Cluster1, consists of 41 mostly international students with a relatively large number of students having a non-advanced mathematics track in high school. To compensate, these students are very active in the MML and MSL digital environments, with more than five times as many attempts, for both topics, as students from Clusters4, with lowest activity levels. Mastery levels inMMLand MSL of Cluster4 students do strongly deviate from those of other clusters: proficiency of students in this cluster is at around 40 percent, in contrast to the 90 percent or higher proficiency levels in the three other clusters. In Cluster4, male students and national students are overrepresented. Cluster2 and Cluster3 are more centrally located in between the extreme cluster solutions. They differ from each other, mainly for the number of attempts in the tools (higher in Cluster2) and the number of worked examples called for (higher in Cluster3). Cluster4 students, those who strongly stay behind in using the two e-tutorial systems, are characterized by high levels of Procrastination. Cluster4 students score low on all adaptive dispositions learning Effort, Mastery-Approach, Persistence, and Planning. In contrast, Cluster1 students score low on Procrastination, and high on all adaptive dispositions learning Effort, Mastery-Approach, Persistence, and Planning. The two centrally located clusters Cluster2 and Cluster3 do not differ a lot on dispositions, with one exception: Cluster3 students score higher on Procrastination, in part explaining their lower scores on tool attempts and higher scores on viewing examples.
- **Notes:** Our study addresses the existing lack of empirical work exploring the connection between self-reported data and student activity trace data, and by doing so, may represent a little step in the undertaking to merge both approaches to measure learning.

## 2018

### ACM
#### **The Learner's Engagement in the Learning Process Designed Based on the Experiential Learning Theory in Post Graduate Program at Open University Malaysia** - [10.1145/3291078.3291079](http://dx.doi.org/10.1145/3291078.3291079)
- **Source:** Proceedings of the 2018 2nd International Conference on Education and E-Learning - ICEEL 2018
- **Problem:** Engagement in learning process did not merely limit to the teacher and the learner but also to the community. The establishment of a connection between the three parties is considered as necessary in enriching the knowledge of the learner. This study will address three research questions as below: (Q1) What is the difference between a level of engagement per hours in a week between the grasp experience (GE) and transform experience (TE); (Q2) What is the level of engagement between the learners in ‘grasp experience’ (GE) and transform experience (TE) in the learning process; (Q3) To what extent does the level of experience in grasp experience contribute to the transformation of new experience in transform experience?
- **Objective:** This study aims to observe the learner's engagement in the learning process designed based on four modes of the Experiential Learning Theory (ELT) model. The purpose of this study was to: (1) observe the level of engagement in a learning process in ‘grasp experience’ and ‘transform experience’ section; (2) evaluate the level of engagement in an online forum between ‘grasp experience’ and ‘transform experience’ section. (3) clarify the level of engagement through the posting in an online forum, which reflects the actual ‘experience.’
- **Methodology:** The implementation of Experiential Learning Theory (ELT) in OUM is regarded as useful where it can benefit the intended audience in creating a meaningful learning outcome that can be applied in their jobs or organisations. The learning cycle starts with the Concrete Experience (CE) followed by Reflective Observation (RO), Abstract Conceptualisation (AC) and Active Experimentation (AE). The four cycle is based on two types of experiences; grasp experience and transform the experience. In the course the students are required to complete four main activities: (1) Share, in the online forums, at least one of the theories you favour the most that you will use to incorporate in your own learning situation. (Grasp Experience); (2) Write a reflective paper describing the key personalities and their respective learning theories. (Grasp Experience); (3) Explore and identify a free MOOC (Massive Open Online Course) or a mobile learning initiative, in terms of how the learning theories are being applied and to observe their effects on learning. (Transform Experience); (4) Select one for a case study. Report your observations by first describing the selected MOOC or mobile learning initiative (Transform Experience); The data use to measure the level of engagement is based on the learning analytic in Moodle. The report from Moodle so-called my INSPIRE is extracted from the Course Participation Report.
- **Dataset:**
- **Metrics:** The level of occurrence for the learner’s engagement rely on the amount of quality posting between the tutors and the learners. The online forum is expected to intensify the student satisfaction and motivation to learn if the level of participation in the activity is high.
- **Case study:** In the case of this study, the learner gathers and build the knowledge based on their experience (grasp experience) then reflect by sharing ideas and constructive comments to produce new ideas or possibilities in solving complex problems. Most of the event is done in an online forum. This study focuses on user’s research. Six graduates from Masters of Instructional Design and Technology involves in this study. Most of them are working adults undergone the fully online learning process in the the course for 14 weeks.
- **Results:** (Q1) Most of the students seem to be active in an online forum in GE. The findings show that there is a significant declination on hours of engagement in the forum between the Grasp Experience (GE) session and the Transform Experience (TE) session; (Q2) Three of the students shows significant drop on hours of engagement between GE and TE; (Q3) There is a significant drop in numbers of hours in engagement in online activities in the Transform Experience (TE). Comparative data between the grasp experience (GE) and transform experience (TE) section reflect the opportunities for the learners adjust the learning experience. In GE session student tends to spread their learning experiences over a more extended period in an online forum by gathering information and shared it with their peers. In TE session, the students tend to ignore the online activities and focus more on reflecting the ideas in writing a case study.
- **Notes:**

#### **A supervised learning framework for learning management systems** - [10.1145/3279996.3280014](http://dx.doi.org/10.1145/3279996.3280014)
- **Source:** Proceedings of the First International Conference on Data Science, E-learning and Information Systems - DATA '18
- **Problem:** Analyse Student performance.
- **Objective:** Presenting software framework to provide a solid foundation for the development of EDM and LA prediction models. To develop a model for predicting students at risk that can be readily used within the framework.
- **Methodology:** We separate the framework into two layers: - TheMoodle Analytics API written in PHP is responsible for generating labelled and unlabelled CSV files fromMoodle’s database contents; - Machine Learning backends are responsible for processing these files. These backends can be written in any programming language; The PHP Machine Learning backend uses both  a Logistic Regression binary classifier to perform its predictions, and Artificial Neural Networks (ANN). Researchers can specify which predictions returned by the Machine Learning backend are worth observing and which predictions can be ignored. E.g. A model that predicts students at risk is only interested in at-risk students, not in students that progress as expected.
- **Dataset:**
- **Metrics:**
- **Case study:** We report one of the implemented prediction models of the Supervised Learning framework. This prediction model classifies students without activity logs during the last quarter of a course as drop-outs and all other students as not-drop-outs. We implemented a Time splitting method to generate 3 predictions along the course duration. The first one is executed once the first quarter of the course is over, using data from the start of the course. The second one is executed after half course is completed and use activity data from the beginning of the course up to that point. The third prediction is executed after the third quarter of the course, also using all the data available from the start of the course up to that point in time. To test the prediction model we used 8 finished anonymised MOOCs with a total of 46,895 students.
- **Results:** Our test results show that the proposed at-risk students model gave an average prediction accuracy of 92.56% using the Neural Network and an average prediction accu- racy of 73.30% using the Logistic Regression classificator.
- **Notes:** The Supervised Learning framework presented in this paper is part of the Moodle core since its 3.4.0 version.

#### **On the Need for Fine-Grained Analysis of Gender Versus Commenting Behaviour in MOOCs** - [10.1145/3234825.3234833](http://dx.doi.org/10.1145/3234825.3234833)
- **Source:** Proceedings of the 2018 The 3rd International Conference on Information and Education Innovations - ICIEI 2018
- **Problem:** Stereotyping has been criticised as being too simplistic, and then, again, applied, due to its simplicity.
- **Objective:** Our main purpose with this research is to predict the learner overall and fine-grain behaviour based on learner characteristics. In this paper, we specifically focus on the gender stereotype, and its relation to the way learners comment in a MOOC.
- **Methodology:** This paper focuses on comments of female and male learners. To understand gender differences, we looked at the total number of comments posted by women. To obtain fine-grained, temporal results, we had to analyse comments on a weekly basis, the number of comments per week.
- **Dataset:**
- **Metrics:** Number of comments by learner per week
- **Case study:** We base our study on a truly massive FutureLearn course collection of 7 courses delivered via 27 runs between 2012-2016. The current study is analysing data extracted from 27 runs of 7 MOOCs courses, on 4 main topics: literature (Literature and Mental Health (LT): 6 Weeks), Shakespeare and his world (SP): 10 Weeks; psychology (The mind is flat (MF): 6 Weeks), Babies in mind (BIM): 4 Weeks; computer science (Big Data (BD): 9 Weeks), and business (Leadership (LS): 6 weeks and Supply chains (SC): 6 Weeks) delivered through FutureLearn, by the University of Warwick. The study covers 19425 female and 6648 male enrolled learners, out of which 11473 female and 3802 male learners have accessed the course material at least once, and out of which 6240 females and 1833 males have commented at least once. The material overall has a total number of 2590 steps.
- **Results:** Expectations in terms of volume of comments coming from female or male learners clearly vary thus with the topic of the course. Therefore, whilst global statements across courses should best be avoided, it is useful to see how students react to a specific course, and then plan for future runs, accordingly. Our analysis shows that, overall, whilst the participation of females is clearly larger in terms of absolute numbers in the relatively varied MOOC courses we have analysed, in terms of comments produced by the two genders, the topic of the course, the course itself, and often, the week of the course determines which of the genders is commenting more often.
- **Notes:**

#### **IntelliEye: Enhancing MOOC Learners' Video Watching Experience through Real-Time Attention Tracking** - [10.1145/3209542.3209547](http://dx.doi.org/10.1145/3209542.3209547)
- **Source:** Proceedings of the 29th on Hypertext and Social Media - HT '18
- **Problem:** Most MOOCs today revolve around a large number of videos and automatically graded quizzes and little else. This setup requires learners to be skilled in self-regulated learning. Many learners lack such skills and as a consequence do not succeed. How exactly can we detect learners’ loss of focus in real-time and at scale? How can we alert the learner to her loss of focus? The research questions are: (Q1) To what extent is MOOC learners’ hardware capable to enable the usage of technologically advanced widgets such as IntelliEye? (Q2) To what extent do MOOC learners accept technology that is designed to aid their learning but at the same time is likely to be perceived as privacy-invading (even though it is not)? Are certain types of MOOC learners (e.g. young learners, or highly educated ones) more likely to accept this technology than others? (Q3) What impact does IntelliEye have on learners’ behaviours and actions? To what extent does IntelliEye affect learners’ video watching behaviour?
- **Objective:** In this paper we present IntelliEye, a system we designed to directly tackle the “loss of focus” issue during MOOC lecture video watching by detecting it in real-time and alerting the learner to it.
- **Methodology:** IntelliEye employs the Webcam feed to observe learners’ activities during their time on the MOOC platform and intervenes if it detects a loss of focus.  In our work we make use of commonly available Webcams and deploy IntelliEye “in the wild”, to 2,612 MOOC learners in an actual MOOC. The goal of IntelliEye is to provide real-time feedback on learner’s attention, and is based on a set of heuristics reliably implementable on a wide variety of hardware setups: (1) if the browser tab/window containing the lecture video is not visible to the learner, IntelliEye triggers an inattention event; (2) we assume a learner is inattentive if her face cannot be detected for a period of time, i.e. we employ face tracking as a robust proxy of attention tracking; (3) if the face tracking module detects a loss of the face we consider the mouse movements as a safety check: if no face is detected but the mouse is being moved, no event is triggered. The learner has four choices: (i) to enable IntelliEye for this particular video only, (ii) to disable IntelliEye for this video only, (iii) to enable IntelliEye for all videos, and, (iv) to disable IntelliEye for all videos. If a learner opts for (iv), we ask the her for feedback on the decision (“You have disabled IntelliEye. Please tell us why.").
- **Dataset:**
- **Metrics:**
- **Case study:** We implemented IntelliEye in JavaScript, as the edX platform allows custom JavaScript to be embedded in course modules. We deployed IntelliEye in the MOOC Introduction to Aeronautical Engineering (AE1110x) offered by TU Delft on the edX platform. The MOOC requires around 80-90 hours ofwork and consists of 104 videos and 332 automatically graded summative assessment questions. The MOOC was opened for enrollment on May 1, 2017 and remained so until March 31, 2018. A total of 2, 612 different learners visited the MOOC during the deployment period and were exposed to IntelliEye. According to their edX profiles, hail from 138 different countries have a supported device setup.
- **Results:** We find that most learners (78%) use hardware and software setups which are capable to support such widgets, making the wide-spread adoption of our approach realistic from a technological point of view; The majority of learners (67%) with capable setups is reluctant to allow the use of Webcam-based attention tracking techniques, citing as main reasons privacy concerns and the lack ofperceived usefulness of such a tool; Among the learners using IntelliEye we observe (i) high levels of inattention (on average one inattention episode occurs every 36 seconds—a significantly higher rate than reported in previous lab studies) and (ii) an adaptation of learners’ behaviour towards the technology (learners in conditions that disturb the learner when inattention occurs exhibit fewer inattention episodes than learners in a condition that provides less disturbance); Learners learnt to adapt their behaviour as needed: learners in the pausing/auditory conditions had significantly fewer inattention events than learners in the non-disruptive visual alert condition.
- **Notes:**

#### **How do Gender, Learning Goals, and Forum Participation Predict Persistence in a Computer Science MOOC?** - [10.1145/3152892](http://dx.doi.org/10.1145/3152892)
- **Source:** ACM Transactions on Computing Education
- **Problem:** Do men and women choose to enroll in a CS MOOC for similar or for different reasons? And, do the reasons students enroll in a CS MOOC relate to how they participate in the forums? Do men and women participate differentially in the forums? And does forum participation relate to course persistence? Does course persistence differ for men and women?
- **Objective:** In general, this investigation seeks to understand (a) if reasons for enrolling in a CS MOOC predict different types of participation in forums for both men and women and (b) whether type ofreasons for enrolling and type offorum participation work together to predict course outcomes.
- **Methodology:** The data analyzed in this study include (1) a survey collected at the beginning of the Android MOOC course, Creative, Serious, and Playful Science of Android Apps, and (2) records of student interaction with the course, including visiting or posting to the forum; To prepare the text for analyses, we undertook several pre-processing tasks using the “tm” package in R. For these preliminary analyses, we used unigrams and a bag-of-words ap- proach to glean insight into these texts. Before analysis, we removed punctuation and numerals, and changed the case of all words to lowercase. Furthermore, we removed a list of English stop words provided with the “tm” package. To select the final terms to be used in k−means clustering, we employed Luhn’s theory of establishing relevant sentences, but with only the unigrams, i.e., individual words. Once all ofthese pre-processing steps and selection strategies were utilized, 139 terms were used to group responses into clusters. Ofthe 139 terms, examples include career, code, comput, creativ, curious, degre, design, educ, expand, hobbi, opportun, play, softwar, student,and write, among others. We considered three types of forum participation, ordinally: no views or posts in the forums, viewing the forum (but not posting to the forum), and viewing and posting in the forum. We looked at the effects of reasons for enrolling to uncover how reasons for enrolling in the MOOC might predict forum participation. To do this, we began by estimating successive ordinal logistic regression models.
- **Dataset:**
- **Metrics:**
- **Case study:** The course used a week-based activity schedule, during a 12-week time period. This course introduced fundamental computer science principles that power apps. Students learned to create their own Android apps using Java and standard software development tools. Forum participation was never graded nor required; however, students were strongly encour- aged to use the forums. Participants were asked demographic questions and their expectations for the course. The survey was optional, where n = 5,807 participants who provided information about their gender, listed a reason for taking the course, and had data available about forum participa- tion; before removing those who did not have this information, the number of participants was N = 146,970.
- **Results:** We used the text that students provided for open-ended responses and used k-means clustering, where the number of centroids was five, as this solution was the most interpretable after experimenting with the number of centroids. (1) Computer Science Student Cluster. We identified a group of students in the course who were interested in being students of computer science, were formerly computer science students, or had a more clearly defined interest in the academic field of computer science; (2) Understanding and Learning Cluster. Some participants in this MOOC identified an interest in creating an understanding of Android app development and an understanding the machinery be-hind applications they use; (3) Programming is Cool Cluster. Some participants identified reasons that indicated their general fascination with programming; (4) Career and Entrepreneurial Activities Cluster. Finally, a portion of the participants in this MOOC identified being interested in career development or using app development in an entrepreneurial way; (5) “Other” Cluster. The final cluster contained the vast majority of students enrolled in the course; In regarding to gender’s relationship with participating in the forums, the data indicate that the proportion of women who post compared to those who only view the forum, and the proportion who view compared to those who do not view the forum, was different for men versus women. Namely, the modal response for men was different: men had a higher proportion of viewing forum posts than women, and women had a higher proportion of posting to the forum or not viewing the forum. In regarding to gender’s and reasons-for-enrolling’s combined relation to predict participating in the forums, we found a dependent relationship between student gender and stated reasons for enrolling in this MOOC, we conducted ordinal logistic regression separately for men and women. (1) Women: the only statistically significant covariate to predict forum participation was when they indicated they took the course due to being a student and wanting to learn (i.e., being identified in the computer-science-student cluster); (2) Men: the only statistically significant covariate to predict forum participation was when men indicated they took the course due to career aspirations, i.e., were in the career-and-entrepreneurial-activities cluster; The major results of this investigation suggest that men and women behave differently in the CS MOOC we examined. Men and women participated in the forums differently, and this was related to their reasons for taking their course as well as their gender: the men who actually visited the forums were less likely to post than the women; moreover, men who offered that they were taking the course to fulfill career aspirations were more likely than other men to participate more substantially in the forums. Women who offered that they were taking the course because they were a CS student were more likely than other women to participate more substantially in the forums. In general, women participate less in the field of CS and less in CS MOOCs than men. Our findings that women posted to the forum at higher proportions than men and that women who posted to the forum had higher odds ofpersisting than women who did not post to the forum (which was true for men as well) provides potential for optimism.
- **Notes:**

#### **Exploring causes for the dropout on massive open online courses** - [10.1145/3210713.3210727](http://dx.doi.org/10.1145/3210713.3210727)
- **Source:** Proceedings of ACM Turing Celebration Conference - China on - TURC '18
- **Problem:** Predict student performance
- **Objective:** We used a new gradient boosting decision tree algorithm to explore which features or characteristics of learner behavior data influence the learning performance.
- **Methodology:** To represent learners as complete as possible, the features are generated from different aspects, such as the efforts, learning regularity, timeliness, learning order and all kinds of behavior preference. 56 different features are extracted and grouped into 9 categories: (1) The efforts made on the course, such as completion rate, accumulative online time, counts and average frequency of logins and clicks; (2) The regularity of login, such as fluctuation and dispersion of logins and login intervals, skewness and kurtosis of logins; (3) Whether to study in time, such as time delay of learning, average login intervals; (4) Whether to learn in the order of teaching syllabus, such as skipping rate; (5) About the video behaviors; (6) About the quiz and homework behaviors; (7) About the post, reply, comment and reading behaviors in forum; (8) About mutual evaluate behaviors; (9) The Register information, like age; Generally, the more important the feature is, the greater impact of the learning performance it gives. We use Tree Ensemble models to evaluate the importance of features. Tree Ensemble Model use multiple Decision Trees to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. Repeated K-fold Cross-Validation is applied during each iteration of modelling to overcome the problem of overfitting by randomly partitioning the original sample of data into folds based on resampling. Dismantling the best model, the contribution of the 56 features are calculated by averaging the results of weight, gain and cover, and converted into percentage score. Then, weremoved 20 minimal contributions and retrained the model and found that the classification performance is almost not affected. After that, the number of features is reduced in turn according to the score range and the classification effect of the model is observed. Obviously, removing unimportant features have almost no negative impact on the model performance, which in turn, get improved, especially on the model trained by only 9 features.
- **Dataset:** More than 1,600,000 click records and 150,000 table records are collected, and the learning behavior tracks of the learners can be obtained from the logs.
- **Metrics:**
- **Case study:** The original data studied in this paper are obtained from icourse163 platform, including user behavior log and the table records, such as the organization of the course, the syllabus, the overview of assignments (forum, exam) and user’s background.
- **Results:** The top 9 features are supposed to be the main causes of dropout: (1) Intervals_Amplitude - Difference between one's Maximum and Minimum of login intervals; (2) Login_Skewness - Skewness of one's logins. It could reflect whether the learner is strict at the beginning and slack at the end; (3) Start_Learn_Delay - Time from enroll to first learning; (4) Intervals_CV - Coefficient of Variation of one's login intervals; (5) Durations_CV - Coefficient of Variation of one's login durations; (6) Rewatch_Rate - The proportion of replay behaviors among all his/her video behaviors; (7) Accumulation - Accumulation time spend on this course term;
(8) Median_Intervals - Median of one's login intervals; (9) Login_Frequency - Average frequency of one’s logins;
- **Notes:**

#### **The half-life of MOOC knowledge: a randomized trial evaluating knowledge retention and retrieval practice in MOOCs** - [10.1145/3170358.3170383](http://dx.doi.org/10.1145/3170358.3170383)
- **Source:** Proceedings of the 8th International Conference on Learning Analytics and Knowledge - LAK '18
- **Problem:** The vast majority of MOOC learners are not disciplined, self-directed autodidacts who engage in such effective learning behavior without additional support. (Q1) How does an adaptive retrieval practice intervention affect learners’ academic achievement, course engagement, and self-regulation compared to generic recommendations of effective study strategies? (Q2) How does a push-based retrieval practice intervention (requiring learners to act) change learners’ retrieval practice behavior? (Q3) To what extent is robust learning facilitated in a MOOC?
- **Objective:** In this paper we explore to what extent the testing effect holds in one of today’s most popular digital learning settings: Massive Open Online Courses (MOOCs). Presenting Adaptive Retrieval Practice System (ARPS), a tool that encourages retrieval practice by automatically and intelligently delivering quiz questions from previously studied course units to learners.
- **Methodology:** The Adaptive Retrieval Practice System (ARPS) is a client- server application (written in JavaScript/node.js) that provides automated, scalable and personalized retrieval practice questions to MOOC learners on a continuous basis. We developed ARPS specifically for use within the edX platform in taking advantage of the RAWHTML input affordance. We push questions to learners instead of requiring the learner to seek the questions out. We adopted this push-based design in order to allow learners to readily engage with the intervention with minimal interruption to the course experience. ARPS is seamlessly integrated in the course, requiring as few additional interactions as possible. In the case of Multiple Choice (MC) questions the entire interaction requires just a single click: the learner selects their chosen response and if correct, receives positive feedback. Incorrect responses invoke negative feedback which disappears after 4 seconds and returns the learner to the original question so they can try the problem again.
- **Dataset:**
- **Metrics:** In order to measure and compare the behavior of learners in both the control and treatment conditions, we consider the following measures of in-course events (tracked and logged on the edX platform): (1) Final grade (a score between 0 and 100); (2) Course completion (binary indicator: pass, no-pass); (3) Course activities: Video interactions (play, pause, fast-forward, rewind, scrub), Quiz submissions (number of submissions, correctness), Discussion forum posts, Duration of time in course; (4) ARPS interactions: Duration of total qCard appearance, Response submissions (with correctness), qCard interactions (respond, close window); The following data were collected in the post-course survey: (1) Course survey data: Post-Exam Quiz Score (between 0-10), Learner intentions (e.g., to complete or just audit), Prior education level (highest degree achieved).
- **Case study:** A total of 2,324 learners enrolled in the course titled Geoscience: the Earth and its Resources (or GeoscienceX), which was offered on the edX.org platform between May 23, 2017 and July 26, 2017. The course consists of 56 lecture videos and 217 graded quiz questions.
- **Results:** (Q1) Many learners who were encouraged did not actually engage in retrieval practice, which is a form of treatment noncompliance. Specifically, of the 523 learners assigned to the treatment, only 102 interacted at least once with a qCard (i.e. complied with the treatment). For this reason, in order to estimate the effect ofretrieval practice itself, we also analyze the experiment as an encouragement design. However, the per-protocol estimates do not have a causal interpre- tation because different subpopulations are compared: all learners in the control group versus those highly motivated learners who comply in the treatment group. For instance, we note that treatment compliance is strongly correlated with receiving a higher grade. (Q2) We hypothesized that learners who receive the treatment would exhibit increased self-regulatory behavior in terms of (i) revisiting previous course content such as lecture videos, (ii) self-monitoring by checking their personal progress page, and (iii) generally persisting longer in the course. No evidence in support of the hypothesized behavior was found. Focusing on learners in the treatment group, we note that learners who attempt qCards at a higher rate would learn more and score higher on regular course assessments. (Q3) In regard to Passing students, we observe a statistically significant decrease in performance between Weeks 2 and 6 (correct response rate dropping from 67% to 49% respectively). While the proportion of ignored responses remains steadily low, the proportion of correct responses drops by 18%.  The rate of incorrect responses increased from 4% to 25%. In regard to Non-Passing, we observe that the choice to ignore qCards was common through the entire course duration, with a slight increase in the later weeks. We also observe a significant decrease in correct response rates for non-passing learners. However, unlike passing learners who exhibited a significant increase in incorrect responses, there is no significant change for non-passing learners. The change, instead, is in the rate of ignored responses, increases from 47% in Week 2 to 61% in Week 6.
- **Notes:**

#### **A generalized classifier to identify online learning tool disengagement at scale** - [10.1145/3170358.3170370](http://dx.doi.org/10.1145/3170358.3170370)
- **Source:** Proceedings of the 8th International Conference on Learning Analytics and Knowledge - LAK '18
- **Problem:** Retention and degree completion is a major focus of many higher education institutions.
- **Objective:** Identify student disengagement.
- **Methodology:** This research used data from Connect by McGraw-Hill Education, a widely used online learning tool, to draw data from 4.5 million students in traditional college classes across 256 disciplines. Instructors use Connect in a variety of ways: to provide access to the ebook, assign practice or graded low-stakes assignments, or function as the primary course website where students complete all assignments and readings, including high-stakes assignments. The main purpose of Connect is delivering content and assessing students; it can be used for assigning homework and practice assignments, as well as quizzes and tests.  Student behavior data is collected as assignments are attempted. These data can be used to better understand disengagement in Connect. To train and test the disengagement classifier, four years of historical data from fall 2013 to spring 2017 was used. To avoid potential data issues related to small class size, low online learning tool usage, incorrect or unreliable assignment dates, or extremely compressed courses, several filtering steps were necessary to clean this his- torical data set. The sections in the data set were limited to those with a duration between 8 and 16 weeks. The data set included data from 4,691,768 students enrolled in 175,850 sections representing 256 academic disciplines. The research goal was to build a time-invariant classifier so that one classifier could be used to predict student disengagement from Connect at any time during the semester. Logistic regression was used because it is an easy algorithm to interpret, provides a floating point estimate between 0 and 1 instead of a hard classification label, and allows for the interpretation of student behavior or feature importance. Features were hand selected by the researchers to identify learner attributes that were different between engaged students and disengaged students. Features were used in the model if the distribution of values were significantly different between disengaged and engaged students. Ten features were identified and all permutations of this set were tested to ensure that inclusion of each feature improved the performance of the classifier. These ten features are: (1) Average grade (2) Average time spent on assignments relative to class average (3) Percent assignments submitted (4) Percent assignments submitted late (5) Percent assignments submitted on time (6) Percent assignments not submitted (7) Percent assignments submitted late or not at all (8) Days since last submission (9) Submission time relative to due date (10) Maximum number of consecutive late assignments. The classifier generates a score representing the likelihood of future disengagement from Connect. The logistic regression model used for the classifier was optimized for two hyperparameters, an elastic net blending parameter and a regularization parameter. The regularization parameter determines the strength of a penalty applied to high classifier weights and is a commonly used logistic regres- sion hyperparameter that helps prevent overfitting. To determine the best hyperparameter to use, a grid search was run over both hyperparameters using the validation data set and optimizing on the area under the precision-recall curve (AUC-PR). To evaluate the performance of the disengagement classifier, the area under the curve of the receiving operating characteristic curve (AUC-ROC) was calculated. This curve plots the true positive rate against the false positive rate and the area under this curve is a value with a standard interpretation in machine learning. Student behavior changes throughout a semester; therefore, it is important to evaluate the classifier as a semester progresses. To do so, the AUC-ROC was calculated separately for each week. Student behavior varies across courses and disciplines; therefore, it is important to evaluate how well the classifier generalizes within and across disciplines.
- **Dataset:**
- **Metrics:** To determine the best hyperparameter to use, a grid search was run over both hyperparameters using the validation data set and optimizing on the area under the precision-recall curve (AUC-PR). For pilot section analysis, two thresholds were identified which divided the classification score into three risk categories; low risk, moderate risk, and high risk. The low-moderate classification threshold was identified at 0.1 and the moderate-high threshold was identified at 0.3.
- **Case study:** To determine if the disengagement classifier is generalizable to a traditional college course, demographic and course performance data from pilot sections of Principles of Microeconomics taught at Colorado State University were combined with the historical data set.
- **Results:** The evaluation metrics for the pilot sections resulted in an AUC-ROC value of 0.94 and an AUC-PR value of 0.70, both of which are nearly identical to the trained classifier; There are subpopulations of students who are most at-risk of course disengagement. These ‘at-risk’ students ultimately earned a low final grade (D or F) or withdrew from either the course or university; Students who disengaged from Connect and then earned an D in the course. Ofthe 13 students in this group (Student 1-13), the disengagement classifier identified 12 as having at least moderate risk for weeks leading up to or on the week of actual disengagement from Connect. Of those 12, the classifier identified nine as high risk at some point before disengagement. The student that was not identified by the classifier prior to disengagement was identified as moderate risk two weeks after disengaging; Students who disengaged from Connect and then earned an F in the course. Of the 31 students in this group (Student 1-31), the disengagement classifier identified 23 as having at least moderate risk for weeks leading up to or on the week of actual disengagement from Connect. Of those 23, the classifier identified nine as high risk at some point before disengagement. Of the eight students that were not identified by the classifier prior to disengagement, two were identified as moderate risk one week after disengaging, and six were identified as moderate risk two weeks after; Students who disengaged from Connect and then dropped the course sometime between week 2 and week 15. These students did not earn a course grade, but instead, a W was transcribed on their transcript for the course. In this case, the classifier identified four ofthe nine students before they disengaged from Connect. Of the five students that were not identified by the classifier prior to disengagement, three were labelled as moderate risk one week after disengaging and two were labelled as moderate risk two weeks after disengaging; Students who disengaged from Connect and then withdrew from the university. Students who withdraw from the university often have a traumatic event (e.g., death of a parent or sudden and difficult illness) that precipitates the withdrawal. The classifier successfully identified ten of the seventeen students before they disengaged from the course. Of the seven students that were not identified prior to disengagement, five were identified as having moderate risk within one week, and two were identified within two weeks; Over all four groups, this classifier identified 70% (49/70) of students before or on the week they disengaged from Connect and identified all students within two weeks of disengaging; Students were grouped by their final course grade as strong pass (i.e., A or B), weak pass (i.e., C or D), or students who fail or withdraw from the course or university (i.e., F/drop/withdraw). The strong pass students are on average in the low risk category for the entire semester. Weak pass students are also in the low risk category for the entire semester but have slightly higher classification scores on average as compared to the strong pass students. The classification score for the weak pass students approached the moderate risk category towards the end of the semester. In stark contrast, the F/drop/withdraw students’ average classification score crosses into the moderate risk category on week four and into the high risk category on week ten of the semester; The number of weeks that the students spent in either moderate or high risk categories was analyzed. Many weeks all of the pilot students spent in either the moderate or high risk categories. Strong pass and weak pass students spend much less time in moderate or high risk categories. The differences between the groups is most obvious when looking at how many students spend zero weeks in either moderate or high risk categories. A total of 79% of strong pass students, 51% ofweak pass students, and 7% of F/drop/withdraw students spent the entire semester in the low risk category. The remaining 93% ofthe F/drop/withdraw students spent at least 1 week in the moderate or high risk categories with about 40% of these students spending ten or more weeks in moderate or high risk categories; A singular disengagement score was created for each student by averaging the weekly disengagement scores over all weeks in the semester. Then, the Pearson’s r correlations for the averaged disengagement scores were compared with the students’ GPAs from high school, GPA from the previous semester, GPA at the end of the semester, as well as the grade earned in Principles of Microeconomics.  Results show that all correlations are negative and statistically significant as shown by a P value less than 0.05, meaning lower disengagement classification scores are correlated with higher GPAs and course grade.
- **Notes:**

#### **Using Visual Learning Analytics to Support Competence-based Learning** - [10.1145/3284179.3284233](http://dx.doi.org/10.1145/3284179.3284233)
- **Source:** Proceedings of the Sixth International Conference on Technological Ecosystems for Enhancing Multiculturality - TEEM'18
- **Problem:** Helping students to improve their learning process.
- **Objective:** In this paper, we present COBLE, a system that promotes reflection and decision-making processes in Competence-based Learning environments by using visual learning analytics and recommendation techniques.
- **Methodology:** Develop a RESTful API architecture which provides a complete separation between the user interface and the data storage and its management.
- **Dataset:**
- **Metrics:**
- **Case study:**
- **Results:** COBLE currently supports importing the data from different platforms through comma-separated values (CSV) text files, where each line represents the information of a particular student. This format was adopted because both LMS such as Moodle and spreadsheets provide facilities to export the data to this format. COBLE provides an Open Social Student Model using Visual Learning Analytics in order to promote student self-reflection and to facilitate decision making processes for students and teachers.Students can observe their knowledge or  competence level and even contextualize it in relation to the class using the different visualizations provided by COBLE. Any student can also observe his or her mastery level on each competence and contextualize it with the mastery level of the class. COBLE also shows how the student evolves in each competence throughout the course, either individually or in comparison to the group. COBLE offers a set of different visualizations to allow students and lecturers to choose the type of chart they are more comfortable with. In addition, all charts are accompanied by a short description about how to interpret the data that users may consult at any time.
- **Notes:**

#### **Modeling Key Differences in Underrepresented Students' Interactions with an Online STEM Course** - [10.1145/3183654.3183681](http://dx.doi.org/10.1145/3183654.3183681)
- **Source:** Proceedings of the Technology, Mind, and Society on ZZZ - TechMindSociety '18
- **Problem:** Certain groups of people, including women and ethnic minorities, are underrepresented in STEM fields. Online versions of STEM courses are not always well suited for underrepresented students, although some of these students are also more likely to enroll in such courses. Furthermore, underrepresented students are less likely to remain in STEM majors at the undergraduate level.
- **Objective:** In this paper, we explore how underrepresented students interact differently from their peers in online STEM courses to inform course design and interventions that are tailored to support these students in their pursuit of STEM degrees and eventual STEM careers. This study explores behaviors recorded in
logs of university students’ interactions with a learning management system (LMS) for an introductory online STEM course.
- **Methodology:** We extracted a variety of features from the students’ clickstream to categorize students’ interaction with the LMS. Access-related features included: the number of weeks students logged in, total logins, events per login, total interaction events, times accessing written materials, grade views, quiz attempts, correct quiz answers, attempted exam questions, correct exam attempts, discussion forum post views, and forum posts made. A feature was also extracted to capture the mean length of forum posts made. We trained logistic regression models to classify whether students were members of underrepresented groups. A separate model was trained for each demographic variable (first generation, non-white, female, non-traditional) as well as models to classify STEM major students and students who achieved a high grade (B- or better). Grade prediction was performed with features derived from only the first two weeks of class interactions, and only for students who interacted with the LMS for at least two weeks. We evaluated model classification results with Cohen’s kappa and the Matthews Correlation Coefficient (MCC).
- **Dataset:**
- **Metrics:**
- **Case study:** We analyzed data from 470 students (of 586 students initially enrolled) in an online introductory STEM course offered by a public university in the Midwestern United States. Data were from seven semesters, including summer sessions, spanning three years of the course. The remaining 470 students included several traditionally underrepresented groups in STEM: 21.5% first-generation college students, 53.6% non-white, 53.4% female, and 24.3% non- traditional. The course was administered via an LMS called LON-CAPA.
- **Results:** There are indeed behavioral differences between underrepresented groups of students and their peers; We note that students who earned high grades could be distinguished from other students based on interaction patterns in the first two weeks of class. This is particularly interesting because it implies early behavioral patterns are related to eventual course outcome; Positive t-test results indicate the value of the feature was higher for the group of interest, e.g., non-traditional students accessed the discussion forums fewer times per week and attempted quiz problems more often. Several key results can be drawn from this analysis. The most predictive feature for each demographic model (lowest mean rank) varied between the different models. First-generation students were distinguished primarily by higher per- week quiz attempts, non-white students by longer discussion forum posts, female students by earlier submission of quizzes, and non-traditional students by less access of the discussion forums. This indicates that underrepresented groups behaved differently from their peers in unique ways, not in ways that consistently identified underrepresented students versus their peers overall; STEM major students logged in significantly fewer times and performed fewer events in each login indicating less total interaction with the LMS. Conversely, students who eventually got a high grade (B- or better) in the class logged in more frequently, at least in the first two weeks. They also viewed the discussion forum more frequently; Non-white students interacted with the LMS significantly less often during the daytime (8am-8pm) than their peers; Also notable is the fact that female students submitted quizzes significantly earlier than their peers. Interacting on Saturday was also characteristic of female students. However, their exam scores were lower on average.
- **Notes:**

#### **Prediction of student learning style using modified decision tree algorithm in e-learning system** - [10.1145/3239283.3239319](http://dx.doi.org/10.1145/3239283.3239319)
- **Source:** Proceedings of the 2018 International Conference on Data Science and Information Technology - DSIT '18
- **Problem:** The problem of identifying how learners would learn and attain new knowledge and skills has been gained the strong interest in research over the last decade.
- **Objective:** The proposed approach is used to determine and predict the student learning styles on their learning performance in the learning system and also to evaluate the classification accuracy of the modified decision tree classifier with other classifiers.
- **Methodology:** Initially, web log file of each learner is collected which contain the records of learners’ behavior while using the system and the participation in using the course related tools such as live chat, forum discussion, quizzes and class assignments based on the three dimensions (processing, perception and input) of FSLSM learning style model; The extracted learners’ actions are given as input to the modified decision tree classifier which includes more significant rules proposed, in order to strengthen the existing decision tree classifier to classify the learning style of each learner automatically, more precisely and accurately based on FSLSM. The final structure of a decision tree includes root, decision and leaf nodes with branches. Each node is labeled with the attributes it tests, and branches are labelled with possible values of the attribute. Then, data preprocessing is done to extract the learner actions based on the standard FSLSM model. This stage analyses the web log file based on the three dimensions of FSLSM model as explained below: (1) Processing dimension: This dimension determines the active and reflective learners by their participation in doing activity something physical or theoretical. Active learners mainly prefer to work in groups and perform more exercises whereas reflective learners prefer to work alone and perform less number of exercises. Based on this learning behavior, the behavior patterns of learners such as forum discussions, assignments and chats are investigated from weblog files; (2) Perception dimension: This dimension determines the sensitive and intuitive learners. The sensitive learners mainly have more attentiveness, carefulness and low trial and high completion towards the performance in exams and exercises whereas intuitive learners’ show being bored by details, carelessness and high trial and low completion in exams and exercises. Based on this learning behavior, the behavior patterns of learners such as assignments and exams are investigated from web log files; (3) Input dimension: This dimension determines the visual and verbal learners by their preference over the learning content and the participation in group activities. Based on the characteristics of verbal and visual learners, behavior patterns such as powerpoint, text, and hyperlinks are investigated from web log files; The extracted learners’ actions are given as input to the modified decision tree classifier which includes more significant rules proposed in order to strengthen the existing decision tree classifier to classify the learning style of each learner automatically, more precisely and accurately based on FSLSM. The final structure of a decision tree includes root, decision and leaf nodes with branches. Each node is labeled with the attributes it tests, and branches are labelled with possible values of the attribute; The values of precision and recall calculated to three dimensions of FSLSM learning styles are compared with the metrics of standard classifiers, J48, Hoeffding, Random and Logic Model Tree obtained from WEKA program using 10 fold cross validation method.
- **Dataset:**
- **Metrics:**
- **Case study:** The proposed approach has experimented on the real-time web log files of approximately around 300 datasets of learners collected from Moodle LMS used in the institution over a period of one year studying across 3 semesters for 5 online computing courses of computing department.
- **Results:** The precision of the proposed decision classification algorithm is high compared to other classification technique; The achieved results for classification accuracy and error rate obtained for proposed decision tree classifier ranges from 88% to 95% shows a high accuracy for the three dimensions of FSLSM compared to other standard classifiers, thereby proving its suitability for identifying the learning style of learners with an average accuracy of 91% and a very less error rate of 9%; The proposed decision tree classifier is able to classify perception dimension with a high accuracy of 95% as most learners did most of the exams and exercises provided for the course and hence significant variance was not found in this dimension. The input dimension is classified with a precision of 90% since most of the students use power point and hyperlinks rather than theoretical notes and hence less variance is shown in the predicted class. The processing dimension is classified with a precision of 88% as the students use forums and chats very less and hence variance was found in this dimension.
- **Notes:**

#### **Finding traces of self-regulated learning in activity streams** - [10.1145/3170358.3170381](http://dx.doi.org/10.1145/3170358.3170381)
- **Source:** Proceedings of the 8th International Conference on Learning Analytics and Knowledge - LAK '18
- **Problem:** This is a difficult number for teachers and tutors to oversee and follow, much less provide personalized feedback. (RQ1) Have motivated students better self-regulated strategies? (RQ2) Do students who report high self-regulation, evidence more self-regulated behavior on the platform? (RQ3) Do students who exhibit self-regulated behavior achieve better scores in the course?
- **Objective:** This paper proposes an analytic methodology based on simple metrics to identify self-regulation in clickstream behavior, and thence establishes its influence in course performance.
- **Methodology:** We analysed i) students self-reported motivation and self-regulation strategies as a baseline, ii) students interactions with the learning system to extract traces of self-regulation, and iii) students’ behavior throughout the course using descriptors of self-regulation strategies; The studies in this paper use the following three datasets: (1) Motivational Beliefs and Self-Regulation Strategies (MBSRS) questionnaire: A printed version of the questionnaire on motivational beliefs and self-regulation was used together with consent and demographics questions; (2) Activity logs: The LMS logs user interaction with course organization pages, content pages, the practical exercises pages, and the interaction with the quizzes; (3) Performance and control phases: We consider points obtained in
quizzes as control measures and points in the final exam as perfor- mance measure. There were eight quizzes awarding a maximum of 800 points. The exam awarded a maximum of 58 points; To answer RQ1 we used responses to the MBSRS questionnaire. Altogether, it consisted of 44 questions covering: self-efficacy (9q), intrinsic value (9q), test anxiety (4q), cognitive strategy use (13q) and self-regulation (9q). We collected 170 answers of which a total of 160 were usable (due to missing answers in different sections). Responses were compiled and compared with the course control and performance phases. We grouped responses to the pre-questionnaire. Reliability of these constructs was validated with Cronbach’s alpha. Thereafter, we analyzed: correlations between motivation and self-regulation dimensions, and correlations with results in the control phase of the course (responses to quizzes; points obtained in the final exam). Finally, regression analysis was performed to assess if the results in control and performance can be explained from the scores in MBSRS; To answer RQ2 we looked at the clickstream data logged during the course, we extract metrics that indicate self-regulated behavior and perform correlation analysis with self-reported scores. The course required students to interact with an online environment providing organizational resources, content pages for each class, questions embedded in content pages, as well as quizzes and compulsory exercise. The clickstream activities were labeled according to the type of activity in self-regulatory behavior as: (1) Planning activities that help organize effort, V_I: viewing content indices, V_O: viewing course organization; (2) Monitoring activities that test the learner acquired knowledge, V_X: viewing exercises, Q_Q: solving quizzes, E_C: solving questions embedded in content. (3) Regulating activities taken to acquire or reinforce knowledge, V_C; viewing content; The clickstream activity was partitioned in sessions, assuming that any two subsequent actions from a user that are separated more than 30m belong to different sessions. To answer RQ3 we analyze correlations between control phases and the indicators of self-regulation behavior. We investigate how users engage with the course by coding activities to identify study strategies. The course counted a number of different control mechanisms: questions embedded in content pages, optional quizzes available for a week after each class, exercises solved in groups and a final exam. The final note is computed with contributions from all these control phases, thus we will not consider it further. Considering each class as a milestone in the course and a phase as the time between milestones, a descriptor is created summarizing the behavior for a phase with one label where: O =no activity, P=planning activities, M=monitoring activities, R=regulation activities, and their combinations: PM, PR, MR, PMR. Hereby, an trajectory is calculated characterizing a student self-regulation behavior as a sequence ofdiscrete states at each phase. For each student there is one trajectory describing her/his behavior in the course, for a total of 321 trajectories. To analyze behavior patterns and how they impact the course performance we performed exploratory sequence analysis. We used Agglomerative hierarchical clustering based on Ward’s method. We compared results obtained in quizzes and the final exam by students in each cluster using one-way, between groups ANOVA and  Tukey Honest Significant Differences(Tukey HSD) for pair-wise comparisons.
- **Dataset:**
- **Metrics:**
- **Case study:** We undertook the analysis of a first-year university course on knowledge technologies, consisting of face-2-face lectures, and web-based system with the course content and quizzes on course content as control phases. The content for the eight major topics was: Knowledge Technologies (KT), Semantic Network Ontologies (SNO), Semantic Web (SW), Information Retrieval (IR), Web 2.0 - Web 3.0 (WW), Recommender Systems (RS), Rule Based Systems (RBS) and Artificial Neural Networks (ANN). The study was conducted in a course with 392 first year (second semester) students, prior consent and anonymization of data. Out of 392 enrolled students, 170 (140M,40F) completed the Motivational Beliefs and Self-Regulation Strategies (MBSRS) questionnaire. The number of students in studies that refer to this questionnaire is smaller than 170 after removal of incomplete responses.
- **Results:** Students that believe they have self efficacy also use self regulations strategies. Students who value their academic work are more willing to put forth effort and spend more time engaged in their school work. In addition, students who are more interested in and value their school work are less likely to be distracted by irrelevant thoughts; These results hint that students who had higher scores in self-efficacy and self-regulation also scored higher in the test. We found a negative correlation between test anxiety and self-efficacy; We found no correlations with the quiz points. Regression analysis did not reveal relationships between dimensions of motivation or self-regulation with quiz points; We first a regular participation in the first two classes and more access before exercise deadlines. The period with less participation matches the Easter holiday. Thereafter, participation grows towards the second exercise period and then decreases. A similar pattern can be seen until the fourth exercise. In general, while some students present a regular participation (e.g., 17 students were active 42 days in average.), the majority of the access take place before the last exam; More time is spent overall in activities related to regulation, as the bottom chart shows; Students with high self regulation not only access more, but also spend more time in the platform and divide the time in sessions, and so evidence a time management strategy. Students reporting higher self-regulation evidenced more planning and monitoring; Higher amount of sessions and time spent interacting with the platform translate in higher points in quizzes and in the final test; Higher planning, monitoring and regulation, translate in more points in the final exam; The clusters illustrates patterns of behavior as follows: (1) Cluster 1 inactive: students who did not engage with the course material (N=94); (2) Cluster 2 continuously active: students who engaged with course material before each class (N=90); (3) Cluster 3 procrastinators: students who become active for deadlines (N=62); (4) Cluster 4 probers: students that concentrate on exercises and quizzes repeatedly (N=75); Students that were continuously active obtained more points than inactive, also significantly more points than procrastinators and probers. Procrastinators obtained significantly better results than inactive students. Probers also obtained significantly more points than inactive students and obtained significantly more points than procrastinators. However, there was no statistical difference in the scores between procrastinators and probers These results support the claim that students who exhibit self-regulated behavior achieve better scores in the course.
- **Notes:**

#### **Visualization, Assessment and Analytics in Data Structures Learning Modules** - [10.1145/3159450.3159460](http://dx.doi.org/10.1145/3159450.3159460)
- **Source:** Proceedings of the 49th ACM Technical Symposium on Computer Science Education - SIGCSE '18
- **Problem:** Difficult to analyse, manage or view data from e-learning systems. Given the rapid increases in the CS population over the past few years these courses are large, and monitoring the performance of all students and reaching out to at-risk students is a challenge.
- **Objective:** Our focus in this work is towards building new interactive learning modules and analyzing student performance in data structures and algorithms courses, which have exhibited significant drop rates.
- **Methodology:** We present visual analytic tools that will make it easier for the instructor to monitor and understand student performance during the course, thus allowing timely interventions of students who might be falling behind, or at-risk students. Our learning modules and visualization tools are extensions to OpenDSA and the Canvas Learning Management System (LMS); (1) An ETL (Extract, Transform, Load) module will read recent student and course data from the OpenDSA database, compute statistical metrics and aggregations to reshape the data, and send the resultant data to our secondary data store. (2) The secondary database will cache the output of the ETL module. This includes data related to student exercise progress, topic proficiency, and overviews of the progress of a class within recent chapters and modules. The database will be implemented with MySQL. (3) A visualization module will let students examine their progress and proficiency and allow instructors to view rele- vant information about the performance of the students in their course; We also propose to make use of biclustering techniques. In our problem a bicluster is a sub-matrix of the matrix grade, composed of a subset of the students and a subset of the grades. Algorithms for biclustering can search for different type of clusters, for instance OPSM tries to identify high-value, low-value which could make sense when analyzing student grades. We selected in this work the CPB algorithm which looks for biclusters where rows are highly correlated and columns are highly correlated, according to Pearson Correlation Coefficient (PCC).
- **Dataset:**
- **Metrics:**
- **Case study:** Introductory courses in computer science.
- **Results:** Two visualizations of student performance in a
course across a combination of assignments, projects and exams. Our system supports sorting this visualization based on average student grades, reordering the columns to reflect comparison across categories (homeworks, projects, exams, etc). Our visualization supports looking at particular biclusters returned by the algorithm.
- **Notes:**

#### **VisForum: A Visual Analysis System for Exploring User Groups in Online Forums** - [10.1145/3162075](http://dx.doi.org/10.1145/3162075)
- **Source:** ACM Transactions on Interactive Intelligent Systems
- **Problem:** However, the dynamics of asynchronous forum data (AFD) and a large volume ofarchived posts pose great challenges for forum administrators and analysts to discover user groups and gain insights from participants’ interactions.
- **Objective:** Develop a system to visualize user groups in AFD.
- **Methodology:** We designed and developed a novel system, VisForum, to visualize user groups in AFD. VisForum consists of two modules, one for data processing and the other for system interface. In the data processing module, we design the Group Sorting Algorithm to gather posts from the same group, as well as craft an adapter to pre-process data from multiple sources; The visualization module has a Web-based interface, which allows interactive exploration of groups from different granularities. Following the guides of a set of well-defined design ratio- nales and tasks, we create multi-coordinated views to assist analysts to efficiently explore groups. Specifically, we design a sortable List View to summarize all threads to facilitate quick identifica- tion of interesting ones. We create group glyph, set glyph, and user glyph to enable users to obtain group information from various levels of details. We further develop a scatter plot equipped with circular-based glyphs to visualize group evolution over time; We designed and implemented the VisForum interface with four coordinated views (1): a Time Variant View, a List View, a Group Sequence View, and a Group Evolution View. The Group Evolution View and the Group Sequence View are core analytical modules. The Time Variant View allows analysts to examine the forum at any time period. The List View is designed to assist users’ exploration of grouping behavior from threads perspective; We focus on systematically evaluating the usability and effectiveness of VisForum. We recruited 10 participants (age range of 23 to 30) for the study. Eight out of ten are students with a background in computer science. Most of them have basic understanding of visualization by using tools such as Microsoft Excel. Notably, two of them have experience in visual analytics. All participants had explored online forums of various kinds. We first introduced VisForum with detailed information on the research context, design purpose, system architecture, and functionalities. To help participants familiarize with the tasks and visual designs, we provided a training session and presented sample questions covering major visual designs and tasks. The study was performed using the “think-aloud” method, which encouraged users to say whatever they were looking at, thinking, doing, and feeling when performing tasks. At the end of the study, we gave each participant a questionnaire with six questions, which evaluated their perceived usability of VisForum on a 7-point Likert scale ranging from (1) totally disagree to (7) totally agree. The measurement items covered topics such as helpfulness, intuitive- ness, and usefulness of each view. We also interviewed these participants after the experiment with open questions to collect their suggestions for improvement. We designed six tasks that the participants had to perform using VisForum. The tasks cover important features of the system, including sorting threads, identifying groups, viewing group and user details, comparing group relationships, and monitoring group evolution.
- **Dataset:** Data from Edx contains 13,289 posts distributed in 1,978 threads. There are 2,712 forum members active in this three-month course. Meanwhile, Lotro has 98,113 posts created by 6,117 users during 2013-10-16 and 2015-03-14.
- **Metrics:**
- **Case study:** This project originated from domain experts’ requirements. We work closely with two educators, who have released courses on Coursera and Edx. We demonstrate the usability of our system with two distinct AFD datasets, one is a course forum released in the MOOC platform Edx3 and the other is web-scraped data from The Lord ofthe Rings OnlineTM forum (we use “Lotro” as abbreviation).
- **Results:** In general, all participants were able to perform the tasks and achieve the goals within a short period of time. Data analysis of the questionnaire also shows positive results of the system’s usability. We note that our system does not support well identify the user group that occurs most frequently within the selected tread.
- **Notes:**

#### **Linking students' timing of engagement to learning design and academic performance** - [10.1145/3170358.3170398](http://dx.doi.org/10.1145/3170358.3170398)
- **Source:** Proceedings of the 8th International Conference on Learning Analytics and Knowledge - LAK '18
- **Problem:** (RQ1) To what extent do students’ timing of engagement align with the instructors’ learning design? (RQ2) How do different levels of performance and learning design relate to different study patterns?
- **Objective:** Exploring the study patterns across different groups of performance (based on final scores). Investigate how students actually study compared to the initial learning design, and how different groups of performance and LD were related to these study patterns.
- **Methodology:** The first dataset which captured the respective module LD was a result of an institutional-wide initiative. The seven types of learning activity were measured in terms of the duration (in hours) that was recommended for each type of activity in a particular week. The second dataset consisted of clickstream data of individual learners from the VLE and was retrieved using SAS Enterprise 9.4. To compare the LD with the actual student behaviour, time spent on task was calculated as the duration between clicks. To address our first research question, we visualized actual study patterns against the LD over 30 weeks. Second, we visualized the study patterns for respective individual study materials across excellent, passed, and failed group. The visualizations were done using Jupyter Notebook and Tableau.
In order to compare study patterns across three groups of performance over time, we used a multilevel modelling (MLM) (or mixed-effect modelling) approach (week t is nested within student i). Compared to the traditional repeated measure ANOVA approach, MLM has less stringent assumptions (homoscedasticity, compound symmetry, and sphericity), allows for missing data, tolerates differently spaced waves of data (e.g. due to Christmas breaks, Easter breaks), accounts for autocorrelation of residuals, and allows for nonlinear relations. First, we started with a random intercept model (weeks are nested within students) as the baseline (not reported here). To address RQ2, we composed two models. The first model (M1) focused on comparing three groups of performance (baseline = passed students) overtime with the time spent on studying ‘in advance’ and ‘catching up’ as the outcomes. The second model (M2) took into account individual student characteristics (age, gender, education, occupation) and time variant characteristics (the designs of assimilative, productive, assessment activities).
- **Dataset:**
- **Metrics:**
- **Case study:** This study took place at a public distance education institution in the UK, namely the Open University UK (OU). Our analyses were conducted using trace data from a Moodle platform longitudinally over 28 weeks, on 387 students, and replicated over two semesters in 2015 and 2016. There were 268 and 267 registered students in Fall 2015 and Fall 2016 respectively. However, the analysis in this study only took into account the students who completed the course. Thus, the analysis was conducted on 182 and 198 students in Fall 2015 and Fall 2016 respectively.
- **Results:** (RQ1) The actual study patterns seemed to follow the same trends in the LD. Overall, students in both semesters spent on average more time studying the materials after the assigned week (catching up and revise) ) than before the assigned week, except for studying the materials in week 8, week 18, and week 27. Overall, given the same study materials, the passed and the excellent group of students spent more time on studying in advance and catch up than the failed students in both semesters. In Fall 2015, passed and excellent students spent on average each week 1.81 hours and 2.3 hours  on studying in advance, compared to failed students with an average of 0.22 hours. Similar trends in the time studying in advance across the three groups was also presented in Fall 2016. In Fall 2015, passed and excellent students followed a similar pattern studying in advance. However, in Fall 2016 passed and failed students portrayed a similar pattern for all study materials from week 1 to week 12. Since then, passed students spent more time studying in advance than failed students. Our findings indicated that the way instructors design for learning significantly influenced how student spent time on VLE. While in general the intended learning design and actual behaviours followed a similar trend over time, there remained substantial discrepancies between what instructors recommended or expected and the actual time spent on respective learning activities by students. In particular, in most weeks students spent less time (nearly a half) studying the assigned materials on the VLE compared to the number of hours recommended by instructors. At the same time, in certain weeks the actual time spent on the assigned materials was equal or above the time recommended by instructors. Students’
actual timing of study engagement could be substantially different from the assigned week. In particular, most students spent more time studying the materials after the week which they were assigned for; (RQ2) Compared to passed students, failed students spent significantly less time on studying in advance in 2015, while excellent students did not have any statistically significant difference. A similar pattern was observed in 2016 for failed students while excellent students spent significantly more time on studying in advance. After adding the LD (Model 2), the relations between different groups of performance and the time spent on studying in advance remained the same. Not only did students exhibit different study patterns compared to the LD, these study patterns also varied significantly across our three groups of performance. Our analysis suggested that excellent students spent the highest amount of time studying on VLE, followed by passed students and failed students. Given the same study materials, excellent students spent a large share amount of time studying in advance, while failed students spent a large proportion of their study time on catching up; Our findings indicated that there were discrepancies between how instructors designed for learning and how students studied in reality. In particular, given the same materials, the time spent on VLE was on average less than the number of hours recommended by instructors in most weeks.
- **Notes:**

#### **Using Big Data Value Chain to Create Government Education Policies** - [10.1145/3274005.3274015](http://dx.doi.org/10.1145/3274005.3274015)
- **Source:** Proceedings of the 19th International Conference on Computer Systems and Technologies - CompSysTech'18
- **Problem:** (RQ1) How to apply Big Data Value Chain to support decision making in Education? (RQ2) Which Big Data Value Chain activities can be automated?
- **Objective:** Find groups with similiar interaction.
- **Methodology:** We have chosen the Microsoft SQL Server Data Tools in order to develop a decision support model for differentiation of schools and to make conclusions about the possibilities for automation of the activities of Big Data Value Chain. The following fields in the Data Set are selected as data model attributes that th analytics  algorithm processes: - Admin Code: administrative code of the schools; - Average Rate BLL: average grade of students on BLL; - Average Rate Maths: average grade of students on Maths; - Average Rate: average grade of students in school; - Average Total: average grade of students on all exams in the external assessment; - Excused Absence; - Percente Low Grades BLL: percent of low grades on BLL; - Percente Low Grades Maths: percent of low grande on Maths; - Students Left: Dropouts; - Students Repeaters; - Students Without Benefits Left: number of dropouts with suspended child allowances; - Students Without Benefits: number of students with suspended child allowances; - Undersized Classes: number of classes, which number of students is below the minimum; - Unexcused Absences; We performed Expectation Maximization (EM) to find groups fo similar items.
- **Dataset:**
- **Metrics:**
- **Case study:** We focused on a current project at the Ministry of Education and Science (MES) of Bulgaria, funded under Operational Programme Science and Education for Smart Growth.
- **Results:** We found seven clusters.
- **Notes:**

#### **Studying MOOC completion at scale using the MOOC replication framework** - [10.1145/3170358.3170369](http://dx.doi.org/10.1145/3170358.3170369)
- **Source:** Proceedings of the 8th International Conference on Learning Analytics and Knowledge  - LAK '18
- **Problem:** A major criticism of MOOC platforms, however, is their frequently high attrition rates with only 10% or fewer learners completing many popular MOOC courses.
- **Objective:** We present a solution that seeks to address this problem of replicability in the context of MOOCs. We do this by investigating the replicability of findings previously published in articles that leveraged learning analytics methods and data through the use of the MOOC Replication Framework.
- **Methodology:** We have developed MORF, the MOOC Replication Framework, a framework for investigating research questions in MOOCs within data from multiple MOOC data sets. Our goal is to determine which relationships (particularly, previously published findings) hold across different courses and iterations of those courses, and which findings are unique to specific kinds of courses and/or kinds of participants; The production rule analysis of MORF makes use of two different kinds of data: 1) clickstream events used to analyze the rules relating to the amount of time spent in the forums and on the assignments, and 2) relational database forum data used to analyze the rules relating to forum behavior and linguistic features; The chi-square test can determine whether the two values are significantly different from each other, and in doing so, determine whether the production rule or its counterfactual significantly generalized to the data set. Odds ratio effect sizes per production rule are also calculated.
- **Dataset:**
- **Metrics:**
- **Case study:** In this study, we tested MORF on 29 data sets obtained from the University of Edinburgh’s large MOOC program. We report the first large-scale use of MORF, attempting to replicate 15 published findings in 29 iterations of 17 MOOCs, listed: Artificial Intelligence; Planning Animal Behavior and Welfare; Astrobiology AstroTech: The Science and Technology Behind Astronomical Discovery; Clinical Psychology; Code Yourself! An Introduction to Programming; E-Learning and Digital Cultures; EDIVET: Do you have what it takes to be a veterinarian?; Equine Nutrition; General Elections 2015; Introduction to Philosophy; Mental Health: A Global Priority; Fundamentals of Music Theory; Nudge-It; Philosophy and the Sciences; Introduction to Sustainability; The Life and Work of Andy Warhol;
- **Results:** Students who start threads less frequently are less likely to complete; Students who used a narrower variety of words were more likely to complete; Students who spend more time in activities is associated with course completion; Students who spend more time with the course content, either through engaging in or observing the discussions in the forums or through engaging with the course assignments, is associated with course completion; We found that writing longer posts, writing posts more frequently, responding more frequently to other students’ posts, and having others respond more frequently to one’s own posts are all significant predictors of course completion; Students groups that init threads in forums tend to complete the course; Students more likely to complete the MOOCs produced more sophisticated language and used more bigrams and trigrams, but used less meaningful words.
- **Notes:** In an environment such as MOOCs, where students have the freedom to disengage at any point in the course, knowing that time spent in the discussion forums is associated with remaining engaged till completion indicates that attention should be spent on designing engaging and positive discussion forum experiences that encourage participation.

#### **ESL student engagement in an introductory blended learning course in chemistry** - [10.1145/3234825.3234836](http://dx.doi.org/10.1145/3234825.3234836)
- **Source:** Proceedings of the 2018 The 3rd International Conference on Information and Education Innovations - ICIEI 2018
- **Problem:** Learning chemistry is, not surprisingly, therefore even more difficult for students for whom English is a second language (ESL students), with the challenges being even greater for those students whose native language is less-related to the family of Indo-European languages to which English belongs. We realized that teaching our students basic concepts in chemistry would be linguistically challenging.
- **Objective:** The focus of this paper is student performance in the invigilated mid-term exam.
- **Methodology:** We decided to employ blended learning to augment the student learning environment. Students’ engagement with the LMS was investigated using data generated via the “report” and “grade” functions in Moodle. Raw data as XML files were exported from Moodle, curated, and initially analyzed by employing pivot tables. Descriptive and inferential statistical analyses employed Excel. Count (hit) measures were defined as the number of times each action was performed by each student; The survey used a five-point Likert-type scale and was conducted on a voluntary basis, with the items using declarative statements in the form of short, simple sentences without jargon; We performed stepwise regression analysis and tested the significance of possible correlations between the number of an individual student’s hits on the total as well as individual course elements and their performance in the mid-term exam.
- **Dataset:**
- **Metrics:**
- **Case study:** The data used in the current study were collected for one course (68 students were registered) at a small-sized university in the United Arab Emirates. Online delivery used Moodle. Course elements in the LMS included the course syllabus, a textbook, readings, lectures (as both PowerPoints and video screencasts), weekly quizzes based on the lecture material, practice exams, and additional exam information and examples.
- **Results:** Based on a five-point Likert scale, and using an average of the Likert responses, students rated the elements from least to most useful (average rank in parentheses) as: the Textbook (3.2), Lecture videos (3.9), Readings (4.0), Lecture PowerPoints (4.3), Quizzes (4.5), and the Mid-term practice exam (4.8). We concluded that students focused heavily on the potential assessment items and many gained an overall picture of the course material primarily using the lecture PowerPoints; Analysis of the percentage of students who accessed the LMS elements at least once showed that only 17% of students accessed the course readings and 26% the textbook, whereas over 80% of students accessed both the mid-term practice exam and lecture PowerPoints; Consistent with there being no correlation between regular student access to the LMS and their results, some of the very highly- achieving students accessed the LMS very infrequently and, conversely, those students who were not highly successful in the mid-term exam accessed the LMS very frequently; Access in the online environment resources increase close exam.
- **Notes:**

#### **Discovery and temporal analysis of latent study patterns in MOOC interaction sequences** - [10.1145/3170358.3170388](http://dx.doi.org/10.1145/3170358.3170388)
- **Source:** Proceedings of the 8th International Conference on Learning Analytics and Knowledge - LAK '18
- **Problem:** What are the different study patterns exhibited by learners during MOOCs assessment periods and how do learners’ study patterns evolve over time?
- **Objective:** In this paper we aim to investigate MOOC study patterns and perform temporal analysis of learners’ longitudinal behaviours.
- **Methodology:** We employ two different methods to answer this question. In the first method, following a hypothesis-driven approach, we label students’ activity sequences according to predefined patterns and perform clustering to identify prototypical participation trajectories over the course duration. In the second method we propose a data-driven approach to automatically capture study patterns from learners’ interaction sequences; To identify such study patterns in learners’ interaction logs, we examine activity sub-sequences for each assessment period accord- ing to two criteria: (1) whether the learner starts his/her learning sequence by watching a video or by submitting the assignment, (2) whether the learner submits to the assignment before the hard deadline. Considering these criteria, we label activity sub-sequence with one of the following study patterns: (1) V_start: learner watched the video(s) before submitting the assignment; (2) A_start: learner submitted the assignment without having watched the corresponding video(s); (3) Audit: learner watched the video(s) but did not submit the assignment; (4) Inactive: learner did not watch the video(s) and did not submit the assignment; Once we have the study pattern sequences for all learner in the course, we apply hierarchical agglomerative clustering on the sequences to extract categories of learners with similar study profiles and identify prototypical study pattern sequences over the course duration.
- **Dataset:**
- **Metrics:**
- **Case study:** The dataset used for this study consists of the interaction logs of participants in “Functional Programming Principles in Scala”, an undergraduate engineering MOOC produced by EPFL university. The course was composed of seven sets of video lectures and six graded assignments. The dataset includes three categories of events, describing learners’ interaction with video lectures (play, pause, download, seek, change speed), assignments (submit) and discussion forums (read, write, or vote a message).
- **Results:** In the most common case (69% of all study sessions), learners watch videos before submitting an assignment (V_start). However, in all assessment periods, around 10% of the learners skip video lectures and directly submit the assignments (A_start). Proportion of learners who watch the videos but do not submit the assignments (Audit), gradually increases towards the end ofthe course (8% vs. 18% in the first and last assignments respectively). Proportion of Inactive students also considerably increases in the last two assessment periods (4% vs. 20% respectively in the first and last assignments); Learners in A_start approach, are likely to have prior knowledge about the assignment topic, since they achieve a high grade in their first (and often only) attempt, without viewing the course lectures; Analysis of individuals’ study pattern sequences shows that 53% of learners continue with their initial study approach during the course duration. These learners can be clustered into three categories, represented as Cluster 1 to 3. Learners who follow the V_start approach in all the assessment tasks, form the largest cluster, Cluster 1, comprising 44% of participants. This group represent typical MOOC learners who rely on lectures to attain the knowledge required for solving the assignments. On the other hand, 2% of participants, represented by Cluster 2, do not spend time on watching the videos before submitting any of the assignments. Their high performance level (average grade of 90 out of 100), reflects their proficiency in the course topics. Earning the completion certificate could therefore be one of the main participation motivation for these learners. On the contrary, 7% of learners, Cluster 3, do not submit any of the assignments, but they follow most of the video lectures during the course period. This group of learners watch the videos as a source ofknowledge without having the intention of receiving a certificate. This group can be referred to as auditing students; Learners who start by watching videos, have a low probability of skipping videos in the next period whereas learners with A_start approach show a relatively high probability of watching the videos before submitting the next assignment. Students who audit the course in one assessment period, will most likely continue auditing or go Inactive in the next period. Once entering the Inactive state, participants are not very likely to get engaged in solving the next assignments, but they might continue watching the videos in the next period; We found 8 clusters of learner pattern trajectories represented as Cluster 4 to 11. Learners in Cluster 4, mainly follow A_start approach, but in few assignments, mostly the first or last ones, they watch the videos prior to submitting. Cluster 5 on the contrary comprises learners whose main approach is V_start, but they skip the videos in one or two assignments during the course. The start time of the learning sequence for these learners is closer to the assignment deadline in the A_start sessions, in comparison with the previous assessment period. Learners in Cluster 6, also mainly prefer to watch the videos first. But in the last two periods, they submit the assignments without watching the videos. These learners achieve nearly complete grade in the first four assignments (average grade 9.7 out of 10 sd = 0.8). Considering that the final grade is calculated based on the assignment grades, such learners are likely to have a high final grade even without receiving the complete score in the remaining assignments. The last five clusters (Clusters 7 to 11), show learners who start the course with an active approach as they get engaged both in watching the videos and submitting the assignments (V_start), but their engagement level decreases over the course duration. Learners in Cluster 7 and 8 remain engaged until the course end. However, over time they loose motivation for doing the assignments and continue watching the course lectures without making any submission. learners in Cluster 7 submit nearly half of the assignments, whereas those in Cluster 8 only submit the first one or two, before switching to the auditing state. Cluster 9, 10, and 11, demonstrate profiles of disengaging learners or dropouts. The dominant pattern in learners’ study profiles in these clusters is to start by V_start approach, change to Audit state (stop submitting the assignments) and finally stop watching the videos and drop out. The three clusters differ in the point at which learners’ engagement level decreases. Participants in Cluster 9 submit the first four assignments and 66% of them acquire enough points to pass the course. Whereas those in Cluster 10 and 11 stop doing assignments after one to three weeks, and eventually drop out about a week after.
- **Notes:**

#### **Profiling students from their questions in a blended learning environment** - [10.1145/3170358.3170389](http://dx.doi.org/10.1145/3170358.3170389)
- **Source:** Proceedings of the 8th International Conference on Learning Analytics and Knowledge - LAK '18
- **Problem:** Our goal was to answer to 3 research questions: (RQ1) Is it possible to define a taxonomy of questions to analyze students’ questions? (RQ2) Can identification of students’ questions be automatized? (RQ3) Are annotated questions asked by a student related to their profile, i.e. is there a relationship between the type of questions asked by a student and their characteristics, in particular in terms of performance?
- **Objective:** In this paper, we are interested in whether the type of questions asked by students on an online platform can be related to their performance and their overall learning behavior.
- **Methodology:** To answer to RQ1, we took a sample of 800 questions (12% of the corpus size) from two courses (biochemistry [BCH], the histology & developmental biology [HBDD]), considered by the pedagogical team to be among the most difficult ones and had the highest number of questions asked. This sample was randomly divided into 4 sub-samples of 200 questions to apply 4 categorization steps: (1) a discovery step, consisted in empirically grouping sentences with similarities to extract significant concepts; (2) a consolidation step, consisted in annotating the second sub-sample to validate the dimensions and values previously identified; (3) a validation step, we performed a double annotation to validate the generality of our categories on the third sub-sample of 200 sentences the entire sample (600 sentences); (4) an evaluation step, the last sub-sample of 200 sentences was annotated manually by the two expert annotators; To answer to RQ2 and to annotate the whole corpus we have chosen to use NLTK (Natural Language Toolkit), a major library in Python to handle natural language data. To answer to RQ3 we decided to distinguish two types of students: the good students (whose rank in the final exam is superior to 200th) and the average ones (whose rank is between 200th and 600th). The students with a rank superior to 600th generally did not ask enough questions to be considered here. Moreover, teachers usually consider that students over rank 600th do not really have a chance to pass the final exam, even with their assistance, and generally want to focus more on average students. We extract clusters of students, and then we tried to identify the characteristics distinguishing each of them. We performed clustering analyses using the K-Means algorithm (with k between 2 and 10) We performed the clustering using as features for each student the proportion of each question asked in each dimension (e.g. the proportion of questions with value 1 in dimension 1) asked (a) overall, (b) during the first half of the course and (c) during the second half of the course (44 features overall).
- **Dataset:**
- **Metrics:**
- **Case study:** The dataset is made of questions asked in 2012 by 1st year medicine/pharmacy students from a major public French university (Université Joseph-Fourier). The Faculty of Medicine and Pharmacy has a specific hybrid training system (part of work is done remotely and the other part in classroom) for their 1st year students.
- **Results:** We obtained 4 clusters. Cluster A represents 18 to 29% of the students and is characterized by grades lower than average, and a course and global attendance that is always lower than the one from students in other clusters. They asked less questions than average, but their questions received more votes than average and were therefore fairly popular. Cluster D represents 14 to 25% of the students in the first semester and is characterized by course and final grades significantly higher than others who tend to be attending most of classes. They ask more questions than average but whose questions are less popular, with less votes overall. Cluster B represents 36 to 40% of students, whose grades, attendance and number of questions asked are similar to cluster D. Overall, their questions are mostly verification of knowledge and on link between concepts, but only in the 1st half of the class. They correspond to students who are in the process of actively building their understanding of the course. Cluster C tend to be an intermediate cluster which always lies in between clusters A and D.
- **Notes:**

#### **Instructor Perspectives on Comparative Heatmap Visualizations of Student Engagement with Lecture Video** - [10.1145/3159450.3159487](http://dx.doi.org/10.1145/3159450.3159487)
- **Source:** Proceedings of the 49th ACM Technical Symposium on Computer Science Education - SIGCSE '18
- **Problem:** Previous research, has typically focused video analyses on holistic visualization of engagement models. (RQ1) By exploring the interface of multiple heatmaps, how do the class instructors or video owners interpret the heatmaps of user engagement with online videos? (RQ2) How does the heatmap of user engagement influence the class instructor in self critiquing the teaching method?
- **Objective:** We aim to examine the potential for comparative visualizations of video engagement, in order to better understand instructor interests and needs in this regard.
- **Methodology:** We developed a learning analytics visualization tool that links data on student viewing and video interactions with student demographics and performance measures. Instructors can use the tool to generate and compare correlated heatmap visualizations representing student engagement across segments of the video. This tool is named VIVA – Video Interaction Visual Analytics. As a given video is watched over time, the analytics tool tracks (1) segments of the video watched and re-watched; (2) click data such as play, pause, and skip; and (3) metadata such as playback speed for segments watched. We presume that engagement correlates and can be modeled by interaction cues. Thus VIVA employs a visual analytics model to create a heatmap visualization of the interaction data to represent engagement. The heatmap corresponds directly to the video timeline, displaying a color that represents the overall degree of engagement at that point in the video. In this research, a green to red color scheme mapping was selected for heatmap display, where green colors represent lower levels of engagement than red colors and lighter versions of each color represent lower levels of engagement than darker. This paper describes an initial user study conducted with instructors to understand how this kind of feedback can inform their use of video in the classroom. We adopted a “Think Aloud” approach for the user study. The study employed a semi-structured session, where the participants interacted freely with the interface and the experi- menter asked pre-determined questions, with follow-up on topics of interest pointed out by the participants.
- **Dataset:**
- **Metrics:**
- **Case study:** We randomly selected a lecture video from a set that had been employed over several semesters in a course on Human-Computer Interaction. The selected video was a lecture video on an- alyzing user study data, close to 20 minutes in length. The collected user interaction data for this video included 141 watching sessions from a pool of 101 students (both undergraduate and graduate) across two semesters. We obtained the demographic and academic data for the students represented in the interaction data. This in- cluded program level (e.g. graduate and undergraduate), grade, GPA, gender and citizenship; Ten participants signed up for the study. The ten participants consist of seven class professors / lecturers and three PhD student TAs experienced with the video material. Four out of the seven professors had directly taught the class and had used the selected video as course material. The remaining professors had substantial experience us- ing online videos as course material. The average study duration was 30 minutes.
- **Results:** A qualitative coding analysis was conducted to analyze the data; All participants indicated that if there were such a tool they would like to compare the patterns and interpret the segments that significantly differentiate others. For example, P10 mentioned “...The second thing would be to look at the hot spots, em..., to see either what was the most engaging but also maybe it was confusing. Then I have to what to interpret...”; Three participants wanted to have a tool that could help them find correlations between the patterns and the video content. Interest- ingly, four responders wanted a tool which could help them simply identify whether the students had watched the video or not. Some participants were interested in very specific functions that would be possible, but were not implemented in the interface for the study; Overall, we observed that participants had a very positive experience with the multiple heatmap comparison. “Interesting” (any words with stemming from “interest”) is the term most often men- tioned (120 occurrences) across all participants’ interaction with different heatmaps. Other positive indicators include “fascinating” (11 times) and “fantastic” (9 times); Five participants confirmed that some watching patterns definitely matched their impressions about the filtered groups of students. For example, P3 (comparing graduate & undergraduate watching). P4 (comparing domestic & international watching) explained; A majority of participants (7) mentioned that comparative analysis would impact classroom coverage of material or concepts covered in the video. Both the “hot” and “cold” spots gained attention. Three of the seven noted that hot spots could indicate difficulty in students’ understanding of the material, and would warrant additional review in class. On the other hand, four were concerned about cold spots that contain particularly important concepts. This could indicate that many students had missed or neglected critical points, which would also warrant additional review in class; Five participants discussed video content modifications based on the results of comparative analysis. Six participants suggested remaking videos to address video content issues that cause lower than expected engagement in segments with important content. Other instructors (P7, P8, P10) suggested refinement is to change the video content to motive the students. Three instructors indicated that they could incentivize students to pay attention and focus on the video by revising video content to address identified issues; Participants commented on the reasons why theywere interested in a particular comparison from two perspectives: (1) the visual results from multiple heatmaps and (2) the functionalities of the multiple heatmaps. The
- **Notes:**

#### **Using Learning Analytics to Promote Student Engagement and Achievement in Blended Learning: An Empirical Study** - [10.1145/3241748.3241760](http://dx.doi.org/10.1145/3241748.3241760)
- **Source:** Proceedings of the 2nd International Conference on E-Education, E-Business and E-Technology - ICEBT 2018
- **Problem:** Low student engagement and lack of autonomy in learning. (1) In the blended learning environment, can the use of intervention based on learning analytics improve student engagement and achievement? (2) What is the correlation between student achievement and engagement (behavioral engagement, emotional engagement, cognitive engagement) in blended learning modes?
- **Objective:** This study aims to examine different learning modes and investigate the advantages of blended learning, mainly in student engagement and achievement, and improve the strength of blended learning through learning analytics.
- **Methodology:** Quantitative self-report and quantitative observational measures were adopted to measure student engagement and two quizzes were used to measure student achievement. This study was carried out in real-classroom setting, quantitative self-report data was collected during class and quantitative observational measures data was collected from Moodle which is a learning manage system we used; We performed a questionnaire for blended learning environment to measure student engagement. We used Likert-type responses that from 1 (strongly disagree) to 5 (strongly agree); We also gathered the data provided by Moodle’s logo file to observe the level of student’s engagement in online learning including the number of logging in to Moodle, time on task, assignment completion and the number of posts to a discussion board; In this study, we chose to use notification to intervene students’ learning process. QQ is a very popular social instant messaging software in China. Messages which were sent to QQ group, which means all of them could see these, called public notification interventions in this study; All descriptive statistics and correlation analysis were conducted using IBM SPSS. The normal distribution test was conducted for the score of two tests and student engagement of two learning modes (blended learning (BL) and blended learning with intervention(BLWI)); Behavioral engagement draws on the idea of participation and includes involvement in academic, social, or extracurricular activities; Emotional engagement is being considered having positive reactions to classmates, teachers and the content of class. Cognitive engagement is defined as the student’s level of investment in learning; In this study, Spearman’s correlation analyses were conducted between academic achievement and student engagement (behavioral engagement, emotional engagement and cognitive engagement) for BL and BLWI.
- **Dataset:**
- **Metrics:**
- **Case study:** All the participants are sophomores. They all participate in course of Multimedia Technology.
- **Results:** Every item of BLWI has increased compared BL. The mean of total engagement (TE) of BLWI is 75.03, a slight increase compared to BL (73.29); On the second day and third day which the days we using interventions, the records show a significant increase. And actually, before the first public intervention which means on the first day, only two students logged in to the Moodle, and on the second day, after the first public interventions, 25 students had logged in to Moodle and produced records. On the third day, after private interventions, all of them participated the previewing of lesson and posting in the discussion forum. However, it is worth to know that in blended learning without intervention, there are 7 students out of 31 did not participate discussion activity after deadline; The relationship between engagement and achievement in BL and BLWI were not found in this study, or in other words, the relationship was not significant. That probably due to this research is conducted on a small sample, therefore, the relationship was not so obvious to observe; Blended learning with intervention can improve student engagement and achievement compared to blended learning without intervention, there is only statistically difference on student achievement; In the blended learning mode, whether the intervention was used or not, it has not found yet that students’ achievement was related to students’ engagement.
- **Notes:**

#### **Supporting Self-Regulated Learning with Visualizations in Online Learning Environments** - [10.1145/3159450.3159509](http://dx.doi.org/10.1145/3159450.3159509)
- **Source:** Proceedings of the 49th ACM Technical Symposium on Computer Science Education - SIGCSE '18
- **Problem:** How do visualizations that support self-regulated learning affect students’ academic performance and behavior?
- **Objective:** Our study focuses both on visualizations for supporting self-regulated learning as well as on students’ motivations.
- **Methodology:** We examine how different visualizations affect students’ academic performance and behavior in an online learning environment. We developed two different visualizations: a textual and a radar visualization. These visualizations show the students their exercise points and time management when completing the problems on a course; The textual visualization showed the variables as a list of values. Each variable had the name of the metric, a short description of it, the value of the metric as a progress bar with a numerical label and a description of where the points came from and how to maximize the value of the metric; The radar visualization showed the metrics using a radar chart. The radar chart consisted of a sequence of spokes where each spoke represented one of the four metrics. The length of each spoke was defined by the value of the metric the spoke was representing; 442 students were divided randomly into three groups. The control group consisted of 168 students, the treatment group A consisted of 141 students, and the treatment group B consisted of 133 students. Group A was assigned the radar visualization, and Group B was assigned the textual visualization. For analysis, the three groups were then divided further into four groups based on their goal orientation determined by the survey: (1) Control: 55 mastery approach; 49 performance approach; 35 mastery avoidance; 29 performance avoidance; total = 187; (2) Group A/Radar: 45 mastery approach; 49 performance approach; 27 mastery avoidance; 20 performance avoidance; total = 141; (3) Group B/Text: 48 mastery approach; 42  performance approach; 25 mastery avoidance; 18 performance avoidance; total = 133; The differences in the visualizations were motivated by performance oriented students striving to perform well relative to others whereas mastery oriented students strive to develop competence. Academic performance was measured through the number of exercise points; We also measured whether the visualizations can be used to influence how early and on how many days the students solve exercises before the deadline to see whether a visualization encourages behavior which has been shown to lead to better academic performance.
- **Dataset:**
- **Metrics:** The four metrics that were used to describe students’ behavior are as follows: (1) Starting measured how soon aſter the release of an exercise set the student started to work on the exercises; (2) Scheduling measured the number of days that the student worked on an exercise set; (3) Earliness measured the average distance from the deadline of a specific exercise set; (4) Exercise points measured how many points the student had received from the exercises of a certain exercise set; Each metric was normalized to a range from 0 to 10, where 0 indicated poorest performance and 10 indicated the best performance.
- **Case study:** The data for this study comes from an eight and a half-week online introductory programming course that was organized during the Spring 2017 at University of Helsinki, a European research first University. The course had a total of 753 students, from whom 442 were included in this study. The 311 students were excluded for reasons including not wanting to participate in the study, not answering the provided questionnaires, or not attending more than a single week of the course.
- **Results:** When comparing the average exercise points from the control group to the treatment groups no statistically significant difference was found; Students in the radar visualization group earned on average 170.23 exercise points, whereas students in the textual visualization group earned on average 147.67 exercise points. In other words, students in the radar group completed approximately 15% more exercises than the students in the textual group; Highest performing students, regardless of the visualization, earned the maximum points in the course without nearly any variance; The lowest two thirds reveal a difference between the groups. Students in the middle third are akin except when using the textual visualization, which seems to be harmful. Students in the lowest third of the radar visualization group earned on average 86.31 exercise points, whereas students in the lowest third of the textual visualization group earned on average 55.98 exercise points. Additionally, the lowest third in the control group earned on average 65.02 exercise points; Performance approach oriented students in the radar visualization group complete more exercises than the performance approach oriented students in the textual visualization group: stu- dents in the radar visualization group earned on average nearly 30 more exercise points than the ones in the textual visualization group; The mastery oriented students also performed better in the radar group than the students in the textual group; Performance approach oriented students in the control group earned approximately on average 173.59 exercise points, while they earned on average 155.12 points in the textual visualization group; Performance approach oriented students who had no visualization performed better than the performance approach oriented students with the textual visualization. This indicates that a visualization may even be harmful to performance approach oriented students if the visualization does not provide a comparison to other students. This observation suggests that performance and mastery approach oriented students may benefit from different types of visualizations – or from the lack of a visualization; Students in the radar visualization group submit exercises on average 13.45 days before the deadline whereas students in the textual visualization group submit exercises on average 11.17 days before the deadline; In the radar visualization group performance approach oriented students submitted exercises approximately on average 13.5 days before the deadline whereas performance approach oriented students in the textual visualization group 10.6 days before the deadline; Similar difference in exercise submission earliness was observable between the performance approach oriented students in the control group and the textual visualization group. Performance approach oriented students in the control group submitted exercises 3.26 days earlier than the performance approach oriented students in the textual visualization group; There was no clear indication that the visualizations would have a positive effect in behavior which is considered to lead into better academic performance.
- **Notes:**

### Elsevier

#### **Mining theory-based patterns from Big data: Identifying self-regulated learning strategies in Massive Open Online Courses** - [10.1016/j.chb.2017.11.011](http://dx.doi.org/10.1016/j.chb.2017.11.011)
- **Source:** Computers in Human Behavior
- **Problem:** Understanding of how learners behave and learn in digital environments. What are the most frequent interactions sequences of learners in MOOCs? How do the interaction sequences of learners with different academic performance differ? How do the interaction sequences between learners with different SRL profiles differ?
- **Objective:** This study focuses on the relationship between the trace data generated through the interaction of learners with the course content (video lectures and assessments) in online sessions and learners' self-reported SRL skills.
- **Methodology:** Learners in the three MOOCs completed an optional questionnaire at the beginning of the course. The questionnaire included items related to demographic measures (age, gender, education) and learners' intentions in the course (towatch all lectures or only some of them). In addition, the questionnaire included the Online Learning Enrollment Intentions (OLEI) scale. The SRL measure consisted of 24 statements related to six SRL strategies and it was originally adapted from multiple established instruments. Learners rated statements using a 5-point scale (coded from 0 to 4). For each strategy, the individual score was computed by averaging ratings of corresponding statements. The SRL measure exhibited high reliability for all strategy subscales with Cronbach's alpha and Pearson's correlation coefficients. We used a Process Mining method. We extracted the trace data from Coursera's data-base in order to study the interaction sequences of learners in the MOOC and obtain a process model representing the behaviour of the learners within the MOOC. Likewise, we selected the Disco algorithm and Celonis algorithm and their implementations in the Disco and Celonis commercial tools. We analysed the process models in the model analysis stage to identify the most frequent interaction sequence patterns. First, we analysed the models, considering all the data from the three courses. Second, we analysed the data from each course separately. We analysed how these patterns vary according to whether or not learners completed the course. To achieve this, we filtered the log file by completer and non-completer status. We use an agglomerative hierarchical clustering technique for grouping learners based on the identified interaction sequence patterns. We used agglomerative hierarchical clustering based on Ward's method. We use other clustering techniques as Gaussian mixture and K-means to define the appropriate number of clusters based on the silhouette score. This lead to selecting the solution with 3 clusters as the best one.
- **Dataset:**
- **Metrics:** We defined six types of interactions depending on the objects that learners interact with: start a video-lecture, complete a video-lecture, review a video-lecture already completed, try an assessment, pass an assessment, and re-view an assessment already passed. A session is a period of time in which the Coursera trace data registers continuous activity of a learner within the course, with intervals of inactivity no greater than 45 min.
- **Case study:** The final study sample comprised N = 3458 online learners in three different MOOCs. This sample is a subset of  4871 respondents who answered the initial questionnaire among the 54,935 learners who registered for the MOOCs. Data collection occurred between April and December 2015. This study encompassed three courses (Aula constructivista, Electrones en accion and Gestion de organizaciones) offered by Pontificia Universidad Catolica de Chile on Coursera.
- **Results:** Only Video-lecture (45.25% of the sessions follow this type of pattern). The most common interaction sequence in this type of interaction pattern is Begin session, then Video-lecture-begin, then End session without completing the video-lecture; Assessment try to Video-lecture: 21.58% of the sessions follow this type of pattern, with the most common interaction sequence of this interaction pattern being a loop between Begin session to Assessment-try to Video-lecture-begin to Assessment-try to Video-lecture-complete to Assessment-try to End session; Explore: 15.67% of the sessions follow this type of pattern, in which the most common behaviour of the learners is to follow a disorganised interaction sequence in which they go from one type of content (assessments or video-lectures) to another without completing them; Only Assessment: 10.76% of the sessions follow this type of pattern, in which the most common interaction sequence is Begin session to Assessment-try to End-session without completing the assessment; Finally, Video-lecture complete to Assessment-try (3.32%), Video-lecture to Assessment-pass (1.10%) and Others (2.32%) interaction sequence patterns are the least common. We found that for completers were more common to perform sessions that contain more assessments than non-completers. Completers' sessions mainly consist of: (a) taking one assessment after another (called Only Assessment) or (b) trying an assessment and then watching a video-lecture (called Assessment try / Video-lecture) or (c) watching video-lectures and trying an assessment without completing either (called Explore). By contrast, non-completers’ sessions consist of watching one video-lecture after another (called Only Video-lecture). We have analysed similarity in the SRL profiles between each group of clusters. As a result, we did not observe statistically significant differences between Cluster 2 and 3, while we observed statistically significant differences when comparing with Cluster 1. Cluster 1 - Sampling learners: this cluster is composed by learners with least SRL scores compared with their counter-Cluster 1 e Sampling learners: this cluster is composed by learners with least SRL scores compared with their counter-parts. Learners in this cluster in average per session perform low number of video-lectures and in average per session perform few attempts to try to solve assessments. These learners have a low activity in the course (generally learners in this group watch just a single video-lecture or start “sample” at the beginning of the course exploring materials with the course already started); Cluster 2 e Comprehensive Learners: this cluster is composed by learners with a SRL scores higher than the learners in cluster 1, so they can be considered as more self-regulated. They watched more video-lectures on average per session than learners in the other clusters. Based on the observed interaction sequences, learners in this cluster tend to follow the path that is provided by the course structure. They also invest more time watching video-lectures and therefore exhibit a higher level of engagement than learners in cluster 3; Cluster 3 e Targeting Learners: this cluster is composed of learners with similar SRL scores to those in cluster 2, which suggests that the difference in observed behaviour is not due to differences in their SRL profiles. Learners in clusters 2 and 3 also complete the course at similar rates (29% and 30% respectively). However, learners in cluster 3 watch fewer video-lectures and complete more assessments on average per session. They also tend to explore the course contents more than learners in clusters 1 and 2. These differences lead us to describe this group of learners as more strategic or goal oriented.
- **Notes:**

#### **Virtual learning environment engagement and learning outcomes at a ‘bricks-and-mortar’ university** - [10.1016/j.compedu.2018.06.031](http://dx.doi.org/10.1016/j.compedu.2018.06.031)
- **Source:** Computers & Education
- **Problem:** Student engagement is highly sensitive to the learning context, specifically to the chosen LD (Learning Design) and assessment method. This relation cannot be studied or reported without taking into account the variability in LD and disciplinary contexts.
- **Objective:** To evaluate the relationship between engagement and outcome in a bricks-and-mortar institution.
- **Methodology:** We extracted data from all students who took each module in the 2015/16 academic year. We fit a predictive model on VLE usage in each degree programme to determine how well grades can be predicted on VLE usage alone. As a test dataset for our predictive model, we used data from all students who took each module the same in the 2016/17 academic year; We say a student is active in a module if their activity sequence shows multiple interactions with that module on the VLE within a 5 min period; We fit ordinal logistic regression (OLR) models to our 2015/16 dataset to predict a student's module grade based on their mean daily VLE usage; We used a chi-squared test to separates high and low VLE usage and high and low success (using the median values as separators). We also used Spearman's rank correlation between VLE usage and module result.
- **Dataset:**
- **Metrics:** We chose our categories as ‘Fail’ (< 40%), ‘3rd/2:1’ (> 40%,<60%), ‘2:1’ (> 60%,<70%), and ‘1st’ (> 70%).
- **Case study:** Our main data is collected from 2015/2016 academic year at the University of Exeter (UoE), an established bricks-and-mortar higher education institution in the UK. To ensure a diverse disciplinary spread of degree programmes for our analysis, we chose a single programme from each college: (1) BSc Biological Sciences; (2) BA English; (3) BSc Mathematics; (4) BA Politics; (5) BSc Economics; (6) BSc Medical Sciences; For each programme, we chose all the compulsory modules for the first year of study. The VLE at the UoE has a hub for each module where lecture slides, worksheets and extra reading material is uploaded by the lecturer. There is also a forum space for discussion of module content and limited functionality for assessments (e.g. class tests, quizzes). The VLE is run on the Moodle software platform.
- **Results:** Module results are reasonably normally distributed with a negative skew most likely due to students who failed to continue with the module after only getting part way through the term. VLE activity appears to show a Pareto or Zipf's distribution often associated with levels of activity, though we do not quantify the fit to these distributions. It appears that a majority of students interact very little with VLE (e.g.<2 min per day) and still get good marks (e.g.>60%). There are also few students who have high VLE activity that do not have a high grade. A chi-squared test shows that this difference is significant, with far fewer students falling in the high-VLE/low-grade quadrant than the null expectation. Higher VLE usage is beneficial to getting a higher mark and is not related to low grades. We find a stronger correlation between VLE usage and module results in students with grades below 40%, compared to students with grades above 40%. The stronger correlation for low performers could suggest that more interaction with VLE might improve results for struggling students. When modules are grouped by programme, we observe clear differences between programmes in the range and spread of student VLE usage associated with each programme. For example, there are students in BSc Biological Sciences, Mathematics, Economics and Medical Sciences who have mean daily VLE usage over 8 min, whereas there is only one person in each of BA English and Politics who have mean usage over 5 min. That mean grades are relatively similar for all programmes, whereas mean daily VLE usage shows greater variation. In addition, the weight of essays in the summative assessment for each module was significantly negatively correlated with the strength of correlation between VLE usage and grades of VLE usage. Thus, a higher weight of assessment by an essay means a weaker correlation between VLE and grades and a lower usage level. Thus overall we find that LD may be an important explanatory factor for the observed differences between programmes/disciplines in terms of the strength of relationship between student engagement with the VLE and their module grades. Regarding our predictive OLR model, it does not predict the most likely outcome, and that its real strength is in assigning probabilities for all grade categories.
- **Notes:**

### IEEE
#### **Supporting Student Engagement Through Explorable Visual Narratives** - [10.1109/TLT.2017.2722416](http://dx.doi.org/10.1109/TLT.2017.2722416)
- **Source:** IEEE Transactions on Learning Technologies
- **Problem:** Student engagement with such environments remains an open issue.
- **Objective:** The goal of this research is to support students using Online Learning Environments (OLEs), to engage with course content through personalized visual narratives.
- **Methodology:** The visual narratives are made available to students through VisEN (a novel framework supporting the construction and consumption of visual narratives). VisENenables narrative authors to construct a story, which is automatically visualized, personalized and made explorable. The visual narrative consists of a personal message, displayed across multiple screens that can have a beginning, middle and end in order to guide the learner. Each narrative slice of the story focuses on guiding the student through the data presented by the visualization via a textual description; The Narrative Builder component is used by narrative authors (course instructor in this work) to construct a story, consisting of a beginning, middle, and end; The Visual Narrative Explorer component enables students to view, interact and explore their visual narratives. This component personalized the visual narratives for each student by 1) updating the data used by the course instructor in the visual narrative to use current student logged data, and 2) personalizing the descriptions in the message; The collection of data included student interactions with their visual narratives, learners’ responses to a post-course questionnaire, and their opinions towards their visual narratives; The quantitative method examined the impact that the visual narrative usage had on course engagement through statistical measures. The quantitative data also examined students’ responses to the post-course questionnaire statements. It also included the running of chi-square tests for independence on the responses; The qualitative method examined student opinions towards the various aspects of their visual narratives (via the open-ended post-course questionnaire); We also examined the correlations between visual narrative usage versus course engagement and grades using the Pearson correlation coefficient. The continuous assessment grade included the mark a student achieved for the deliverables that he/she worked on using AMAS. It also included the mark that a student achieved for his/her project submission at the end of the course; All analyses were conducted using 1) student logged data (consisting of student interactions with the course and their visual narratives), 2) students’ questionnaire responses, and 3) their opinions towards their visual narratives.
- **Dataset:** 120,000 student interactions for both academic years.
- **Metrics:**
- **Case study:** VisENwas used during two successive academic years (108 students in 2013-2014 and 125 in 2014-2015) in Trinity College Dublin to provide visual narratives to a total of 233 undergraduate students studying the Information Management and Data Engineering module. The difference in setup between the two years included two extra slice transformations in the second year and an upgrade to the servers in the same year to improve the speed by which the pages were loaded.
- **Results:** The students who had engagement levels of below average and who subsequently improved their engagement to average or higher could be clearly identified (‘improving engagement students’). From the 233 students across both academic years, 97 were identified as improving engagement students. The improving engagement students had on average at least two times more visual narrative interactions than the rest of the class; In 2013-2014, A and B the increase in visual narrative sessions, A highlighting the number of sessions during below average engagement and B showing the number of sessions during improved engagement. The same holds true for 2014-2015, shown through A’ (below average engagement) and B’ (improved engagement). There are students did not improve their engagement to the same level as the improving engagement students as it remained at below average; The responses from the improving engagement students showed that 71 percent of them in both academic years, strongly agreed or agreed that the visual narratives supported them in engaging with the course. The response from the rest of the class was spread across the five options, where 45 and 56 percent of these students from the 2013-2014 and 2014-2015 academic years respectively, strongly agreed or agreed with the statement; The findings from the first two parts of Analysis 1 showed that the majority of students who improved their engagement following below average engagement level  notifications increased the usage of their visual narratives; The visual narratives had an important role in supporting course engagement; There was a moderate positive correlation between visual narrative usage and the AMAS deliverables marks. For the historically weaker students, there was a strong positive correlation between visual narrative usage and the AMAS deliverables marks; In the 2014-2015 academic year, only a combined mark for the AMAS submissions and the end of the course project was available. There was a weak positive correlation between this combined mark and visual narrative usage for the class. For the historically weaker students, there was a moderate positive correlation between these two variables; There was no relationship between visual narrative interactions and the AMAS related questions grades; There are strong positive correlations between improving engagement students’ course engagement and visual narrative interactions and between historically weaker students’ continuous assessment grades and visual narrative interactions; The students with a higher level of interactions with their visual narratives found the slice transformations more useful.
- **Notes:**

#### **Analyzing Learners Behavior in MOOCs: An Examination of Performance and Motivation Using a Data-Driven Approach** - [10.1109/ACCESS.2018.2876755](http://dx.doi.org/10.1109/ACCESS.2018.2876755)
- **Source:** IEEE Access
- **Problem:** Although motivation plays an important role in the online learning context, a limited number of contemporary studies considered behavioural activity interplay factors that could affect participant motivation
- **Objective:** In this research, we examine the links between engagement, performance, and motivation, in the context of geographical influences. The aim of the experiments in this study is to analyze and evaluate log data that reflect learner activity.
- **Methodology:** We employ LA tools in evaluating the links between the learners’ educational background, engagement level and performance. Moreover, machine learning models are applied in the prediction of learner motivational status. Hence, the predictors in our experiments are based on quantitative log data, rather than questionnaire responses; Two sets of experiments are conducted in this research. In the first experiment, we investigate the link between the level of engagement and performance, considering the geographical location of the learners. Behavioral features are employed to examine the association of engagement level with performance. As behavioral features are represented with continuous variables, statistical techniques are used in their analysis and interpretation. The statistical analysis makes inferences about the successful and failing learner groups in terms of the number of usage videos and reads chapters. To evaluate whether the descriptive results are significant, we use hypothesis testing (Analysis of Covariance); In the second set of experiments, the target is to identify learner motivational status and the reasons behind student drop out from varying viewpoints. Traditional statistical analysis has limited ability in predicting student motivational status as it is not designed to discover the non-linear features that separate the students’ motivational categories. Moreover, statistical analysis requires human input in making assumptions about the relationships between variables. Therefore, additional analysis was performed using machine learning techniques that do not rely on classical assumptions. Machine learning approaches are used to categorise learner motivation using predictors extracted from the log data, allowing the interaction of learners to be evaluated. We performed Decision Tree (DT), Neural Networks (NN) and Regularized Discriminant Analysis (RDA); To evaluate the results ofdescriptive statistics, analysis of covariance (ANCOVA) was used. The Chi-squared test is a statistical hypothesis test which was used to examine the difference between failure and success groups per course with respect to the learners’ academic level.
- **Dataset:**
- **Metrics:**
- **Case study:** The dataset used in this study was obtained from Harvard University. In this study, two courses were selected for analysis, namely, ‘‘Introduction to Computer Science’’ and ‘‘Circuits and Electronics’’. Fall courses were delivered in the fall of 2012 and spring courses were covered in the spring of 2013 (total of 4 courses).
- **Results:** The results also demonstrate that successful learners watch more videos than failing students. Europe dominated the top rankings in the successful learners group for ‘‘Computer Science Fall’’, ‘‘Electronics Spring’’, and the ‘‘Electronics Spring’’ courses, respectively. However, the highest number of successful learners in ‘‘Electronics Fall’’ lived in Africa; In general, successful learners read learning materials three times more than unsuccessful learners; In general, the engagement level of the successful group is higher than the failing group, when considering both the number of videos viewed and number of chapters read by learners; The learners’ educational background is associated with the learners’ performance level; The learners’ educational level impacts on their performance. Overall, most completion learners are reported as secondary, Bachelors (60%) and Masters (50%) qualified, with a smaller number of doctorate learners aiming to earn certification; The results of descriptive statistical analysis show that Europe ranks the highest in terms of learner success rates, while Asia reports the highest ratio of failing group participants; The overall accuracy and kappa results, showing the best result of 0.7546 generated by the DT network, while the lowest result was achieved by RDA, with an average value of 0.7372; The F1-Measure for NN shows slightly better results than DT. The lowest F1-Measure is reported for class ‘‘intrinsic’’ with a value of 0.6111 for the NN model; We found that the clickstream followed by the "ndays_act" (Number of unique days that the learner interacted with teh course) features were the most important parameters for prediction purposes; The results reveal that behavioral features can be used to detect the lack of students’ motivation at the early stage of online courses.
- **Notes:** Engagement can be classified into three main categories, namely, behavioural, emotional, and cognitive engagement. Emotional engagement occurs when students feel emotionally engaged in a learning activity. Cognitive engagement refers to the students’ feelings in regards to progress in the academic task, while behavioural engagement refers to the level of student participation in the learning activity.

## 2019

### ACM
#### **OneUp: Engaging Students in a Gamified Data Structures Course** - [10.1145/3287324.3287480](http://dx.doi.org/10.1145/3287324.3287480)
- **Source:** Proceedings of the 50th ACM Technical Symposium on Computer Science Education - SIGCSE '19
- **Problem:** Student engagement and motivation in computing courses, especially at the lower level, is a commonly recognized problem. The majority of available educational tools for practicing are crafted to support skill development through self-study, which makes it hard to mandate and control their use. Self-study tools typically support recommended content/activities and as such, their use rarely counts towards the final grade, which results in a low usage. (RQ1) Does gamification encourage out-of-class practice? (RQ2) Does gamification improve grades? (RQ3) What is students’ perception of the usefulness of the gamification platform and their engagement with it?
- **Objective:** Our approach to ameliorating this problem is through increasing students’ motivation by employing gamification.
- **Methodology:** We developed the OneUp Learning platform, which is aimed at facilitating the process of gamifying academic courses and enabling tailoring of the gamification features to meet the vision of the course instructor; The main functionality of OneUp includes: (1) Support for instructors to incorporate game design principles and mechanics in the instructional methods they use in their course; (2) Support for creation and automatic checking of static and dynamic problems, and (3) Learning analytics and visualization to inform students and instructors of student performance and progress throughout the course; Students can see aggregated information about their performance in their personal learning analytics dashboard. This dashboard displays student experience points (XP), practice points (collected and possible), and course bucks they have earned so far; For the first two (R1 and R2), we used the OneUp system log to extract data for tracking student visits to the gamification-related pages, how many practice quizzes they have completed, etc. We also used student final course grades to evaluate the impact of gamifying the course on students’ academic performance. To answer the third research question (R3), we conducted a survey with the experimental group at the end of the semester.
- **Dataset:**
- **Metrics:**
- **Case study:** In this study we decided to use OneUp as both an online practice system and a gamified framework for maintaining information not only about student practicing but also about student overall course engagement and performance. We used OneUp in addition to the Blackboard CMS system. Data Structures course consisted of two parts: creating warm-up challenges for student practicing and configuring the course in the OneUp platform. The rewards in the gamified course were in the form of badges and virtual currency (VC), which students could earn and spend based on rules specified by the instructor. We decided to use badges primarily to reward students for their performance and to use virtual currency mainly to reward their course engagement and efforts to learn, including practicing, attending classes, etc. We used the fall 2017 class (16 students) as a control group and the spring 2018 class (11 students) as an experimental group. The same instructor taught both classes using the same instructional materials, teaching methodology, and student assessment. Both groups used the OneUp platform for out- of-class learning and practicing, but for the experimental group all gamification features were activated, while for the control group they were disabled.
- **Results:** The page most frequently visited by the students is the Learning Dashboard, where they could see the aggregated information about their course performance; The next in popularity is the page reporting the student’s virtual currency transactions. This was the place where students could track the status of a purchase made in the Course Shop and the corresponding message from the instructor; Adding gamification features resulted in a significant increase of the taken warmup challenges: the experimental group took 239 distinct warmup challenges with a total of 554 attempts, while the control group took only 45 distinct warmup challenges with a total of 73 attempts. These results signal that after the gamification intervention, students’ practicing has intensified significantly; While we cannot claim statistical significance of these results, they reveal a drastic change of the numbers of Fs and a significant increase of the passing grade (C) for the experimental group. For comparison, the Fs and Ds for the previous two course offerings are as follows: F16 – 10% Ds and 10% Fs and for S17 – 15% Ds and 20% Fs. There is also a slight increase of the As and Bs; The majority of students perceived OneUp as very useful; The overall result shows a good engagement of the students at the time of practicing; Of interest are student responses to questions about what motivates them to keep practicing once they have started. The desire to improve their grades (Q5) and to earn more virtual currency (Q7) are the leading factors for student persistency to practice (100% Agree and Strongly Agree for both).
- **Notes:**